{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "#  Improved Predictive-Policing | Google Colab script\n",
        "#  Enhanced with panel data structure, temporal validation, and explainability\n",
        "#  [FIXED VERSION WITH IMPROVED ERROR HANDLING]\n",
        "# ============================================================\n",
        "\n",
        "# ---------- 1. imports & configuration ---------------\n",
        "import os, json, warnings, requests, calendar\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import yaml  # Add this import for saving conda environments\n",
        "\n",
        "# Machine learning models - focused selection for small data\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Spatial visualization\n",
        "try:\n",
        "    import folium\n",
        "    from folium.plugins import HeatMap\n",
        "    FOLIUM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Folium not available. Install with: pip install folium\")\n",
        "    FOLIUM_AVAILABLE = False\n",
        "\n",
        "# Try to import optional libraries\n",
        "try:\n",
        "    import statsmodels.api as sm\n",
        "    import statsmodels.formula.api as smf\n",
        "    from statsmodels.regression.mixed_linear_model import MixedLM\n",
        "    MIXED_MODELS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Statsmodels not available. Install with: pip install statsmodels\")\n",
        "    MIXED_MODELS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "    OPTUNA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"Optuna not available. Install with: pip install optuna\")\n",
        "    OPTUNA_AVAILABLE = False\n",
        "\n",
        "# SHAP for model explainability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"SHAP not available. Install with: pip install shap\")\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "# LIME for local explanations\n",
        "try:\n",
        "    import lime\n",
        "    from lime import lime_tabular\n",
        "    LIME_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"LIME not available. Install with: pip install lime\")\n",
        "    LIME_AVAILABLE = False\n",
        "\n",
        "# Fairness libraries - improved error handling\n",
        "try:\n",
        "    from fairlearn.metrics import MetricFrame\n",
        "    from fairlearn.reductions import Reweighing, GridSearch, ExponentiatedGradient\n",
        "    from fairlearn.postprocessing import ThresholdOptimizer\n",
        "    FAIRLEARN_AVAILABLE = True\n",
        "    print(\"Fairlearn available - fairness analysis will be performed\")\n",
        "except ImportError:\n",
        "    print(\"Fairlearn not available. Install with: pip install fairlearn\")\n",
        "    FAIRLEARN_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import mlflow\n",
        "    MLFLOW_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"MLflow not available. Install with: pip install mlflow\")\n",
        "    MLFLOW_AVAILABLE = False\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ---------- 2. configuration ---------------------------------------\n",
        "# User settings\n",
        "TIME_RANGE = [2022, 2023, 2024]    # Years to include (train, val, test)\n",
        "USE_CRIME_RATE = True              # Use per-capita crime rate instead of counts\n",
        "USE_PANEL_DATA = True              # Create panel data (area × month)\n",
        "MAX_RECORDS = 2_000_000            # Adjust based on computing resources\n",
        "BATCH_SIZE = 100_000               # For memory efficiency\n",
        "N_TRIALS = 20                      # Optuna trials - reduced to save time\n",
        "CV_FOLDS = 3                       # Time series cross-validation folds\n",
        "RAND_STATE = 42                    # For reproducibility\n",
        "MAX_CORRELATION = 0.80             # Maximum allowed correlation between features\n",
        "MAX_FEATURES = 30                  # Maximum features to select for models\n",
        "USE_MLFLOW = MLFLOW_AVAILABLE      # Use MLflow for model tracking\n",
        "APPLY_FAIRNESS = FAIRLEARN_AVAILABLE  # Apply fairness mitigation\n",
        "USE_EXPLAINERS = SHAP_AVAILABLE or LIME_AVAILABLE  # Use model explainers\n",
        "\n",
        "# Directories - auto-create in Colab\n",
        "BASE_DIR = \"/content/drive/MyDrive/predictive_policing\"\n",
        "# Create these directories if they don't exist (for Google Colab)\n",
        "if not os.path.exists(BASE_DIR):\n",
        "    BASE_DIR = \"/content/predictive_policing\"  # Fallback path for Colab\n",
        "\n",
        "CACHE_DIR = f\"{BASE_DIR}/cache\"\n",
        "VIZ_DIR = f\"{BASE_DIR}/viz\"\n",
        "MODELS_DIR = f\"{BASE_DIR}/models\"\n",
        "LOGS_DIR = f\"{BASE_DIR}/logs\"\n",
        "\n",
        "# Create directories\n",
        "for directory in [BASE_DIR, CACHE_DIR, VIZ_DIR, MODELS_DIR, LOGS_DIR]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# API endpoints\n",
        "CRIME_API = \"https://data.cityofchicago.org/resource/ijzp-q8t2.json\"\n",
        "COMM_API = \"https://data.cityofchicago.org/resource/igwz-8jzy.json\"\n",
        "CENSUS_API = \"https://data.cityofchicago.org/resource/kn9c-c2s2.json\"\n",
        "BOUNDS_API = \"https://data.cityofchicago.org/resource/bt9m-d2mf.json\"\n",
        "\n",
        "# MLflow experiment setup if available\n",
        "if USE_MLFLOW and MLFLOW_AVAILABLE:\n",
        "    mlflow.set_tracking_uri(f\"file://{BASE_DIR}/mlruns\")\n",
        "    mlflow.set_experiment(\"predictive_policing\")\n",
        "\n",
        "# ---------- 3. helper functions --------------------------------------\n",
        "def log(msg, level=\"info\"):\n",
        "    \"\"\"Log messages to console and file\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
        "    log_msg = f\"{timestamp} | {msg}\"\n",
        "    print(log_msg)\n",
        "\n",
        "    # Also save to log file\n",
        "    with open(f\"{LOGS_DIR}/run_{datetime.now():%Y%m%d}.log\", \"a\") as f:\n",
        "        f.write(log_msg + \"\\n\")\n",
        "\n",
        "# ---- Centralized NaN & Type Coercion ----\n",
        "def safe_median(s: pd.Series) -> float:\n",
        "    \"\"\"Safely compute median of a Series, handling NaN values and conversion issues\"\"\"\n",
        "    try:\n",
        "        median = s.dropna().median()\n",
        "        if hasattr(median, 'item'):\n",
        "            return float(median.item())\n",
        "        else:\n",
        "            return float(median) if pd.notna(median) else 0.0\n",
        "    except (TypeError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "def safe_convert(value):\n",
        "    \"\"\"Safely convert a value to float, handling various pandas/numpy types\"\"\"\n",
        "    try:\n",
        "        if hasattr(value, 'item'):\n",
        "            return value.item()\n",
        "        elif pd.isna(value):\n",
        "            return 0.0\n",
        "        else:\n",
        "            return float(value)\n",
        "    except (TypeError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "def impute_missing_values(df):\n",
        "    \"\"\"Impute missing values in the dataframe using safe operations\"\"\"\n",
        "    log(f\"Imputing missing values (before: {df.isna().sum().sum()} NaNs)\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # For numeric columns, use median imputation\n",
        "    for col in df_copy.select_dtypes(include=['number']).columns:\n",
        "        null_count = df_copy[col].isna().sum()\n",
        "        if null_count > 0:\n",
        "            median_val = safe_median(df_copy[col])\n",
        "            df_copy[col] = df_copy[col].fillna(median_val)\n",
        "            log(f\"  Imputed {null_count} missing values in '{col}' with median={median_val:.3f}\")\n",
        "\n",
        "    # For categorical columns, use mode imputation\n",
        "    for col in df_copy.select_dtypes(exclude=['number']).columns:\n",
        "        null_count = df_copy[col].isna().sum()\n",
        "        if null_count > 0:\n",
        "            mode_val = df_copy[col].mode()[0] if not df_copy[col].mode().empty else \"UNKNOWN\"\n",
        "            df_copy[col] = df_copy[col].fillna(mode_val)\n",
        "            log(f\"  Imputed {null_count} missing values in '{col}' with mode={mode_val}\")\n",
        "\n",
        "    log(f\"After imputation: {df_copy.isna().sum().sum()} NaNs remaining\")\n",
        "    return df_copy\n",
        "\n",
        "def fetch_batches(api, limit, max_rows, params=None):\n",
        "    \"\"\"Fetch data in batches with query parameters\"\"\"\n",
        "    rows, off = [], 0\n",
        "    pbar = tqdm(total=min(max_rows, 10_000_000), unit=\"rec\")\n",
        "\n",
        "    query_params = {\"$limit\": limit, \"$offset\": off}\n",
        "    if params:\n",
        "        query_params.update(params)\n",
        "\n",
        "    while off < max_rows:\n",
        "        query_params[\"$offset\"] = off\n",
        "        try:\n",
        "            r = requests.get(api, params=query_params, timeout=60)\n",
        "            if r.status_code != 200:\n",
        "                log(f\"API error: status code {r.status_code}\")\n",
        "                break\n",
        "            batch = r.json()\n",
        "            if not batch:\n",
        "                break\n",
        "            rows.extend(batch)\n",
        "            off += len(batch)\n",
        "            pbar.update(len(batch))\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            log(f\"Request error: {str(e)}\")\n",
        "            break\n",
        "\n",
        "    pbar.close()\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def check_data_quality(df, name):\n",
        "    \"\"\"Check for data quality issues\"\"\"\n",
        "    log(f\"Data quality check for {name} dataset:\")\n",
        "    log(f\"  Shape: {df.shape}\")\n",
        "    log(f\"  Missing values: {df.isna().sum().sum()}\")\n",
        "    log(f\"  Columns: {', '.join(df.columns)}\")\n",
        "\n",
        "    # Check for outliers in numeric columns\n",
        "    for col in df.select_dtypes(include=['number']).columns:\n",
        "        if df[col].nunique() > 1:  # Skip constant columns\n",
        "            try:\n",
        "                q1, q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
        "                iqr = q3 - q1\n",
        "                outliers = ((df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr))).sum()\n",
        "                if outliers > 0:\n",
        "                    log(f\"  Column {col}: {outliers} potential outliers detected\")\n",
        "            except Exception as e:\n",
        "                log(f\"  Error checking outliers for {col}: {str(e)}\")\n",
        "\n",
        "    return 1.0 - (df.isna().sum().sum() / (df.shape[0] * df.shape[1]))\n",
        "\n",
        "def ensure_numeric(df, columns):\n",
        "    \"\"\"Ensure columns are numeric types\"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        if col in df_copy.columns:\n",
        "            try:\n",
        "                # Store original count of non-null values\n",
        "                orig_count = df_copy[col].notna().sum()\n",
        "\n",
        "                # Try to convert to numeric, coercing errors to NaN\n",
        "                df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
        "\n",
        "                # Count NaN values created\n",
        "                nan_count = df_copy[col].isna().sum()\n",
        "\n",
        "                # Report if values were lost\n",
        "                if nan_count > (df_copy.shape[0] - orig_count):\n",
        "                    log(f\"  Converted {col} to numeric, {nan_count - (df_copy.shape[0] - orig_count)} values became NaN\")\n",
        "            except Exception as e:\n",
        "                log(f\"  Error converting {col} to numeric: {str(e)}\")\n",
        "    return df_copy\n",
        "\n",
        "def ensure_numeric_coordinates(df, coord_columns=['latitude', 'longitude', 'center_lat', 'center_lon']):\n",
        "    \"\"\"Ensure coordinate columns are properly converted to numeric with appropriate error handling\"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    for col in coord_columns:\n",
        "        if col in df_copy.columns:\n",
        "            # Store original count of non-null values\n",
        "            orig_count = df_copy[col].notna().sum()\n",
        "\n",
        "            # Convert to numeric, coercing errors to NaN\n",
        "            df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
        "\n",
        "            # Report conversion results\n",
        "            new_count = df_copy[col].notna().sum()\n",
        "            if new_count < orig_count:\n",
        "                log(f\"  Warning: {orig_count - new_count} values in '{col}' couldn't be converted to numeric\")\n",
        "\n",
        "            # Fill NaN values with median if there are still valid values\n",
        "            if df_copy[col].notna().any():\n",
        "                median_val = df_copy[col].median()\n",
        "                na_count = df_copy[col].isna().sum()\n",
        "                if na_count > 0:\n",
        "                    df_copy[col] = df_copy[col].fillna(median_val)\n",
        "                    log(f\"  Imputed {na_count} missing values in '{col}' with median={median_val:.4f}\")\n",
        "            else:\n",
        "                # If all values are NaN, fill with a default value\n",
        "                df_copy[col] = df_copy[col].fillna(0.0)\n",
        "                log(f\"  No valid values in '{col}', filled all with 0.0\")\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def clean_object_columns(df):\n",
        "    \"\"\"Clean object columns that might contain unhashable types like dicts\"\"\"\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    object_cols = df_copy.select_dtypes(include=['object']).columns\n",
        "    for col in object_cols:\n",
        "        # Check if column contains dicts or other unhashable types\n",
        "        try:\n",
        "            # Test a small sample to see if there are unhashable types\n",
        "            sample = df_copy[col].dropna().head(100)\n",
        "            if len(sample) > 0:\n",
        "                _ = pd.Categorical(sample)\n",
        "        except (TypeError, ValueError):\n",
        "            # If that fails, convert to string representation\n",
        "            log(f\"Converting unhashable values in column {col} to strings\")\n",
        "            df_copy[col] = df_copy[col].apply(lambda x: str(x) if x is not None else x)\n",
        "    return df_copy\n",
        "\n",
        "def standardize_community_area_columns(crime_df, comm_df, census_df=None):\n",
        "    \"\"\"Standardize community area column names across datasets\"\"\"\n",
        "    log(\"Standardizing community area columns across datasets\")\n",
        "\n",
        "    # Make copies to avoid modifying originals\n",
        "    crime_df_copy = crime_df.copy()\n",
        "    comm_df_copy = comm_df.copy()\n",
        "    census_df_copy = census_df.copy() if census_df is not None else None\n",
        "\n",
        "    # List of possible column names for community area\n",
        "    possible_area_cols = [\n",
        "        'community_area', 'area', 'area_number', 'area_numbe', 'area_num',\n",
        "        'area_num_1', 'community_area_number', 'community_area_id'\n",
        "    ]\n",
        "\n",
        "    # Standardize in crime data\n",
        "    crime_area_col = next((col for col in possible_area_cols if col in crime_df_copy.columns), None)\n",
        "    if crime_area_col is None:\n",
        "        if 'block' in crime_df_copy.columns:\n",
        "            crime_df_copy['community_area'] = crime_df_copy.block.str.extract(r'(\\d+)', expand=False)\n",
        "        elif 'district' in crime_df_copy.columns:\n",
        "            crime_df_copy['community_area'] = crime_df_copy.district\n",
        "        else:\n",
        "            crime_df_copy['community_area'] = '1'  # Default value\n",
        "    elif crime_area_col != 'community_area':\n",
        "        crime_df_copy['community_area'] = crime_df_copy[crime_area_col]\n",
        "\n",
        "    # Standardize in community data\n",
        "    comm_area_col = next((col for col in possible_area_cols if col in comm_df_copy.columns), None)\n",
        "    if comm_area_col is None:\n",
        "        comm_df_copy['community_area'] = comm_df_copy.index.astype(str)\n",
        "    elif comm_area_col != 'community_area':\n",
        "        comm_df_copy['community_area'] = comm_df_copy[comm_area_col]\n",
        "\n",
        "    # Standardize in census data if provided\n",
        "    if census_df_copy is not None and not census_df_copy.empty:\n",
        "        census_area_col = next((col for col in possible_area_cols if col in census_df_copy.columns), None)\n",
        "        if census_area_col is None:\n",
        "            census_df_copy['community_area'] = census_df_copy.index.astype(str)\n",
        "        elif census_area_col != 'community_area':\n",
        "            census_df_copy['community_area'] = census_df_copy[census_area_col]\n",
        "        census_df_copy['community_area'] = census_df_copy['community_area'].astype(str)\n",
        "\n",
        "    # Ensure all are string type and clean values\n",
        "    crime_df_copy['community_area'] = crime_df_copy['community_area'].astype(str).str.strip()\n",
        "    crime_df_copy['community_area'] = crime_df_copy['community_area'].str.extract(r'(\\d+)', expand=False)\n",
        "\n",
        "    comm_df_copy['community_area'] = comm_df_copy['community_area'].astype(str).str.strip()\n",
        "    comm_df_copy['community_area'] = comm_df_copy['community_area'].str.extract(r'(\\d+)', expand=False)\n",
        "\n",
        "    if census_df_copy is not None:\n",
        "        return crime_df_copy, comm_df_copy, census_df_copy\n",
        "    else:\n",
        "        return crime_df_copy, comm_df_copy\n",
        "\n",
        "def extract_temporal_features(crime_df):\n",
        "    \"\"\"Extract temporal features including seasonality and trends\"\"\"\n",
        "    log(\"Extracting temporal features\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    crime_df_copy = crime_df.copy()\n",
        "\n",
        "    if 'date' not in crime_df_copy.columns:\n",
        "        log(\"No date column available for temporal features\")\n",
        "        return crime_df_copy\n",
        "\n",
        "    # Ensure date is datetime\n",
        "    crime_df_copy['date'] = pd.to_datetime(crime_df_copy['date'], errors='coerce')\n",
        "\n",
        "    # Check for invalid dates\n",
        "    invalid_dates = crime_df_copy['date'].isna().sum()\n",
        "    if invalid_dates > 0:\n",
        "        log(f\"Warning: {invalid_dates} invalid dates found and will be excluded from temporal features\")\n",
        "        # Fill with a default date to avoid losing rows\n",
        "        crime_df_copy['date'] = crime_df_copy['date'].fillna(pd.Timestamp('2000-01-01'))\n",
        "\n",
        "    # Extract basic time components\n",
        "    crime_df_copy['hour'] = crime_df_copy.date.dt.hour\n",
        "    crime_df_copy['day'] = crime_df_copy.date.dt.day\n",
        "    crime_df_copy['day_of_week'] = crime_df_copy.date.dt.dayofweek\n",
        "    crime_df_copy['month'] = crime_df_copy.date.dt.month\n",
        "    crime_df_copy['year'] = crime_df_copy.date.dt.year\n",
        "    crime_df_copy['quarter'] = crime_df_copy.date.dt.quarter\n",
        "    crime_df_copy['month_year'] = crime_df_copy.date.dt.to_period('M')\n",
        "\n",
        "    # Time of day categories\n",
        "    bins = [0, 6, 12, 18, 24]\n",
        "    labels = ['Night', 'Morning', 'Afternoon', 'Evening']\n",
        "    crime_df_copy['time_of_day'] = pd.cut(\n",
        "        crime_df_copy['hour'],\n",
        "        bins=bins,\n",
        "        labels=labels,\n",
        "        include_lowest=True\n",
        "    )\n",
        "\n",
        "    # Is weekend\n",
        "    crime_df_copy['is_weekend'] = (crime_df_copy.day_of_week >= 5).astype(int)\n",
        "\n",
        "    # Is holiday (simplified - just major US holidays)\n",
        "    holidays = [\n",
        "        # New Year's Day\n",
        "        (1, 1),\n",
        "        # Memorial Day (last Monday in May) - approximation\n",
        "        (5, 31),\n",
        "        # Independence Day\n",
        "        (7, 4),\n",
        "        # Labor Day (first Monday in September) - approximation\n",
        "        (9, 1),\n",
        "        # Thanksgiving (fourth Thursday in November) - approximation\n",
        "        (11, 28),\n",
        "        # Christmas\n",
        "        (12, 25)\n",
        "    ]\n",
        "\n",
        "    crime_df_copy['is_holiday'] = 0\n",
        "    for month, day in holidays:\n",
        "        holiday_mask = (crime_df_copy.month == month) & (crime_df_copy.day == day)\n",
        "        crime_df_copy.loc[holiday_mask, 'is_holiday'] = 1\n",
        "\n",
        "    # Create season indicators\n",
        "    crime_df_copy['is_winter'] = ((crime_df_copy.month == 12) | (crime_df_copy.month <= 2)).astype(int)\n",
        "    crime_df_copy['is_spring'] = ((crime_df_copy.month >= 3) & (crime_df_copy.month <= 5)).astype(int)\n",
        "    crime_df_copy['is_summer'] = ((crime_df_copy.month >= 6) & (crime_df_copy.month <= 8)).astype(int)\n",
        "    crime_df_copy['is_fall'] = ((crime_df_copy.month >= 9) & (crime_df_copy.month <= 11)).astype(int)\n",
        "\n",
        "    log(\"Created temporal features\")\n",
        "\n",
        "    return crime_df_copy\n",
        "\n",
        "def create_spatial_features(crime_df, comm_df):\n",
        "    \"\"\"Create spatial features with row-by-row calculation for safety\"\"\"\n",
        "    log(\"Creating spatial features with individual row processing\")\n",
        "\n",
        "    # Make copies\n",
        "    crime_df_copy = crime_df.copy()\n",
        "    comm_df_copy = comm_df.copy()\n",
        "\n",
        "    # Initialize columns\n",
        "    if 'geometry' not in comm_df_copy.columns:\n",
        "        comm_df_copy['geometry'] = None\n",
        "    comm_df_copy['center_lat'] = 0.0\n",
        "    comm_df_copy['center_lon'] = 0.0\n",
        "    crime_df_copy['dist_to_downtown'] = 0.0\n",
        "\n",
        "    log(\"Created basic spatial features\")\n",
        "\n",
        "    # Calculate distance features if coordinates exist\n",
        "    if all(col in crime_df_copy.columns for col in ['latitude', 'longitude']):\n",
        "        try:\n",
        "            # Convert coordinates to numeric\n",
        "            crime_df_copy['latitude'] = pd.to_numeric(crime_df_copy['latitude'], errors='coerce')\n",
        "            crime_df_copy['longitude'] = pd.to_numeric(crime_df_copy['longitude'], errors='coerce')\n",
        "\n",
        "            # Downtown coordinates\n",
        "            downtown_lat, downtown_lon = 41.8781, -87.6298\n",
        "\n",
        "            # Create distance function that accepts scalar values\n",
        "            def haversine_distance(lat, lon):\n",
        "                \"\"\"Calculate distance from a point to downtown\"\"\"\n",
        "                # Skip invalid points\n",
        "                if pd.isna(lat) or pd.isna(lon):\n",
        "                    return np.nan\n",
        "\n",
        "                # Convert to radians - one point at a time\n",
        "                lat1, lon1 = np.radians(float(lat)), np.radians(float(lon))\n",
        "                lat2, lon2 = np.radians(downtown_lat), np.radians(downtown_lon)\n",
        "\n",
        "                # Haversine formula with scalar values\n",
        "                dlat = lat2 - lat1\n",
        "                dlon = lon2 - lon1\n",
        "                a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "                c = 2 * np.arcsin(np.sqrt(a))\n",
        "                r = 6371  # Radius of Earth in kilometers\n",
        "                return c * r\n",
        "\n",
        "            # CRITICAL FIX: Process row by row instead of vectorized\n",
        "            valid_rows = crime_df_copy.dropna(subset=['latitude', 'longitude'])\n",
        "            log(f\"Processing {len(valid_rows)} rows with valid coordinates\")\n",
        "\n",
        "            # Create a new Series for distances\n",
        "            distances = pd.Series(index=valid_rows.index)\n",
        "\n",
        "            # Calculate for each row individually - no vectorized operations\n",
        "            for idx, row in valid_rows.iterrows():\n",
        "                try:\n",
        "                    dist = haversine_distance(row['latitude'], row['longitude'])\n",
        "                    distances.loc[idx] = dist\n",
        "                except Exception:\n",
        "                    distances.loc[idx] = np.nan\n",
        "\n",
        "            # Merge back to main DataFrame\n",
        "            crime_df_copy.loc[distances.index, 'dist_to_downtown'] = distances\n",
        "\n",
        "            log(f\"Successfully calculated {distances.notna().sum()} distances to downtown\")\n",
        "\n",
        "            # Calculate community area stats\n",
        "            if 'community_area' in crime_df_copy.columns:\n",
        "                try:\n",
        "                    # Get valid data subset\n",
        "                    valid_data = crime_df_copy.dropna(subset=['dist_to_downtown', 'community_area'])\n",
        "\n",
        "                    # Group by community area safely\n",
        "                    if len(valid_data) > 0:\n",
        "                        # Find min distance to downtown\n",
        "                        area_distances = valid_data.groupby('community_area')['dist_to_downtown'].min()\n",
        "\n",
        "                        # Convert to DataFrame with reset index\n",
        "                        min_dist_df = area_distances.reset_index()\n",
        "                        min_dist_df.columns = ['community_area', 'min_dist_to_downtown']\n",
        "\n",
        "                        # Join safely\n",
        "                        comm_df_copy = pd.merge(\n",
        "                            comm_df_copy,\n",
        "                            min_dist_df,\n",
        "                            on='community_area',\n",
        "                            how='left'\n",
        "                        )\n",
        "\n",
        "                        # Fill missing values\n",
        "                        comm_df_copy['min_dist_to_downtown'] = comm_df_copy['min_dist_to_downtown'].fillna(0)\n",
        "\n",
        "                        log(\"Added minimum distance to downtown by community area\")\n",
        "\n",
        "                        # Calculate centroids\n",
        "                        centroid_data = valid_data.groupby('community_area').agg({\n",
        "                            'latitude': 'mean',\n",
        "                            'longitude': 'mean'\n",
        "                        }).reset_index()\n",
        "\n",
        "                        # Rename for clarity\n",
        "                        centroid_data.rename(columns={\n",
        "                            'latitude': 'center_lat',\n",
        "                            'longitude': 'center_lon'\n",
        "                        }, inplace=True)\n",
        "\n",
        "                        # Join with community data\n",
        "                        comm_df_copy = pd.merge(\n",
        "                            comm_df_copy,\n",
        "                            centroid_data,\n",
        "                            on='community_area',\n",
        "                            how='left'\n",
        "                        )\n",
        "\n",
        "                        # Fill missing values\n",
        "                        for col in ['center_lat', 'center_lon']:\n",
        "                            if col in comm_df_copy.columns:\n",
        "                                comm_df_copy[col] = comm_df_copy[col].fillna(0)\n",
        "\n",
        "                        log(\"Added community area centroids\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error in community area processing: {str(e)}\")\n",
        "                    # Ensure columns exist with defaults\n",
        "                    if 'min_dist_to_downtown' not in comm_df_copy.columns:\n",
        "                        comm_df_copy['min_dist_to_downtown'] = 0\n",
        "        except Exception as e:\n",
        "            log(f\"Error creating distance features: {str(e)}\")\n",
        "\n",
        "    return crime_df_copy, comm_df_copy\n",
        "\n",
        "def create_panel_data(crime_df, community_areas, time_range):\n",
        "    \"\"\"\n",
        "    Create panel data structure (area × month) for time series prediction\n",
        "    Returns a dataframe with rows for each community area and month\n",
        "    \"\"\"\n",
        "    log(\"Creating panel data structure (area × month)\")\n",
        "\n",
        "    if 'date' not in crime_df.columns or 'community_area' not in crime_df.columns:\n",
        "        log(\"Required columns missing for panel data creation\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Convert date and ensure community_area is string\n",
        "        crime_df['date'] = pd.to_datetime(crime_df['date'], errors='coerce')\n",
        "        crime_df['community_area'] = crime_df['community_area'].astype(str)\n",
        "\n",
        "        # Create month-year column if not already present\n",
        "        if 'month_year' not in crime_df.columns:\n",
        "            crime_df['month_year'] = crime_df['date'].dt.to_period('M')\n",
        "\n",
        "        # Filter to time range years\n",
        "        crime_df['year'] = crime_df['date'].dt.year\n",
        "        years_filter = crime_df['year'].isin(time_range)\n",
        "        crime_filtered = crime_df[years_filter].copy()\n",
        "\n",
        "        # Count crimes by community area and month\n",
        "        crime_counts = crime_filtered.groupby(['community_area', 'month_year']).size().reset_index(name='crime_count')\n",
        "\n",
        "        # Create a complete panel with all area-month combinations\n",
        "        # Generate all months in the time range\n",
        "        start_date = pd.Period(f\"{min(time_range)}-01\", freq='M')\n",
        "        end_date = pd.Period(f\"{max(time_range)}-12\", freq='M')\n",
        "        all_months = pd.period_range(start=start_date, end=end_date, freq='M')\n",
        "\n",
        "        # Create cross product of all areas and all months\n",
        "        panel_index = pd.MultiIndex.from_product(\n",
        "            [community_areas, all_months],\n",
        "            names=['community_area', 'month_year']\n",
        "        )\n",
        "        panel_df = pd.DataFrame(index=panel_index).reset_index()\n",
        "\n",
        "        # Merge with crime counts\n",
        "        panel_df = pd.merge(\n",
        "            panel_df,\n",
        "            crime_counts,\n",
        "            on=['community_area', 'month_year'],\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Fill missing crime counts with 0\n",
        "        panel_df['crime_count'] = panel_df['crime_count'].fillna(0)\n",
        "\n",
        "        # Extract year and month from month_year for easier handling\n",
        "        panel_df['year'] = panel_df['month_year'].dt.year\n",
        "        panel_df['month'] = panel_df['month_year'].dt.month\n",
        "\n",
        "        # Sort data for proper lag creation\n",
        "        panel_df = panel_df.sort_values(['community_area', 'month_year'])\n",
        "\n",
        "        # Create lag features (previous 1, 2, 3 months)\n",
        "        # Group by community area and create lag features\n",
        "        for lag in range(1, 4):\n",
        "            panel_df[f'crime_count_lag{lag}'] = panel_df.groupby('community_area')['crime_count'].shift(lag)\n",
        "\n",
        "        # Create rolling statistics (mean, std, min, max over last 3 months)\n",
        "        # These are properly lagged to avoid leakage\n",
        "        panel_df['rolling_mean_3m'] = panel_df.groupby('community_area')['crime_count'].shift(1).rolling(window=3, min_periods=1).mean()\n",
        "        panel_df['rolling_std_3m'] = panel_df.groupby('community_area')['crime_count'].shift(1).rolling(window=3, min_periods=1).std().fillna(0)\n",
        "        panel_df['rolling_min_3m'] = panel_df.groupby('community_area')['crime_count'].shift(1).rolling(window=3, min_periods=1).min()\n",
        "        panel_df['rolling_max_3m'] = panel_df.groupby('community_area')['crime_count'].shift(1).rolling(window=3, min_periods=1).max()\n",
        "\n",
        "        # Create year-over-year change (same month last year)\n",
        "        panel_df['yoy_change'] = panel_df['crime_count'] - panel_df.groupby(['community_area', 'month'])['crime_count'].shift(12)\n",
        "\n",
        "        # Create month-of-year indicators for seasonality\n",
        "        for month in range(1, 13):\n",
        "            panel_df[f'month_{month}'] = (panel_df['month'] == month).astype(int)\n",
        "\n",
        "        # Create quarter indicators\n",
        "        for quarter in range(1, 5):\n",
        "            start_month = (quarter - 1) * 3 + 1\n",
        "            end_month = quarter * 3\n",
        "            panel_df[f'quarter_{quarter}'] = ((panel_df['month'] >= start_month) & (panel_df['month'] <= end_month)).astype(int)\n",
        "\n",
        "        # Check for NaNs in lagged features\n",
        "        lag_cols = [col for col in panel_df.columns if 'lag' in col or 'rolling' in col]\n",
        "        for col in lag_cols:\n",
        "            if col in panel_df.columns and panel_df[col].isna().sum() > 0:\n",
        "                log(f\"  Column {col} has {panel_df[col].isna().sum()} missing values\")\n",
        "                # Fill missing values with forward fill, then backward fill\n",
        "                panel_df[col] = panel_df[col].fillna(method='ffill')\n",
        "                panel_df[col] = panel_df[col].fillna(method='bfill')\n",
        "                panel_df[col] = panel_df[col].fillna(0)  # Last resort, fill with 0\n",
        "\n",
        "        log(f\"Created panel dataset with shape {panel_df.shape}\")\n",
        "        log(f\"Panel data missing values: {panel_df.isna().sum().sum()}\")\n",
        "\n",
        "        return panel_df\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error creating panel data: {str(e)}\")\n",
        "        # Create minimal panel data as fallback\n",
        "        try:\n",
        "            # Generate a simple panel with just community areas and months\n",
        "            simple_panel = []\n",
        "            for area in community_areas:\n",
        "                for year in time_range:\n",
        "                    for month in range(1, 13):\n",
        "                        simple_panel.append({\n",
        "                            'community_area': str(area),\n",
        "                            'year': year,\n",
        "                            'month': month,\n",
        "                            'crime_count': 0  # Default value\n",
        "                        })\n",
        "            fallback_df = pd.DataFrame(simple_panel)\n",
        "            log(f\"Created fallback panel data with shape {fallback_df.shape}\")\n",
        "            return fallback_df\n",
        "        except:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "def create_crime_heatmap_safe(crime_df, comm_df, viz_dir):\n",
        "    \"\"\"Create a simplified folium heatmap with maximum error protection\"\"\"\n",
        "    if not FOLIUM_AVAILABLE:\n",
        "        log(\"Folium not available for heatmap visualization\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        log(\"Creating simplified crime heatmap with maximum safeguards\")\n",
        "\n",
        "        # Ensure we have coordinates\n",
        "        if 'latitude' not in crime_df.columns or 'longitude' not in crime_df.columns:\n",
        "            log(\"Latitude/longitude columns required for heatmap visualization\")\n",
        "            return False\n",
        "\n",
        "        # Extract coordinates using numpy arrays to avoid pandas operations\n",
        "        try:\n",
        "            # Convert columns to numeric\n",
        "            lat_values = pd.to_numeric(crime_df['latitude'], errors='coerce').dropna().values\n",
        "            lon_values = pd.to_numeric(crime_df['longitude'], errors='coerce').dropna().values\n",
        "\n",
        "            # Get only matching length of valid coordinates\n",
        "            min_length = min(len(lat_values), len(lon_values))\n",
        "            if min_length < 10:\n",
        "                log(\"Insufficient valid coordinates\")\n",
        "                return False\n",
        "\n",
        "            lat_values = lat_values[:min_length]\n",
        "            lon_values = lon_values[:min_length]\n",
        "\n",
        "            log(f\"Extracted {min_length} valid coordinate pairs\")\n",
        "\n",
        "            # Sample if needed\n",
        "            if min_length > 50000:\n",
        "                indices = np.random.choice(min_length, 50000, replace=False)\n",
        "                lat_values = lat_values[indices]\n",
        "                lon_values = lon_values[indices]\n",
        "                log(f\"Sampled to 50,000 coordinates\")\n",
        "        except Exception as e:\n",
        "            log(f\"Error extracting coordinates: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "        # Create coordinate list manually without any string operations\n",
        "        try:\n",
        "            # Manually build list of coordinates\n",
        "            heat_data = []\n",
        "            for i in range(len(lat_values)):\n",
        "                lat = float(lat_values[i])  # Explicit Python float conversion\n",
        "                lon = float(lon_values[i])\n",
        "\n",
        "                # Basic validation\n",
        "                if -90 <= lat <= 90 and -180 <= lon <= 180:\n",
        "                    heat_data.append([lat, lon])\n",
        "\n",
        "            if len(heat_data) < 10:\n",
        "                log(\"Too few valid coordinates after filtering\")\n",
        "                return False\n",
        "\n",
        "            log(f\"Created list with {len(heat_data)} valid coordinates\")\n",
        "        except Exception as e:\n",
        "            log(f\"Error creating coordinate list: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "        # Create and save map - with minimal folium operations\n",
        "        try:\n",
        "            # Create map with simplest possible syntax\n",
        "            m = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
        "\n",
        "            # Add heatmap layer directly\n",
        "            HeatMap(data=heat_data).add_to(m)\n",
        "\n",
        "            # Save map\n",
        "            m.save(f\"{viz_dir}/crime_heatmap.html\")\n",
        "            log(\"Successfully saved simplified heatmap\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            log(f\"Error in map creation/saving: {str(e)}, {type(e)}\")\n",
        "\n",
        "            # Debug - print sample of heat_data\n",
        "            if len(heat_data) > 0:\n",
        "                log(f\"Sample heat_data: {heat_data[0]}, Type: {type(heat_data[0])}\")\n",
        "                if len(heat_data[0]) > 0:\n",
        "                    log(f\"Sample lat: {heat_data[0][0]}, Type: {type(heat_data[0][0])}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Outer error in heatmap creation: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def filter_highly_correlated_features_safe(df, target_col, threshold=0.8):\n",
        "    \"\"\"Handle correlation calculation with duplicate index protection\"\"\"\n",
        "    log(f\"Identifying features with correlation > {threshold} (safe implementation)\")\n",
        "\n",
        "    # Make a copy for imputation and reset index to avoid duplicates\n",
        "    df_imputed = df.copy().reset_index(drop=True)\n",
        "\n",
        "    # Get only numeric columns for correlation analysis\n",
        "    numeric_cols = df_imputed.select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "    # Check if target is in numeric columns\n",
        "    if target_col not in numeric_cols:\n",
        "        log(f\"Target column '{target_col}' is not numeric or not in dataframe\")\n",
        "        return df.columns.tolist()  # Return all columns as fallback\n",
        "\n",
        "    # Initialize feature correlations and features to drop\n",
        "    feature_correlations = {}\n",
        "    features_to_drop = set()\n",
        "\n",
        "    # Calculate correlation with target for each numeric feature\n",
        "    for col in numeric_cols:\n",
        "        if col == target_col:\n",
        "            continue\n",
        "\n",
        "        # Safely calculate correlation\n",
        "        try:\n",
        "            # Get non-null values in both columns\n",
        "            mask = df_imputed[col].notna() & df_imputed[target_col].notna()\n",
        "            if mask.sum() > 10:  # Only calculate if we have enough valid data points\n",
        "                correlation = np.corrcoef(df_imputed[col][mask], df_imputed[target_col][mask])[0, 1]\n",
        "                feature_correlations[col] = abs(correlation)\n",
        "\n",
        "                # Check if highly correlated with target\n",
        "                if abs(correlation) > threshold:\n",
        "                    log(f\"Feature highly correlated with {target_col}: {col} (corr={abs(correlation):.3f})\")\n",
        "                    features_to_drop.add(col)  # Drop features too correlated with target\n",
        "        except Exception as e:\n",
        "            log(f\"  Error calculating correlation between {col} and {target_col}: {str(e)}\")\n",
        "\n",
        "    # Find high correlations between features\n",
        "    for i, col1 in enumerate(numeric_cols):\n",
        "        if col1 == target_col or col1 in features_to_drop:\n",
        "            continue\n",
        "\n",
        "        for j, col2 in enumerate(numeric_cols[i+1:], i+1):\n",
        "            if col2 == target_col or col2 in features_to_drop:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Get non-null values in both columns\n",
        "                mask = df_imputed[col1].notna() & df_imputed[col2].notna()\n",
        "                if mask.sum() > 10:  # Only calculate if we have enough valid data points\n",
        "                    correlation = abs(np.corrcoef(df_imputed[col1][mask], df_imputed[col2][mask])[0, 1])\n",
        "\n",
        "                    if correlation > threshold:\n",
        "                        # Determine which feature to keep based on correlation with target\n",
        "                        if col1 in feature_correlations and col2 in feature_correlations:\n",
        "                            if feature_correlations[col1] > feature_correlations[col2]:\n",
        "                                features_to_drop.add(col2)\n",
        "                                log(f\"Excluding {col2} (corr={correlation:.3f}) in favor of {col1} (corr={feature_correlations[col1]:.3f})\")\n",
        "                            else:\n",
        "                                features_to_drop.add(col1)\n",
        "                                log(f\"Excluding {col1} (corr={correlation:.3f}) in favor of {col2} (corr={feature_correlations[col2]:.3f})\")\n",
        "                                break  # Break inner loop since col1 is dropped\n",
        "            except Exception as e:\n",
        "                log(f\"  Error checking correlation between {col1} and {col2}: {str(e)}\")\n",
        "\n",
        "    # Get final feature list\n",
        "    features_to_keep = [col for col in df.columns if col not in features_to_drop or col == target_col]\n",
        "\n",
        "    log(f\"Excluded {len(features_to_drop)} features with high correlation\")\n",
        "    log(f\"Keeping {len(features_to_keep)} features\")\n",
        "\n",
        "    return features_to_keep\n",
        "\n",
        "def select_features(X, y, max_features=30):\n",
        "    \"\"\"Select the most important features using multiple methods\"\"\"\n",
        "\n",
        "    log(f\"Selecting up to {max_features} important features\")\n",
        "\n",
        "    # First impute any missing values\n",
        "    X_imputed = X.copy()\n",
        "    for col in X_imputed.columns:\n",
        "        # Check data type of the column, not the DataFrame\n",
        "        if pd.api.types.is_numeric_dtype(X_imputed[col]) and X_imputed[col].isna().any():\n",
        "            X_imputed[col] = X_imputed[col].fillna(X_imputed[col].median())\n",
        "\n",
        "    # Start with removing low variance features\n",
        "    try:\n",
        "        var_selector = VarianceThreshold(threshold=0.01)\n",
        "        X_var = var_selector.fit_transform(X_imputed)\n",
        "        var_features = X_imputed.columns[var_selector.get_support()].tolist()\n",
        "        log(f\"Selected {len(var_features)} features with variance > 0.01\")\n",
        "\n",
        "        if len(var_features) <= max_features:\n",
        "            return var_features\n",
        "\n",
        "        # If we still have too many features, use a tree-based selector\n",
        "        X_var_df = X_imputed[var_features]\n",
        "\n",
        "        # Use a HistGradientBoostingRegressor which handles missing values natively\n",
        "        gb = HistGradientBoostingRegressor(max_iter=100, random_state=42)\n",
        "        gb.fit(X_var_df, y)\n",
        "\n",
        "        # Get feature importances\n",
        "        if hasattr(gb, 'feature_importances_'):\n",
        "            importances = gb.feature_importances_\n",
        "        else:\n",
        "            # IMPROVED FIX: Better fallback for feature importance\n",
        "            try:\n",
        "                # Create a standard GradientBoostingRegressor as fallback\n",
        "                gb_fallback = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "                gb_fallback.fit(X_var_df, y)\n",
        "                importances = gb_fallback.feature_importances_\n",
        "                log(\"Using fallback GradientBoostingRegressor for feature importances\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error in fallback feature importance: {str(e)}\")\n",
        "                # For models without feature_importances_, assign equal importance\n",
        "                importances = np.ones(X_var_df.shape[1]) / X_var_df.shape[1]\n",
        "\n",
        "        # Select top features\n",
        "        top_indices = np.argsort(importances)[::-1][:max_features]\n",
        "        selected_features = [var_features[i] for i in top_indices]\n",
        "\n",
        "        log(f\"Selected {len(selected_features)} features by importance\")\n",
        "\n",
        "        return selected_features\n",
        "    except Exception as e:\n",
        "        log(f\"Error in feature selection: {str(e)}\")\n",
        "        # Return all columns if there's an error\n",
        "        return X.columns.tolist()[:max_features]\n",
        "\n",
        "def perform_fairness_analysis(y_test, predictions, protected_attrs, viz_dir):\n",
        "    \"\"\"Perform fairness analysis using protected attributes\"\"\"\n",
        "    log(\"Conducting fairness analysis\")\n",
        "\n",
        "    if not protected_attrs:\n",
        "        log(\"No protected attributes available for fairness analysis\")\n",
        "        return {}\n",
        "\n",
        "    fairness_results = {}\n",
        "\n",
        "    # Check if Fairlearn is available\n",
        "    fairlearn_available = False\n",
        "    try:\n",
        "        from fairlearn.metrics import MetricFrame\n",
        "        fairlearn_available = True\n",
        "    except ImportError:\n",
        "        log(\"Fairlearn not available - using basic fairness metrics instead\")\n",
        "\n",
        "    # Define metrics - compatible with both Fairlearn and basic implementation\n",
        "    metrics_dict = {\n",
        "        'rmse': lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "        'mae': mean_absolute_error,\n",
        "        'mean_prediction': lambda y_true, y_pred: np.mean(y_pred),\n",
        "        'median_prediction': lambda y_true, y_pred: np.median(y_pred)\n",
        "    }\n",
        "\n",
        "    for attr_name, attr_values in protected_attrs.items():\n",
        "        log(f\"Fairness analysis for {attr_name}\")\n",
        "\n",
        "        # Handle quantile-based attributes consistently\n",
        "        if np.issubdtype(attr_values.dtype, np.number) and attr_values.nunique() > 2:\n",
        "            # For numeric attributes, create quartiles\n",
        "            try:\n",
        "                quartiles = pd.qcut(attr_values, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "                # Convert to string to avoid categorical dtype issues\n",
        "                quartiles = quartiles.astype(str)\n",
        "            except ValueError:  # Handle case with duplicate values\n",
        "                quartiles = pd.qcut(attr_values, 4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')\n",
        "                quartiles = quartiles.astype(str)\n",
        "\n",
        "            # Analyze by quartile\n",
        "            results_by_quartile = {}\n",
        "\n",
        "            # Iterate through quartiles\n",
        "            for quartile in quartiles.unique():\n",
        "                mask = quartiles == quartile\n",
        "                quartile_y = y_test[mask]\n",
        "                quartile_pred = predictions[mask]\n",
        "\n",
        "                if len(quartile_y) > 0:\n",
        "                    quartile_results = {}\n",
        "                    for metric_name, metric_func in metrics_dict.items():\n",
        "                        try:\n",
        "                            quartile_results[metric_name] = metric_func(quartile_y, quartile_pred)\n",
        "                        except Exception as e:\n",
        "                            log(f\"  Error calculating {metric_name} for {quartile}: {str(e)}\")\n",
        "                            quartile_results[metric_name] = np.nan\n",
        "\n",
        "                    results_by_quartile[quartile] = quartile_results\n",
        "\n",
        "                    # Log results\n",
        "                    log(f\"  {attr_name} {quartile}:\")\n",
        "                    for metric_name, value in quartile_results.items():\n",
        "                        if not np.isnan(value):\n",
        "                            log(f\"    {metric_name}: {value:.3f}\")\n",
        "\n",
        "            # Calculate disparities across quartiles\n",
        "            disparities = {}\n",
        "            for metric_name in metrics_dict.keys():\n",
        "                try:\n",
        "                    metric_values = [results[metric_name] for results in results_by_quartile.values()\n",
        "                                    if not np.isnan(results.get(metric_name, np.nan))]\n",
        "                    if metric_values:\n",
        "                        min_val = min(metric_values)\n",
        "                        max_val = max(metric_values)\n",
        "                        disparities[metric_name] = max_val - min_val\n",
        "                        log(f\"  {metric_name} disparity across quartiles: {disparities[metric_name]:.3f}\")\n",
        "                except Exception as e:\n",
        "                    log(f\"  Error calculating {metric_name} disparity: {str(e)}\")\n",
        "\n",
        "            fairness_results[attr_name] = {\n",
        "                'quartile_results': results_by_quartile,\n",
        "                'disparities': disparities\n",
        "            }\n",
        "\n",
        "            # Visualize by quartile\n",
        "            try:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                metric_names = list(metrics_dict.keys())\n",
        "                quartiles = list(results_by_quartile.keys())\n",
        "\n",
        "                x = np.arange(len(metric_names))\n",
        "                width = 0.2\n",
        "\n",
        "                # Plot bars for each quartile\n",
        "                for i, quartile in enumerate(quartiles):\n",
        "                    values = []\n",
        "                    for metric in metric_names:\n",
        "                        if metric in results_by_quartile[quartile] and not np.isnan(results_by_quartile[quartile][metric]):\n",
        "                            values.append(results_by_quartile[quartile][metric])\n",
        "                        else:\n",
        "                            values.append(0)  # Default for missing values\n",
        "                    plt.bar(x + i*width, values, width, label=f'{quartile}')\n",
        "\n",
        "                plt.xlabel('Metric')\n",
        "                plt.ylabel('Value')\n",
        "                plt.title(f'Fairness Analysis for {attr_name} by Quartile')\n",
        "                plt.xticks(x + width*1.5, metric_names)\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{viz_dir}/fairness_{attr_name}_quartiles.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                log(f\"Error creating quartiles visualization: {str(e)}\")\n",
        "\n",
        "        # Handle categorical attributes (for hardship_quartile)\n",
        "        elif isinstance(attr_values.dtype, pd.CategoricalDtype) or attr_values.dtype == 'object':\n",
        "            # Handle categorical variables by converting to string\n",
        "            cat_values = attr_values.astype(str)\n",
        "            categories = sorted(cat_values.unique())\n",
        "\n",
        "            # Analyze by category\n",
        "            results_by_category = {}\n",
        "            for category in categories:\n",
        "                mask = cat_values == category\n",
        "                cat_y = y_test[mask]\n",
        "                cat_pred = predictions[mask]\n",
        "\n",
        "                if len(cat_y) > 0:\n",
        "                    cat_results = {}\n",
        "                    for metric_name, metric_func in metrics_dict.items():\n",
        "                        try:\n",
        "                            cat_results[metric_name] = metric_func(cat_y, cat_pred)\n",
        "                        except Exception as e:\n",
        "                            log(f\"  Error calculating {metric_name} for {category}: {str(e)}\")\n",
        "                            cat_results[metric_name] = np.nan\n",
        "\n",
        "                    results_by_category[category] = cat_results\n",
        "\n",
        "                    # Log results\n",
        "                    log(f\"  {attr_name}={category}:\")\n",
        "                    for metric_name, value in cat_results.items():\n",
        "                        if not np.isnan(value):\n",
        "                            log(f\"    {metric_name}: {value:.3f}\")\n",
        "\n",
        "            # Calculate disparities\n",
        "            disparities = {}\n",
        "            for metric_name in metrics_dict.keys():\n",
        "                try:\n",
        "                    metric_values = [results[metric_name] for results in results_by_category.values()\n",
        "                                    if not np.isnan(results.get(metric_name, np.nan))]\n",
        "                    if metric_values:\n",
        "                        min_val = min(metric_values)\n",
        "                        max_val = max(metric_values)\n",
        "                        disparities[metric_name] = max_val - min_val\n",
        "                        log(f\"  {metric_name} disparity: {disparities[metric_name]:.3f}\")\n",
        "                except Exception as e:\n",
        "                    log(f\"  Error calculating {metric_name} disparity: {str(e)}\")\n",
        "\n",
        "            fairness_results[attr_name] = {\n",
        "                'category_results': results_by_category,\n",
        "                'disparities': disparities\n",
        "            }\n",
        "\n",
        "            # Visualize\n",
        "            try:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                metric_names = list(metrics_dict.keys())\n",
        "                categories = list(results_by_category.keys())\n",
        "\n",
        "                x = np.arange(len(metric_names))\n",
        "                width = 0.8 / len(categories)\n",
        "\n",
        "                # Plot bars for each category\n",
        "                for i, category in enumerate(categories):\n",
        "                    values = []\n",
        "                    for metric in metric_names:\n",
        "                        if metric in results_by_category[category] and not np.isnan(results_by_category[category][metric]):\n",
        "                            values.append(results_by_category[category][metric])\n",
        "                        else:\n",
        "                            values.append(0)  # Default for missing values\n",
        "                    plt.bar(x + i*width - 0.4 + width/2, values, width, label=f'{category}')\n",
        "\n",
        "                plt.xlabel('Metric')\n",
        "                plt.ylabel('Value')\n",
        "                plt.title(f'Fairness Analysis for {attr_name}')\n",
        "                plt.xticks(x, metric_names)\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{viz_dir}/fairness_{attr_name}.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                log(f\"Error creating categories visualization: {str(e)}\")\n",
        "\n",
        "        # Use Fairlearn for more detailed analysis if available\n",
        "        elif fairlearn_available:\n",
        "            try:\n",
        "                # Convert to string to avoid dtype issues\n",
        "                attr_series = pd.Series(attr_values, index=y_test.index).astype(str)\n",
        "\n",
        "                mf = MetricFrame(\n",
        "                    metrics=metrics_dict,\n",
        "                    y_true=y_test,\n",
        "                    y_pred=predictions,\n",
        "                    sensitive_features=attr_series\n",
        "                )\n",
        "\n",
        "                # Log metrics by group\n",
        "                log(f\"Metrics by {attr_name} group:\")\n",
        "                for group, metrics_dict in mf.by_group.iterrows():\n",
        "                    for metric, value in metrics_dict.items():\n",
        "                        log(f\"  {attr_name}={group}, {metric}: {value:.3f}\")\n",
        "\n",
        "                # Calculate disparities\n",
        "                disparities = {}\n",
        "                for metric in metrics_dict:\n",
        "                    group_values = mf.by_group[metric]\n",
        "                    if len(group_values) >= 2:\n",
        "                        min_val = group_values.min()\n",
        "                        max_val = group_values.max()\n",
        "                        diff = max_val - min_val\n",
        "                        disparities[metric] = diff\n",
        "                        log(f\"  {metric} disparity: {diff:.3f}\")\n",
        "\n",
        "                fairness_results[attr_name] = disparities\n",
        "\n",
        "                # Visualize fairness metrics\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                metric_names = list(metrics_dict.keys())\n",
        "                group_values = list(mf.by_group.index)\n",
        "\n",
        "                x = np.arange(len(metric_names))\n",
        "                width = 0.35\n",
        "\n",
        "                # Plot bars for each group\n",
        "                for i, group in enumerate(group_values):\n",
        "                    values = [mf.by_group.loc[group, metric] for metric in metric_names]\n",
        "                    plt.bar(x + i*width, values, width, label=f'{attr_name}={group}')\n",
        "\n",
        "                plt.xlabel('Metric')\n",
        "                plt.ylabel('Value')\n",
        "                plt.title(f'Fairness Analysis for {attr_name}')\n",
        "                plt.xticks(x + width/2, metric_names)\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{viz_dir}/fairness_{attr_name}.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                log(f\"Error in Fairlearn fairness analysis: {str(e)}\")\n",
        "        else:\n",
        "            log(\"Neither Fairlearn nor categorical/quantile analysis applicable. Using basic analysis.\")\n",
        "            # Basic approach for binary or simple attributes\n",
        "            attr_str = attr_values.astype(str)\n",
        "            unique_groups = sorted(attr_str.unique())\n",
        "\n",
        "            # Basic analysis\n",
        "            results_by_group = {}\n",
        "            for group in unique_groups:\n",
        "                mask = attr_str == group\n",
        "                group_y = y_test[mask]\n",
        "                group_pred = predictions[mask]\n",
        "\n",
        "                if len(group_y) > 0:\n",
        "                    group_results = {}\n",
        "                    for metric_name, metric_func in metrics_dict.items():\n",
        "                        try:\n",
        "                            group_results[metric_name] = metric_func(group_y, group_pred)\n",
        "                        except Exception as e:\n",
        "                            group_results[metric_name] = np.nan\n",
        "                            log(f\"  Error calculating {metric_name} for group {group}: {str(e)}\")\n",
        "\n",
        "                    results_by_group[group] = group_results\n",
        "\n",
        "                    # Log results\n",
        "                    log(f\"  {attr_name}={group}:\")\n",
        "                    for metric_name, value in group_results.items():\n",
        "                        if not np.isnan(value):\n",
        "                            log(f\"    {metric_name}: {value:.3f}\")\n",
        "\n",
        "            # Calculate disparities\n",
        "            disparities = {}\n",
        "            for metric_name in metrics_dict.keys():\n",
        "                try:\n",
        "                    metric_values = [results.get(metric_name) for results in results_by_group.values()\n",
        "                                   if not np.isnan(results.get(metric_name, np.nan))]\n",
        "                    if metric_values:\n",
        "                        disparities[metric_name] = max(metric_values) - min(metric_values)\n",
        "                except Exception as e:\n",
        "                    log(f\"  Error calculating disparity for {metric_name}: {str(e)}\")\n",
        "\n",
        "            fairness_results[attr_name] = {\n",
        "                'group_results': results_by_group,\n",
        "                'disparities': disparities\n",
        "            }\n",
        "\n",
        "    return fairness_results\n",
        "\n",
        "def create_shap_visualizations(model, X_test, feature_names, viz_dir):\n",
        "    \"\"\"Create SHAP visualizations for model explainability\"\"\"\n",
        "    if not SHAP_AVAILABLE:\n",
        "        log(\"SHAP not available for model explainability\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        log(\"Creating SHAP visualizations for model explainability\")\n",
        "\n",
        "        # Ensure we have no NaN values\n",
        "        X_test_clean = X_test.copy()\n",
        "        for col in X_test_clean.columns:\n",
        "            if X_test_clean[col].isna().any():\n",
        "                median_val = X_test_clean[col].median()\n",
        "                X_test_clean[col] = X_test_clean[col].fillna(median_val)\n",
        "                log(f\"  Filled {X_test_clean[col].isna().sum()} NaN values in {col} with median\")\n",
        "\n",
        "        # Create SHAP explainer based on model type\n",
        "        if hasattr(model, 'predict'):\n",
        "            # For HistGradientBoostingRegressor, create a custom explainer\n",
        "            if isinstance(model, HistGradientBoostingRegressor):\n",
        "                log(\"Creating SHAP explainer for HistGradientBoostingRegressor\")\n",
        "\n",
        "                # Use a small sample for background data (faster)\n",
        "                sample_size = min(100, X_test_clean.shape[0])\n",
        "                background_data = X_test_clean.sample(sample_size, random_state=RAND_STATE)\n",
        "\n",
        "                try:\n",
        "                    # Try Explainer first (newer API)\n",
        "                    explainer = shap.Explainer(model.predict, background_data)\n",
        "                except Exception as e:\n",
        "                    log(f\"Error with shap.Explainer: {str(e)}. Trying KernelExplainer instead.\")\n",
        "                    explainer = shap.KernelExplainer(model.predict, background_data)\n",
        "            elif isinstance(model, GradientBoostingRegressor):\n",
        "                log(\"Creating TreeExplainer for GradientBoostingRegressor\")\n",
        "                try:\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                except Exception as e:\n",
        "                    log(f\"Error with TreeExplainer: {str(e)}. Trying KernelExplainer instead.\")\n",
        "                    sample_size = min(100, X_test_clean.shape[0])\n",
        "                    background_data = X_test_clean.sample(sample_size, random_state=RAND_STATE)\n",
        "                    explainer = shap.KernelExplainer(model.predict, background_data)\n",
        "            else:\n",
        "                # For other models\n",
        "                log(f\"Creating default SHAP explainer for {type(model).__name__}\")\n",
        "                sample_size = min(100, X_test_clean.shape[0])\n",
        "                background_data = X_test_clean.sample(sample_size, random_state=RAND_STATE)\n",
        "                explainer = shap.KernelExplainer(model.predict, background_data)\n",
        "\n",
        "            # Calculate SHAP values - use smaller sample for efficiency\n",
        "            sample_size = min(100, X_test_clean.shape[0])\n",
        "            data_sample = X_test_clean.sample(sample_size, random_state=RAND_STATE)\n",
        "\n",
        "            try:\n",
        "                # Try modern API first\n",
        "                shap_values = explainer(data_sample)\n",
        "                log(\"Generated SHAP values using modern API\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error calculating SHAP values with __call__: {str(e)}. Trying legacy API.\")\n",
        "                try:\n",
        "                    # Try legacy API as fallback\n",
        "                    shap_values = explainer.shap_values(data_sample)\n",
        "                    # Convert to shap.Explanation if needed\n",
        "                    if not isinstance(shap_values, shap.Explanation):\n",
        "                        shap_values = shap.Explanation(\n",
        "                            values=np.array(shap_values),\n",
        "                            base_values=np.array([explainer.expected_value] * len(data_sample)),\n",
        "                            data=data_sample.values,\n",
        "                            feature_names=feature_names\n",
        "                        )\n",
        "                    log(\"Generated SHAP values using legacy API\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error calculating SHAP values with legacy API: {str(e)}\")\n",
        "                    return None\n",
        "\n",
        "            # Create visualizations with proper error handling\n",
        "            try:\n",
        "                # Create summary plot (bar)\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                if isinstance(shap_values, shap.Explanation):\n",
        "                    shap.summary_plot(shap_values, data_sample, plot_type=\"bar\", feature_names=feature_names, show=False)\n",
        "                else:\n",
        "                    shap.summary_plot(shap_values, data_sample, plot_type=\"bar\", feature_names=feature_names, show=False)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{viz_dir}/shap_summary_bar.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                log(\"Created SHAP bar summary plot\")\n",
        "\n",
        "                # Create beeswarm plot for detailed feature impact\n",
        "                plt.figure(figsize=(12, 10))\n",
        "                if isinstance(shap_values, shap.Explanation):\n",
        "                    shap.summary_plot(shap_values, data_sample, feature_names=feature_names, show=False)\n",
        "                else:\n",
        "                    shap.summary_plot(shap_values, data_sample, feature_names=feature_names, show=False)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{viz_dir}/shap_summary_beeswarm.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                log(\"Created SHAP beeswarm plot\")\n",
        "\n",
        "                # Create dependence plots for top features\n",
        "                if isinstance(shap_values, shap.Explanation):\n",
        "                    vals = np.abs(shap_values.values).mean(0)\n",
        "                    top_indices = np.argsort(vals)[-3:]\n",
        "                else:\n",
        "                    if isinstance(shap_values, list):\n",
        "                        vals = np.abs(np.array(shap_values)).mean(0)\n",
        "                    else:\n",
        "                        vals = np.abs(shap_values).mean(0)\n",
        "                    top_indices = np.argsort(vals)[-3:]\n",
        "\n",
        "                for feature_idx in top_indices:\n",
        "                    if feature_idx >= len(feature_names):\n",
        "                        continue  # Skip if index out of bounds\n",
        "\n",
        "                    feature_name = feature_names[feature_idx]\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    try:\n",
        "                        if isinstance(shap_values, shap.Explanation):\n",
        "                            shap.dependence_plot(\n",
        "                                feature_idx, shap_values.values, data_sample,\n",
        "                                feature_names=feature_names, show=False\n",
        "                            )\n",
        "                        else:\n",
        "                            shap.dependence_plot(\n",
        "                                feature_idx, shap_values, data_sample,\n",
        "                                feature_names=feature_names, show=False\n",
        "                            )\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(f\"{viz_dir}/shap_dependence_{feature_name}.png\", dpi=200, bbox_inches='tight')\n",
        "                        plt.close()\n",
        "                        log(f\"Created SHAP dependence plot for {feature_name}\")\n",
        "                    except Exception as e:\n",
        "                        log(f\"Error creating dependence plot for {feature_name}: {str(e)}\")\n",
        "                        plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                log(f\"Error creating SHAP visualizations: {str(e)}\")\n",
        "\n",
        "            log(\"SHAP visualizations created successfully\")\n",
        "            return shap_values\n",
        "        else:\n",
        "            log(\"Model does not have predict method, cannot create SHAP visualizations\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        log(f\"Error creating SHAP visualizations: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def create_lime_explanations(model, X_train, X_test, feature_names, viz_dir, num_samples=5):\n",
        "    \"\"\"Create LIME explanations for model interpretability with proper version handling\"\"\"\n",
        "    if not LIME_AVAILABLE:\n",
        "        log(\"LIME not available for model interpretability\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log(\"Creating LIME explanations for sample predictions\")\n",
        "\n",
        "        # Ensure X_train and X_test don't have NaN values\n",
        "        X_train_clean = X_train.copy()\n",
        "        X_test_clean = X_test.copy()\n",
        "\n",
        "        for df in [X_train_clean, X_test_clean]:\n",
        "            for col in df.columns:\n",
        "                if df[col].isna().any():\n",
        "                    median_val = df[col].median()\n",
        "                    df[col] = df[col].fillna(median_val)\n",
        "\n",
        "        # Create LIME explainer\n",
        "        try:\n",
        "            explainer = lime_tabular.LimeTabularExplainer(\n",
        "                X_train_clean.values,\n",
        "                feature_names=feature_names,\n",
        "                mode='regression',\n",
        "                verbose=False\n",
        "            )\n",
        "\n",
        "            # Sample a few instances to explain\n",
        "            sample_indices = np.random.choice(X_test_clean.shape[0], min(num_samples, X_test_clean.shape[0]), replace=False)\n",
        "\n",
        "            for i, idx in enumerate(sample_indices):\n",
        "                try:\n",
        "                    # Get explanation for this instance\n",
        "                    exp = explainer.explain_instance(\n",
        "                        X_test_clean.iloc[idx].values,\n",
        "                        model.predict,\n",
        "                        num_features=10\n",
        "                    )\n",
        "\n",
        "                    # Create a figure and save it manually - WITHOUT using ax parameter\n",
        "                    # which causes compatibility issues\n",
        "                    try:\n",
        "                        # Try legacy method without passing ax parameter\n",
        "                        fig = plt.figure(figsize=(10, 6))\n",
        "                        exp.as_pyplot_figure()  # Do not pass ax parameter\n",
        "                        plt.title(f\"LIME Explanation for Instance {idx}\")\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(f\"{viz_dir}/lime_explanation_{i}.png\", dpi=200, bbox_inches='tight')\n",
        "                        plt.close()\n",
        "                    except Exception as viz_error:\n",
        "                        log(f\"Error visualizing LIME with pyplot: {str(viz_error)}\")\n",
        "                        # Fallback: Just save the explanation text\n",
        "                        explanation_text = \"\\n\".join([f\"{feature}: {weight:.4f}\" for feature, weight in exp.as_list()])\n",
        "                        with open(f\"{viz_dir}/lime_explanation_{i}.txt\", \"w\") as f:\n",
        "                            f.write(f\"LIME Explanation for Instance {idx}\\n\\n{explanation_text}\")\n",
        "\n",
        "                    # Log the explanation details\n",
        "                    log(f\"LIME explanation for instance {idx}:\")\n",
        "                    explanation_list = exp.as_list()\n",
        "                    for feature, weight in explanation_list:\n",
        "                        log(f\"  {feature}: {weight:.4f}\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error creating explanation for instance {idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            log(f\"Created LIME explanations for {len(sample_indices)} instances\")\n",
        "        except Exception as e:\n",
        "            log(f\"Error creating LIME explainer: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        log(f\"Error in LIME explanation process: {str(e)}\")\n",
        "\n",
        "def convert_to_serializable(obj):\n",
        "    \"\"\"Convert NumPy types to Python native types for JSON serialization with improved error handling\"\"\"\n",
        "    try:\n",
        "        if hasattr(np, 'integer') and isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif hasattr(np, 'floating') and isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.int8, np.int16, np.int32, np.int64,\n",
        "                           np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float16, np.float32, np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        elif isinstance(obj, pd.Period):\n",
        "            return str(obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
        "        elif isinstance(obj, list):\n",
        "            return [convert_to_serializable(i) for i in obj]\n",
        "        elif pd.isna(obj):\n",
        "            return None\n",
        "        else:\n",
        "            # Try str() as a last resort\n",
        "            try:\n",
        "                return str(obj)\n",
        "            except:\n",
        "                return \"non-serializable\"\n",
        "    except Exception as e:\n",
        "        # Last line of defense - return a placeholder for anything that can't be serialized\n",
        "        return f\"non-serializable ({type(obj).__name__})\"\n",
        "\n",
        "def plot_model_comparison(model_results, viz_dir):\n",
        "    \"\"\"Create visualization of model comparison metrics with improved error handling\"\"\"\n",
        "    try:\n",
        "        metrics = ['RMSE', 'R2', 'MAE']  # Use R2 instead of R² to avoid issues\n",
        "        model_names = list(model_results.keys())\n",
        "\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        for i, metric in enumerate(metrics):\n",
        "            plt.subplot(1, 3, i+1)\n",
        "\n",
        "            # Get lowercase metric name for dictionary access\n",
        "            lowercase_metric = metric.lower()\n",
        "\n",
        "            # Extract values, handling potential missing metrics\n",
        "            values = []\n",
        "            for model in model_names:\n",
        "                if lowercase_metric in model_results[model]:\n",
        "                    # Handle NaN values\n",
        "                    value = model_results[model][lowercase_metric]\n",
        "                    if pd.isna(value):\n",
        "                        values.append(0)\n",
        "                    else:\n",
        "                        values.append(value)\n",
        "                else:\n",
        "                    values.append(0)  # Default value if metric is missing\n",
        "\n",
        "            bars = plt.bar(model_names, values)\n",
        "\n",
        "            # Add value labels on top of bars\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                if height != 0:  # Only add labels for non-zero values\n",
        "                    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                           f'{height:.3f}',\n",
        "                           ha='center', va='bottom', rotation=90 if len(model_names) > 3 else 0)\n",
        "\n",
        "            # Use display-friendly labels\n",
        "            display_metric = 'R²' if metric == 'R2' else metric\n",
        "            plt.title(f'Comparison by {display_metric}')\n",
        "            plt.ylabel(display_metric)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "\n",
        "        plt.savefig(f\"{viz_dir}/model_comparison.png\", dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        log(\"Model comparison chart created successfully\")\n",
        "    except Exception as e:\n",
        "        log(f\"Error creating model comparison chart: {str(e)}\")\n",
        "        # Try a simpler chart as fallback\n",
        "        try:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "\n",
        "            # Just plot RMSE as a simple bar chart\n",
        "            values = []\n",
        "            for model in model_results:\n",
        "                if 'rmse' in model_results[model]:\n",
        "                    value = model_results[model]['rmse']\n",
        "                    if pd.isna(value):\n",
        "                        values.append(0)\n",
        "                    else:\n",
        "                        values.append(value)\n",
        "                else:\n",
        "                    values.append(0)\n",
        "\n",
        "            plt.bar(model_names, values)\n",
        "            plt.title('Model Comparison by RMSE')\n",
        "            plt.ylabel('RMSE')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{viz_dir}/model_comparison_simple.png\", dpi=200, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            log(\"Created simplified model comparison chart as fallback\")\n",
        "        except Exception as e2:\n",
        "            log(f\"Error creating fallback comparison chart: {str(e2)}\")\n",
        "\n",
        "def plot_seasonal_patterns(panel_df, viz_dir):\n",
        "    \"\"\"Visualize seasonal patterns in crime data with improved error handling\"\"\"\n",
        "    try:\n",
        "        log(\"Creating seasonal patterns visualization\")\n",
        "\n",
        "        # Check if required columns exist\n",
        "        if not all(col in panel_df.columns for col in ['year', 'month', 'crime_count']):\n",
        "            log(\"Required columns missing for seasonal patterns visualization\")\n",
        "            return\n",
        "\n",
        "        # Ensure data types are correct\n",
        "        panel_df_clean = panel_df.copy()\n",
        "        panel_df_clean['year'] = pd.to_numeric(panel_df_clean['year'], errors='coerce')\n",
        "        panel_df_clean['month'] = pd.to_numeric(panel_df_clean['month'], errors='coerce')\n",
        "        panel_df_clean['crime_count'] = pd.to_numeric(panel_df_clean['crime_count'], errors='coerce')\n",
        "\n",
        "        # Drop rows with NaN values after conversion\n",
        "        panel_df_clean = panel_df_clean.dropna(subset=['year', 'month', 'crime_count'])\n",
        "\n",
        "        if len(panel_df_clean) == 0:\n",
        "            log(\"No valid data after cleaning for seasonal patterns\")\n",
        "            return\n",
        "\n",
        "        # Create monthly average crime by year\n",
        "        monthly_crime = panel_df_clean.groupby(['year', 'month'])['crime_count'].mean().reset_index()\n",
        "\n",
        "        # Pivot for better visualization\n",
        "        try:\n",
        "            monthly_pivot = monthly_crime.pivot(index='month', columns='year', values='crime_count')\n",
        "\n",
        "            # Plot\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.lineplot(data=monthly_pivot)\n",
        "            plt.title('Monthly Crime Patterns by Year')\n",
        "            plt.xlabel('Month')\n",
        "            plt.ylabel('Average Crime Count per Community Area')\n",
        "            plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                                    'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "            plt.legend(title='Year')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{viz_dir}/monthly_crime_patterns.png\", dpi=200, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            log(\"Created monthly crime patterns visualization\")\n",
        "        except Exception as e:\n",
        "            log(f\"Error creating monthly pivot: {str(e)}\")\n",
        "            # Try alternative visualization\n",
        "            try:\n",
        "                plt.figure(figsize=(12, 8))\n",
        "                for year in sorted(monthly_crime['year'].unique()):\n",
        "                    year_data = monthly_crime[monthly_crime['year'] == year]\n",
        "                    plt.plot(year_data['month'], year_data['crime_count'], label=str(int(year)))\n",
        "                plt.title('Monthly Crime Patterns by Year')\n",
        "                plt.xlabel('Month')\n",
        "                plt.ylabel('Average Crime Count per Community Area')\n",
        "                plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
        "                                        'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
        "                plt.legend(title='Year')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{viz_dir}/monthly_crime_patterns_alt.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                log(\"Created alternative monthly crime patterns visualization\")\n",
        "            except Exception as e2:\n",
        "                log(f\"Error creating alternative monthly visualization: {str(e2)}\")\n",
        "\n",
        "        # Create day of week patterns\n",
        "        if 'day_of_week' in panel_df_clean.columns:\n",
        "            try:\n",
        "                panel_df_clean['day_of_week'] = pd.to_numeric(panel_df_clean['day_of_week'], errors='coerce')\n",
        "                dow_crime = panel_df_clean.groupby(['year', 'day_of_week'])['crime_count'].mean().reset_index()\n",
        "\n",
        "                try:\n",
        "                    dow_pivot = dow_crime.pivot(index='day_of_week', columns='year', values='crime_count')\n",
        "\n",
        "                    plt.figure(figsize=(10, 6))\n",
        "                    sns.lineplot(data=dow_pivot)\n",
        "                    plt.title('Day of Week Crime Patterns by Year')\n",
        "                    plt.xlabel('Day of Week')\n",
        "                    plt.ylabel('Average Crime Count')\n",
        "                    plt.xticks(range(0, 7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "                    plt.legend(title='Year')\n",
        "                    plt.grid(True, alpha=0.3)\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f\"{viz_dir}/day_of_week_patterns.png\", dpi=200, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                    log(\"Created day of week patterns visualization\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error creating day of week pivot: {str(e)}\")\n",
        "                    # Try alternative\n",
        "                    try:\n",
        "                        plt.figure(figsize=(10, 6))\n",
        "                        for year in sorted(dow_crime['year'].unique()):\n",
        "                            year_data = dow_crime[dow_crime['year'] == year]\n",
        "                            plt.plot(year_data['day_of_week'], year_data['crime_count'], label=str(int(year)))\n",
        "                        plt.title('Day of Week Crime Patterns by Year')\n",
        "                        plt.xlabel('Day of Week')\n",
        "                        plt.ylabel('Average Crime Count')\n",
        "                        plt.xticks(range(0, 7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
        "                        plt.legend(title='Year')\n",
        "                        plt.grid(True, alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(f\"{viz_dir}/day_of_week_patterns_alt.png\", dpi=200, bbox_inches='tight')\n",
        "                        plt.close()\n",
        "                        log(\"Created alternative day of week visualization\")\n",
        "                    except Exception as e2:\n",
        "                        log(f\"Error creating alternative day of week visualization: {str(e2)}\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error processing day of week data: {str(e)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error creating seasonal patterns visualization: {str(e)}\")\n",
        "\n",
        "def add_fairness_mitigation(X_train, y_train, train_df, model_name=\"HistGradientBoostingRegressor\"):\n",
        "    \"\"\"\n",
        "    Apply fairness mitigation to training data with improved error handling\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : DataFrame\n",
        "        Training features\n",
        "    y_train : Series\n",
        "        Target variable\n",
        "    train_df : DataFrame\n",
        "        Complete training dataframe containing sensitive attributes\n",
        "    model_name : str\n",
        "        The model type to use (default: HistGradientBoostingRegressor)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        Tuple containing (mitigated_X, mitigated_y, sample_weights)\n",
        "    \"\"\"\n",
        "    if not FAIRLEARN_AVAILABLE:\n",
        "        log(\"Fairlearn not available for fairness mitigation\")\n",
        "        return X_train, y_train, None\n",
        "\n",
        "    try:\n",
        "        log(\"Applying fairness mitigation techniques\")\n",
        "\n",
        "        # Detect sensitive attribute - try hardship_quartile first, then hardship_index if available\n",
        "        sens_attr = None\n",
        "        sens_attr_name = None\n",
        "\n",
        "        if 'hardship_quartile' in train_df.columns:\n",
        "            try:\n",
        "                # Convert to string to avoid categorical issues\n",
        "                sens_attr = train_df['hardship_quartile'].astype(str)\n",
        "                sens_attr_name = 'hardship_quartile'\n",
        "                log(\"Using 'hardship_quartile' as sensitive attribute\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error converting hardship_quartile: {str(e)}\")\n",
        "\n",
        "        elif 'hardship_index' in train_df.columns:\n",
        "            try:\n",
        "                # Create quartiles if using continuous value\n",
        "                train_df['hardship_index'] = pd.to_numeric(train_df['hardship_index'], errors='coerce')\n",
        "                if train_df['hardship_index'].notna().any():\n",
        "                    # Fill NaNs with median to allow quartile creation\n",
        "                    train_df['hardship_index'] = train_df['hardship_index'].fillna(train_df['hardship_index'].median())\n",
        "\n",
        "                    try:\n",
        "                        # Try to create quartiles\n",
        "                        quartiles = pd.qcut(train_df['hardship_index'], 4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
        "                        sens_attr = quartiles.astype(str)\n",
        "                        sens_attr_name = 'hardship_index_quartile'\n",
        "                        log(\"Using 'hardship_index' (quartile-binned) as sensitive attribute\")\n",
        "                    except ValueError:  # Handle case with duplicate values\n",
        "                        quartiles = pd.qcut(train_df['hardship_index'],\n",
        "                                          [0, 0.25, 0.5, 0.75, 1],\n",
        "                                          labels=['Q1', 'Q2', 'Q3', 'Q4'],\n",
        "                                          duplicates='drop')\n",
        "                        sens_attr = quartiles.astype(str)\n",
        "                        sens_attr_name = 'hardship_index_quartile'\n",
        "                        log(\"Using 'hardship_index' (quantile-binned) as sensitive attribute\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error processing hardship_index: {str(e)}\")\n",
        "\n",
        "        if sens_attr is not None:\n",
        "            mitigation_techniques = []\n",
        "\n",
        "            # 1. Apply Reweighing (always available in fairlearn)\n",
        "            try:\n",
        "                reweighing = Reweighing()\n",
        "                X_reweigh, y_reweigh, sample_weights_reweigh = reweighing.fit_transform(\n",
        "                    X_train, y_train, sensitive_features=sens_attr\n",
        "                )\n",
        "\n",
        "                # Log information about weights\n",
        "                weight_summary = pd.Series(sample_weights_reweigh).describe()\n",
        "                log(f\"Reweighing sample weights - min: {weight_summary['min']:.3f}, \"\n",
        "                    f\"mean: {weight_summary['mean']:.3f}, max: {weight_summary['max']:.3f}\")\n",
        "\n",
        "                mitigation_techniques.append(('Reweighing', X_reweigh, y_reweigh, sample_weights_reweigh))\n",
        "            except Exception as e:\n",
        "                log(f\"Error applying Reweighing: {str(e)}\")\n",
        "\n",
        "            # Evaluate each mitigation technique\n",
        "            if mitigation_techniques:\n",
        "                # Select the best technique (for now, default to Reweighing as it's most reliable)\n",
        "                best_technique = mitigation_techniques[0]\n",
        "                log(f\"Using {best_technique[0]} as the fairness mitigation technique\")\n",
        "\n",
        "                # Create ethical warning based on disparity\n",
        "                if sens_attr_name:\n",
        "                    # Try to check for high disparity across protected groups\n",
        "                    try:\n",
        "                        # Group crime data by sensitive attribute\n",
        "                        group_stats = train_df.groupby(sens_attr_name)['crime_count'].mean()\n",
        "                        if group_stats.max() > 0 and group_stats.min() > 0:\n",
        "                            max_rate = group_stats.max()\n",
        "                            min_rate = group_stats.min()\n",
        "                            disparity_ratio = max_rate / min_rate\n",
        "\n",
        "                            if disparity_ratio > 3:\n",
        "                                log(\"⚠️ ETHICAL WARNING - RED FLAG: High disparity detected across protected groups\")\n",
        "                                log(f\"Crime rate ratio between highest and lowest groups: {disparity_ratio:.2f}x\")\n",
        "                                log(\"This may lead to biased predictions that reinforce existing inequalities.\")\n",
        "                            elif disparity_ratio > 1.5:\n",
        "                                log(\"⚠️ ETHICAL WARNING - YELLOW FLAG: Moderate disparity detected\")\n",
        "                                log(f\"Crime rate ratio between highest and lowest groups: {disparity_ratio:.2f}x\")\n",
        "                            else:\n",
        "                                log(\"✓ Disparity across protected groups is within acceptable limits\")\n",
        "                    except Exception as e:\n",
        "                        log(f\"Error calculating disparity ratio: {str(e)}\")\n",
        "\n",
        "                return best_technique[1], best_technique[2], best_technique[3]\n",
        "            else:\n",
        "                log(\"No fairness mitigation techniques were successfully applied\")\n",
        "                return X_train, y_train, None\n",
        "        else:\n",
        "            log(\"No suitable sensitive attribute found for fairness mitigation\")\n",
        "            return X_train, y_train, None\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f\"Error applying fairness mitigation: {str(e)}\")\n",
        "        return X_train, y_train, None\n",
        "\n",
        "# Add the main execution flow here, incorporating all the improved functions\n",
        "# For example:\n",
        "\n",
        "# ---------- 4. load datasets --------------------------------------\n",
        "def main():\n",
        "    # Load or fetch crime data\n",
        "    crime_cache = f\"{CACHE_DIR}/crime.parquet\"\n",
        "    comm_cache = f\"{CACHE_DIR}/comm.parquet\"\n",
        "    census_cache = f\"{CACHE_DIR}/census.parquet\"\n",
        "    bounds_cache = f\"{CACHE_DIR}/bounds.parquet\"\n",
        "\n",
        "    if os.path.exists(crime_cache):\n",
        "        crime = pd.read_parquet(crime_cache)\n",
        "        log(\"Loading cached crime data\")\n",
        "    else:\n",
        "        crime = fetch_batches(CRIME_API, BATCH_SIZE, MAX_RECORDS)\n",
        "        if 'date' in crime.columns:\n",
        "            crime['date'] = pd.to_datetime(crime['date'], errors='coerce')\n",
        "        crime.to_parquet(crime_cache, index=False)\n",
        "        log(\"Crime data downloaded and cached\")\n",
        "\n",
        "    if os.path.exists(comm_cache):\n",
        "        comm = pd.read_parquet(comm_cache)\n",
        "        log(\"Loading cached community data\")\n",
        "    else:\n",
        "        comm = pd.DataFrame(requests.get(COMM_API, timeout=60).json())\n",
        "        comm.to_parquet(comm_cache, index=False)\n",
        "        log(\"Community data downloaded and cached\")\n",
        "\n",
        "    if os.path.exists(census_cache):\n",
        "        census = pd.read_parquet(census_cache)\n",
        "        log(\"Loading cached census data\")\n",
        "    else:\n",
        "        try:\n",
        "            census = pd.DataFrame(requests.get(CENSUS_API, timeout=60).json())\n",
        "            census.to_parquet(census_cache, index=False)\n",
        "            log(\"Census data downloaded and cached\")\n",
        "        except Exception as e:\n",
        "            census = pd.DataFrame()\n",
        "            log(f\"Error fetching census data: {str(e)}\")\n",
        "\n",
        "    # Load or fetch boundaries for spatial analysis\n",
        "    if os.path.exists(bounds_cache):\n",
        "        bounds = pd.read_parquet(bounds_cache)\n",
        "        log(\"Loading cached boundary data\")\n",
        "    else:\n",
        "        try:\n",
        "            bounds = pd.DataFrame(requests.get(BOUNDS_API, timeout=60).json())\n",
        "            bounds.to_parquet(bounds_cache, index=False)\n",
        "            log(\"Boundary data downloaded and cached\")\n",
        "        except Exception as e:\n",
        "            bounds = pd.DataFrame()\n",
        "            log(f\"Error fetching boundary data: {str(e)}\")\n",
        "\n",
        "    # Apply year filter if specified\n",
        "    if TIME_RANGE:\n",
        "        original_count = len(crime)\n",
        "        # Ensure date column is datetime and handle NaNs\n",
        "        crime['date'] = pd.to_datetime(crime['date'], errors='coerce')\n",
        "        valid_dates = ~crime['date'].isna()\n",
        "        crime = crime[valid_dates & crime.date.dt.year.isin(TIME_RANGE)]\n",
        "        log(f\"Applied year filter {TIME_RANGE}: {len(crime):,} rows (from {original_count:,})\")\n",
        "\n",
        "    # Check data quality\n",
        "    check_data_quality(crime, \"Crime\")\n",
        "    check_data_quality(comm, \"Community\")\n",
        "    if not census.empty:\n",
        "        check_data_quality(census, \"Census\")\n",
        "\n",
        "    # Clean any unhashable types in dataframes\n",
        "    crime = clean_object_columns(crime)\n",
        "    comm = clean_object_columns(comm)\n",
        "    if not census.empty:\n",
        "        census = clean_object_columns(census)\n",
        "    if not bounds.empty:\n",
        "        bounds = clean_object_columns(bounds)\n",
        "\n",
        "    # Standardize community area columns\n",
        "    crime, comm, census = standardize_community_area_columns(crime, comm, census)\n",
        "\n",
        "    # Convert numeric columns in census data\n",
        "    if not census.empty:\n",
        "        numeric_cols = ['hardship_index', 'per_capita_income', 'percent_aged_16_unemployed',\n",
        "                       'percent_aged_25_without_high_school_diploma', 'percent_households_below_poverty',\n",
        "                       'percent_aged_under_18_or_over_64', 'percent_housing_crowded',\n",
        "                       'percent_households_below_poverty', 'percent_minority']\n",
        "\n",
        "        census = ensure_numeric(census, numeric_cols)\n",
        "\n",
        "    # ---------- 5. enhanced feature engineering -----------------------\n",
        "    log(\"Starting enhanced feature engineering\")\n",
        "\n",
        "    # Extract temporal features\n",
        "    crime = extract_temporal_features(crime)\n",
        "\n",
        "    # Create spatial features\n",
        "    crime, comm = create_spatial_features(crime, comm)\n",
        "\n",
        "    # Create crime type indicators with more detailed categories\n",
        "    if 'primary_type' in crime.columns:\n",
        "        # Violent crimes\n",
        "        up = crime.primary_type.str.upper()\n",
        "        crime['violent'] = up.isin(['HOMICIDE', 'ASSAULT', 'BATTERY', 'CRIMINAL SEXUAL ASSAULT', 'ROBBERY']).astype(int)\n",
        "\n",
        "        # Public order crimes\n",
        "        public_order_types = ['NARCOTICS', 'LIQUOR LAW VIOLATION', 'PUBLIC PEACE VIOLATION',\n",
        "                             'INTERFERENCE WITH PUBLIC OFFICER', 'GAMBLING', 'OBSCENITY']\n",
        "        crime['public_order'] = up.isin(public_order_types).astype(int)\n",
        "\n",
        "        # Property crimes\n",
        "        property_types = ['THEFT', 'BURGLARY', 'MOTOR VEHICLE THEFT', 'ARSON',\n",
        "                         'CRIMINAL DAMAGE', 'CRIMINAL TRESPASS']\n",
        "        crime['property'] = up.isin(property_types).astype(int)\n",
        "\n",
        "        # Financial crimes\n",
        "        financial_types = ['DECEPTIVE PRACTICE', 'FORGERY', 'FRAUD']\n",
        "        crime['financial'] = up.isin(financial_types).astype(int)\n",
        "\n",
        "        # Create severity levels based on FBI Uniform Crime Reporting\n",
        "        part1_types = ['HOMICIDE', 'CRIMINAL SEXUAL ASSAULT', 'ROBBERY', 'ASSAULT',\n",
        "                      'BURGLARY', 'THEFT', 'MOTOR VEHICLE THEFT', 'ARSON']\n",
        "        crime['serious_crime'] = up.isin(part1_types).astype(int)\n",
        "\n",
        "    # Create crime heatmap and visualizations\n",
        "    # Use improved safe version\n",
        "    create_crime_heatmap_safe(crime, comm, VIZ_DIR)\n",
        "\n",
        "    # Get unique community areas\n",
        "    community_areas = crime['community_area'].unique()\n",
        "\n",
        "    # Create panel data structure (one row per community area per month)\n",
        "    if USE_PANEL_DATA:\n",
        "        panel_df = create_panel_data(crime, community_areas, TIME_RANGE)\n",
        "\n",
        "        # Create seasonal patterns visualization\n",
        "        plot_seasonal_patterns(panel_df, VIZ_DIR)\n",
        "\n",
        "        # Join community data to panel\n",
        "        comm['community_area'] = comm['community_area'].astype(str)\n",
        "        panel_df['community_area'] = panel_df['community_area'].astype(str)\n",
        "\n",
        "        panel_df = pd.merge(panel_df, comm, on='community_area', how='left')\n",
        "        log(f\"Merged panel data with community data: {panel_df.shape}\")\n",
        "\n",
        "        # Join census data if available\n",
        "        if not census.empty:\n",
        "            census['community_area'] = census['community_area'].astype(str)\n",
        "            panel_df = pd.merge(panel_df, census, on='community_area', how='left')\n",
        "            log(f\"Merged panel data with census data: {panel_df.shape}\")\n",
        "\n",
        "        # Calculate crime rate if population data available\n",
        "        TARGET = 'crime_count'  # Default target\n",
        "        if USE_CRIME_RATE and 'population' in panel_df.columns:\n",
        "            # Convert population to numeric if needed\n",
        "            panel_df['population'] = pd.to_numeric(panel_df['population'], errors='coerce')\n",
        "            log(\"Converted population to numeric\")\n",
        "\n",
        "            # Only use if we have valid values\n",
        "            if 'population' in panel_df.columns and panel_df['population'].sum() > 0:\n",
        "                # Replace zeros with 1 to avoid division by zero\n",
        "                panel_df['population'] = panel_df['population'].replace(0, 1)\n",
        "                panel_df['crime_rate'] = (panel_df.crime_count / (panel_df.population / 1000)).round(3)\n",
        "                TARGET = 'crime_rate'\n",
        "                log(f\"Created {TARGET} as target variable\")\n",
        "            else:\n",
        "                log(f\"Using {TARGET} as target variable (population data invalid)\")\n",
        "        else:\n",
        "            log(f\"Using {TARGET} as target variable\")\n",
        "\n",
        "        # Create hardship quartiles for fairness analysis\n",
        "        if 'hardship_index' in panel_df.columns:\n",
        "            # Convert hardship_index to numeric if needed\n",
        "            panel_df['hardship_index'] = pd.to_numeric(panel_df['hardship_index'], errors='coerce')\n",
        "            log(\"Converted hardship_index to numeric\")\n",
        "\n",
        "            # Only create if we have valid values\n",
        "            if not panel_df['hardship_index'].isna().all():\n",
        "                # Fill missing values before creating quartiles\n",
        "                panel_df['hardship_index'] = panel_df['hardship_index'].fillna(safe_median(panel_df['hardship_index']))\n",
        "                try:\n",
        "                    # Create quartiles - use string labels to avoid categorical dtype issues\n",
        "                    panel_df['hardship_quartile'] = pd.qcut(\n",
        "                        panel_df['hardship_index'],\n",
        "                        q=4,\n",
        "                        labels=['Low', 'Medium-Low', 'Medium-High', 'High']\n",
        "                    ).astype(str)\n",
        "                    log(\"Created hardship quartiles for fairness analysis\")\n",
        "                except ValueError:\n",
        "                    # Handle case with duplicate values\n",
        "                    try:\n",
        "                        panel_df['hardship_quartile'] = pd.qcut(\n",
        "                            panel_df['hardship_index'],\n",
        "                            q=[0, 0.25, 0.5, 0.75, 1],\n",
        "                            labels=['Low', 'Medium-Low', 'Medium-High', 'High'],\n",
        "                            duplicates='drop'\n",
        "                        ).astype(str)\n",
        "                        log(\"Created hardship quartiles (with duplicate handling) for fairness analysis\")\n",
        "                    except Exception as e:\n",
        "                        log(f\"Could not create hardship quartiles: {str(e)}\")\n",
        "                        # Create a simple binary version as fallback\n",
        "                        median_hardship = panel_df['hardship_index'].median()\n",
        "                        panel_df['hardship_quartile'] = np.where(\n",
        "                            panel_df['hardship_index'] > median_hardship,\n",
        "                            'High',\n",
        "                            'Low'\n",
        "                        )\n",
        "                        log(\"Created binary hardship indicator as fallback\")\n",
        "\n",
        "        # Make sure the dataframe has no unhashable types\n",
        "        panel_df = clean_object_columns(panel_df)\n",
        "\n",
        "        # Filter out highly correlated features - use safer version\n",
        "        columns_to_keep = filter_highly_correlated_features_safe(\n",
        "            panel_df,\n",
        "            TARGET,\n",
        "            threshold=MAX_CORRELATION\n",
        "        )\n",
        "\n",
        "        panel_df = panel_df[columns_to_keep]\n",
        "        log(f\"Panel data shape after correlation filtering: {panel_df.shape}\")\n",
        "\n",
        "        # Convert month_year to string for proper data splitting\n",
        "        if 'month_year' in panel_df.columns:\n",
        "            panel_df['month_year_str'] = panel_df['month_year'].astype(str)\n",
        "\n",
        "        # Split data by time\n",
        "        if len(TIME_RANGE) >= 3:\n",
        "            train_mask = panel_df['year'] == TIME_RANGE[0]  # First year for training\n",
        "            val_mask = panel_df['year'] == TIME_RANGE[1]    # Second year for validation\n",
        "            test_mask = panel_df['year'] == TIME_RANGE[-1]  # Last year for testing\n",
        "\n",
        "            train_df = panel_df[train_mask].copy()\n",
        "            val_df = panel_df[val_mask].copy()\n",
        "            test_df = panel_df[test_mask].copy()\n",
        "\n",
        "            log(f\"Train data: {train_df.shape} rows from {TIME_RANGE[0]}\")\n",
        "            log(f\"Validation data: {val_df.shape} rows from {TIME_RANGE[1]}\")\n",
        "            log(f\"Test data: {test_df.shape} rows from {TIME_RANGE[-1]}\")\n",
        "        else:\n",
        "            # Fallback if not enough years in TIME_RANGE\n",
        "            log(\"Not enough years in TIME_RANGE for proper temporal split. Using random split.\")\n",
        "            train_val_df, test_df = train_test_split(panel_df, test_size=0.2, random_state=RAND_STATE)\n",
        "            train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=RAND_STATE)\n",
        "\n",
        "            log(f\"Train data: {train_df.shape} rows (random split)\")\n",
        "            log(f\"Validation data: {val_df.shape} rows (random split)\")\n",
        "            log(f\"Test data: {test_df.shape} rows (random split)\")\n",
        "\n",
        "        # Prepare features and target\n",
        "        # Drop non-numeric and administrative columns\n",
        "        exclude_cols = [\n",
        "            'month_year', 'month_year_str', TARGET,\n",
        "            'community_area', 'geometry', 'the_geom',\n",
        "            'community', 'area_numbe', 'shape_area', 'shape_len'\n",
        "        ]\n",
        "\n",
        "        feature_cols = [col for col in panel_df.columns if col not in exclude_cols]\n",
        "\n",
        "        # Select only numeric columns\n",
        "        X_train = train_df[feature_cols].select_dtypes(include=['number'])\n",
        "        y_train = train_df[TARGET]\n",
        "\n",
        "        X_val = val_df[feature_cols].select_dtypes(include=['number'])\n",
        "        y_val = val_df[TARGET]\n",
        "\n",
        "        X_test = test_df[feature_cols].select_dtypes(include=['number'])\n",
        "        y_test = test_df[TARGET]\n",
        "\n",
        "        # Feature selection\n",
        "        selected_features = select_features(X_train, y_train, MAX_FEATURES)\n",
        "\n",
        "        X_train = X_train[selected_features]\n",
        "        X_val = X_val[selected_features]\n",
        "        X_test = X_test[selected_features]\n",
        "\n",
        "        log(f\"Final feature set: {X_train.shape[1]} features\")\n",
        "\n",
        "        # Impute missing values before modeling\n",
        "        X_train = impute_missing_values(X_train)\n",
        "        X_val = impute_missing_values(X_val)\n",
        "        X_test = impute_missing_values(X_test)\n",
        "\n",
        "        log(f\"Data shapes after imputation - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "        # Log feature correlations with target\n",
        "        if len(selected_features) > 0:\n",
        "            # Calculate correlations safely\n",
        "            corr_with_target = pd.Series(index=X_train.columns, dtype=float)\n",
        "            for col in X_train.columns:\n",
        "                try:\n",
        "                    # Use only non-NA values in both columns\n",
        "                    mask = ~X_train[col].isna() & ~y_train.isna()\n",
        "                    if mask.sum() > 10:  # Only calculate if we have enough valid points\n",
        "                        corr = np.corrcoef(X_train[col][mask], y_train[mask])[0, 1]\n",
        "                        corr_with_target[col] = abs(corr)\n",
        "                except Exception as e:\n",
        "                    log(f\"Error calculating correlation for {col}: {str(e)}\")\n",
        "                    corr_with_target[col] = 0.0\n",
        "\n",
        "            corr_with_target = corr_with_target.sort_values(ascending=False)\n",
        "            log(\"Top feature correlations with target:\")\n",
        "            for i, (col, corr) in enumerate(corr_with_target.items()):\n",
        "                if i < 10:  # Show top 10\n",
        "                    log(f\"  {i+1}. {col}: {corr:.3f}\")\n",
        "\n",
        "        # Extract protected attributes for fairness analysis\n",
        "        protected_attrs = {}\n",
        "\n",
        "        if 'hardship_quartile' in test_df.columns:\n",
        "            protected_attrs['hardship_quartile'] = test_df['hardship_quartile']\n",
        "\n",
        "        if 'hardship_index' in test_df.columns:\n",
        "            protected_attrs['hardship_index'] = test_df['hardship_index']\n",
        "\n",
        "        if protected_attrs:\n",
        "            log(f\"Protected attributes for fairness analysis: {list(protected_attrs.keys())}\")\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Convert to DataFrames to keep column names\n",
        "        X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "        X_val_scaled_df = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
        "        X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "        # Check for any remaining NaNs after scaling\n",
        "        if X_train_scaled_df.isna().any().any():\n",
        "            log(f\"WARNING: {X_train_scaled_df.isna().sum().sum()} NaNs remain in training data after scaling\")\n",
        "            X_train_scaled_df = X_train_scaled_df.fillna(0)  # Fill any remaining NaNs\n",
        "        if X_val_scaled_df.isna().any().any():\n",
        "            log(f\"WARNING: {X_val_scaled_df.isna().sum().sum()} NaNs remain in validation data after scaling\")\n",
        "            X_val_scaled_df = X_val_scaled_df.fillna(0)\n",
        "        if X_test_scaled_df.isna().any().any():\n",
        "            log(f\"WARNING: {X_test_scaled_df.isna().sum().sum()} NaNs remain in test data after scaling\")\n",
        "            X_test_scaled_df = X_test_scaled_df.fillna(0)\n",
        "\n",
        "        # ---------- 6. model training and evaluation -------------------\n",
        "        log(\"Training models with time-based validation\")\n",
        "\n",
        "        # Dictionary to store model results\n",
        "        model_results = {}\n",
        "        trained_models = {}\n",
        "\n",
        "        # Apply fairness mitigation if enabled\n",
        "        if APPLY_FAIRNESS and FAIRLEARN_AVAILABLE:\n",
        "            # Apply fairness mitigation before model training\n",
        "            X_train_mit, y_train_mit, sample_weights = add_fairness_mitigation(\n",
        "                X_train_scaled_df, y_train, train_df\n",
        "            )\n",
        "        else:\n",
        "            X_train_mit, y_train_mit, sample_weights = X_train_scaled_df, y_train, None\n",
        "\n",
        "        # 1. Ridge Regression using Pipeline with imputation\n",
        "        log(\"Training Ridge Regression model\")\n",
        "\n",
        "        best_alpha = 1.0  # Default value\n",
        "\n",
        "        try:\n",
        "            # Define objective function for Ridge alpha tuning with built-in imputation\n",
        "            if OPTUNA_AVAILABLE:\n",
        "                def objective_ridge(trial):\n",
        "                    alpha = trial.suggest_float(\"alpha\", 0.01, 100.0, log=True)\n",
        "\n",
        "                    # Create a pipeline with imputation\n",
        "                    pipeline = Pipeline([\n",
        "                        ('imputer', SimpleImputer(strategy='median')),\n",
        "                        ('ridge', Ridge(alpha=alpha, random_state=RAND_STATE))\n",
        "                    ])\n",
        "\n",
        "                    # Fit and evaluate\n",
        "                    if sample_weights is not None:\n",
        "                        pipeline.fit(X_train_mit, y_train_mit, ridge__sample_weight=sample_weights)\n",
        "                    else:\n",
        "                        pipeline.fit(X_train_mit, y_train_mit)\n",
        "\n",
        "                    # Calculate RMSE\n",
        "                    return np.sqrt(mean_squared_error(y_val, pipeline.predict(X_val_scaled_df)))\n",
        "\n",
        "                # Run optimization with error handling\n",
        "                try:\n",
        "                    study = optuna.create_study(direction=\"minimize\")\n",
        "                    study.optimize(objective_ridge, n_trials=N_TRIALS)\n",
        "                    best_alpha = study.best_params[\"alpha\"]\n",
        "                    log(f\"Best Ridge alpha: {best_alpha:.4f}\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error during Ridge hyperparameter optimization: {str(e)}\")\n",
        "                    best_alpha = 1.0  # Fallback to default value\n",
        "\n",
        "            # Create pipeline with imputation\n",
        "            ridge_pipeline = Pipeline([\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('ridge', Ridge(alpha=best_alpha, random_state=RAND_STATE))\n",
        "            ])\n",
        "\n",
        "            # Train the pipeline\n",
        "            if sample_weights is not None:\n",
        "                ridge_pipeline.fit(X_train_mit, y_train_mit, ridge__sample_weight=sample_weights)\n",
        "            else:\n",
        "                ridge_pipeline.fit(X_train_mit, y_train_mit)\n",
        "\n",
        "            # Evaluate on test set\n",
        "            ridge_pred = ridge_pipeline.predict(X_test_scaled_df)\n",
        "            ridge_rmse = np.sqrt(mean_squared_error(y_test, ridge_pred))\n",
        "            ridge_r2 = r2_score(y_test, ridge_pred)\n",
        "            ridge_mae = mean_absolute_error(y_test, ridge_pred)\n",
        "\n",
        "            log(f\"Ridge - RMSE: {ridge_rmse:.3f}, R²: {ridge_r2:.3f}, MAE: {ridge_mae:.3f}\")\n",
        "            model_results['Ridge'] = {'rmse': ridge_rmse, 'r2': ridge_r2, 'mae': ridge_mae}\n",
        "            trained_models['Ridge'] = (ridge_pipeline, ridge_pred)\n",
        "\n",
        "            # Log important coefficients\n",
        "            ridge_model = ridge_pipeline.named_steps['ridge']\n",
        "            coef = pd.Series(ridge_model.coef_, index=X_train.columns)\n",
        "            important_coef = coef.abs().sort_values(ascending=False)\n",
        "            log(\"Top Ridge coefficients:\")\n",
        "            for i, (col, val) in enumerate(important_coef.items()):\n",
        "                if i < 5:  # Show top 5\n",
        "                    log(f\"  {i+1}. {col}: {val:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log(f\"Error training Ridge model: {str(e)}\")\n",
        "\n",
        "        # 2. Gradient Boosting using HistGradientBoostingRegressor (handles missing values)\n",
        "        log(\"Training Gradient Boosting model\")\n",
        "\n",
        "        # Default parameters\n",
        "        best_params = {\n",
        "            'max_iter': 100,\n",
        "            'max_depth': 3,\n",
        "            'learning_rate': 0.1,\n",
        "            'max_bins': 255,\n",
        "            'early_stopping': True,\n",
        "            'random_state': RAND_STATE\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            if OPTUNA_AVAILABLE:\n",
        "                # Define objective function for GB\n",
        "                def objective_gb(trial):\n",
        "                    params = {\n",
        "                        'max_iter': trial.suggest_int(\"max_iter\", 50, 300),\n",
        "                        'max_depth': trial.suggest_int(\"max_depth\", 2, 6),\n",
        "                        'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
        "                        'max_bins': trial.suggest_int(\"max_bins\", 32, 255),\n",
        "                        'early_stopping': True,\n",
        "                        'random_state': RAND_STATE\n",
        "                    }\n",
        "\n",
        "                    # Use HistGradientBoostingRegressor which handles missing values natively\n",
        "                    model = HistGradientBoostingRegressor(**params)\n",
        "\n",
        "                    if sample_weights is not None:\n",
        "                        model.fit(X_train_mit, y_train_mit, sample_weight=sample_weights)\n",
        "                    else:\n",
        "                        model.fit(X_train_mit, y_train_mit)\n",
        "\n",
        "                    # Calculate RMSE\n",
        "                    return np.sqrt(mean_squared_error(y_val, model.predict(X_val_scaled_df)))\n",
        "\n",
        "                # Run optimization with error handling\n",
        "                try:\n",
        "                    study = optuna.create_study(direction=\"minimize\")\n",
        "                    study.optimize(objective_gb, n_trials=N_TRIALS)\n",
        "                    best_params = study.best_params\n",
        "                    best_params['random_state'] = RAND_STATE\n",
        "                    best_params['early_stopping'] = True\n",
        "                    log(f\"Best GB parameters: {best_params}\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error during GB hyperparameter optimization: {str(e)}\")\n",
        "                    # Use default parameters\n",
        "\n",
        "            # Train with best/default parameters using HistGradientBoostingRegressor\n",
        "            gb = HistGradientBoostingRegressor(**best_params)\n",
        "\n",
        "            if sample_weights is not None:\n",
        "                gb.fit(X_train_mit, y_train_mit, sample_weight=sample_weights)\n",
        "            else:\n",
        "                gb.fit(X_train_mit, y_train_mit)\n",
        "\n",
        "            # Evaluate on test set\n",
        "            gb_pred = gb.predict(X_test_scaled_df)\n",
        "            gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
        "            gb_r2 = r2_score(y_test, gb_pred)\n",
        "            gb_mae = mean_absolute_error(y_test, gb_pred)\n",
        "\n",
        "            log(f\"Gradient Boosting - RMSE: {gb_rmse:.3f}, R²: {gb_r2:.3f}, MAE: {gb_mae:.3f}\")\n",
        "            model_results['Gradient Boosting'] = {'rmse': gb_rmse, 'r2': gb_r2, 'mae': gb_mae}\n",
        "            trained_models['Gradient Boosting'] = (gb, gb_pred)\n",
        "\n",
        "            # Extract feature importances with better fallback options\n",
        "            try:\n",
        "                if hasattr(gb, 'feature_importances_'):\n",
        "                    feature_importances = gb.feature_importances_\n",
        "                elif hasattr(gb, '_estimator') and hasattr(gb._estimator, 'feature_importances_'):\n",
        "                    feature_importances = gb._estimator.feature_importances_\n",
        "                else:\n",
        "                    # Create a surrogate model for feature importance\n",
        "                    log(\"Creating surrogate GradientBoostingRegressor for feature importance\")\n",
        "                    surrogate_gb = GradientBoostingRegressor(n_estimators=100, random_state=RAND_STATE)\n",
        "                    surrogate_gb.fit(X_train_scaled_df, y_train)\n",
        "                    feature_importances = surrogate_gb.feature_importances_\n",
        "\n",
        "                # Report feature importances\n",
        "                importances = pd.Series(feature_importances, index=X_train.columns)\n",
        "                importances = importances.sort_values(ascending=False)\n",
        "\n",
        "                log(\"Top GB feature importances:\")\n",
        "                for i, (col, val) in enumerate(importances.items()):\n",
        "                    if i < 5:  # Show top 5\n",
        "                        log(f\"  {i+1}. {col}: {val:.4f}\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error extracting feature importances: {str(e)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log(f\"Error training Gradient Boosting model: {str(e)}\")\n",
        "\n",
        "        # 3. Mixed Effects Model if available\n",
        "        if MIXED_MODELS_AVAILABLE:\n",
        "            log(\"Training Mixed Effects Model\")\n",
        "\n",
        "            try:\n",
        "                # Get top features from correlations with target\n",
        "                top_indices = corr_with_target.index[:min(3, len(corr_with_target))]\n",
        "\n",
        "                # Create dataframe for mixed effects model\n",
        "                me_df = pd.DataFrame()\n",
        "                me_df['y'] = y_train\n",
        "                for feat in top_indices:\n",
        "                    me_df[feat] = X_train[feat]\n",
        "\n",
        "                # Add community area as grouping variable\n",
        "                me_df['community_area'] = train_df['community_area'].values\n",
        "\n",
        "                # Impute any missing values\n",
        "                for col in me_df.columns:\n",
        "                    if col != 'community_area' and pd.api.types.is_numeric_dtype(me_df[col]) and me_df[col].isna().any():\n",
        "                        me_df[col] = me_df[col].fillna(safe_median(me_df[col]))\n",
        "\n",
        "                # Fit mixed effects model\n",
        "                formula = f\"y ~ {' + '.join(top_indices)}\"\n",
        "                mixed_model = smf.mixedlm(\n",
        "                    formula,\n",
        "                    me_df,\n",
        "                    groups=me_df['community_area']\n",
        "                )\n",
        "\n",
        "                # Handle convergence warnings\n",
        "                try:\n",
        "                    mixed_result = mixed_model.fit()\n",
        "                except:\n",
        "                    # Try with different optimizer\n",
        "                    log(\"Mixed model optimization failed. Trying with alternative optimizer.\")\n",
        "                    mixed_result = mixed_model.fit(method='powell')\n",
        "\n",
        "                # Create test dataframe for prediction\n",
        "                me_test = pd.DataFrame()\n",
        "                for feat in top_indices:\n",
        "                    me_test[feat] = X_test[feat]\n",
        "\n",
        "                # Impute missing values in test data\n",
        "                for col in me_test.columns:\n",
        "                    if pd.api.types.is_numeric_dtype(me_test[col]) and me_test[col].isna().any():\n",
        "                        me_test[col] = me_test[col].fillna(safe_median(me_test[col]))\n",
        "\n",
        "                # Predict with fixed effects only (simplification)\n",
        "                mixed_pred = mixed_result.predict(me_test)\n",
        "\n",
        "                # Evaluate\n",
        "                mixed_rmse = np.sqrt(mean_squared_error(y_test, mixed_pred))\n",
        "                mixed_r2 = r2_score(y_test, mixed_pred)\n",
        "                mixed_mae = mean_absolute_error(y_test, mixed_pred)\n",
        "\n",
        "                log(f\"Mixed Effects Model - RMSE: {mixed_rmse:.3f}, R²: {mixed_r2:.3f}, MAE: {mixed_mae:.3f}\")\n",
        "                model_results['Mixed Effects'] = {'rmse': mixed_rmse, 'r2': mixed_r2, 'mae': mixed_mae}\n",
        "                trained_models['Mixed Effects'] = (mixed_result, mixed_pred)\n",
        "\n",
        "                # Log model summary\n",
        "                log(\"Mixed Effects Model Summary:\")\n",
        "                for param, value in mixed_result.params.items():\n",
        "                    log(f\"  {param}: {value:.4f}\")\n",
        "\n",
        "                # Add random effects groups variance\n",
        "                if hasattr(mixed_result, 'cov_re'):\n",
        "                    cov_re_value = mixed_result.cov_re\n",
        "                    # Handle multi-dimensional cov_re\n",
        "                    if hasattr(cov_re_value, 'item'):\n",
        "                        log(f\"  Group Var: {cov_re_value.item():.4f}\")\n",
        "                    else:\n",
        "                        log(f\"  Group Var: {safe_convert(cov_re_value):.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log(f\"Error training Mixed Effects model: {str(e)}\")\n",
        "\n",
        "        # 4. Baseline model for comparison\n",
        "        log(\"Training baseline model\")\n",
        "        try:\n",
        "            # Create a pipeline with proper imputation\n",
        "            baseline_pipeline = Pipeline([\n",
        "                ('imputer', SimpleImputer(strategy='median')),\n",
        "                ('model', DummyRegressor(strategy='mean'))\n",
        "            ])\n",
        "\n",
        "            # Train\n",
        "            baseline_pipeline.fit(X_train, y_train)\n",
        "            baseline_pred = baseline_pipeline.predict(X_test)\n",
        "\n",
        "            baseline_rmse = np.sqrt(mean_squared_error(y_test, baseline_pred))\n",
        "            baseline_r2 = r2_score(y_test, baseline_pred)\n",
        "            baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
        "\n",
        "            log(f\"Baseline (Mean) - RMSE: {baseline_rmse:.3f}, R²: {baseline_r2:.3f}, MAE: {baseline_mae:.3f}\")\n",
        "            model_results['Baseline'] = {'rmse': baseline_rmse, 'r2': baseline_r2, 'mae': baseline_mae}\n",
        "            trained_models['Baseline'] = (baseline_pipeline, baseline_pred)\n",
        "        except Exception as e:\n",
        "            log(f\"Error training Baseline model: {str(e)}\")\n",
        "\n",
        "        # ---------- 7. model evaluation and fairness analysis ------------\n",
        "        log(\"Performing model evaluation and fairness analysis\")\n",
        "\n",
        "        # Identify best model\n",
        "        best_model_name = None\n",
        "        best_rmse = float('inf')\n",
        "        for model_name, metrics in model_results.items():\n",
        "            if 'rmse' in metrics and metrics['rmse'] < best_rmse:\n",
        "                best_rmse = metrics['rmse']\n",
        "                best_model_name = model_name\n",
        "\n",
        "        if best_model_name:\n",
        "            log(f\"Best model based on RMSE: {best_model_name} ({best_rmse:.3f})\")\n",
        "        else:\n",
        "            log(\"No valid models found for comparison\")\n",
        "\n",
        "        # Plot model comparison\n",
        "        try:\n",
        "            plot_model_comparison(model_results, VIZ_DIR)\n",
        "            log(\"Created model comparison visualization\")\n",
        "        except Exception as e:\n",
        "            log(f\"Error creating model comparison plot: {str(e)}\")\n",
        "\n",
        "        # Feature importance visualization for GB model\n",
        "        if 'Gradient Boosting' in trained_models:\n",
        "            try:\n",
        "                gb_model, _ = trained_models['Gradient Boosting']\n",
        "\n",
        "                # Create feature importance visualization\n",
        "                try:\n",
        "                    # Extract feature importances with proper error handling\n",
        "                    feature_importances = None\n",
        "                    if hasattr(gb_model, 'feature_importances_'):\n",
        "                        feature_importances = gb_model.feature_importances_\n",
        "                    elif hasattr(gb_model, '_estimator') and hasattr(gb_model._estimator, 'feature_importances_'):\n",
        "                        feature_importances = gb_model._estimator.feature_importances_\n",
        "                    else:\n",
        "                        # Create a surrogate model for feature importance\n",
        "                        log(\"Creating surrogate GradientBoostingRegressor for feature importance\")\n",
        "                        surrogate_gb = GradientBoostingRegressor(n_estimators=100, random_state=RAND_STATE)\n",
        "                        surrogate_gb.fit(X_train_scaled_df, y_train)\n",
        "                        feature_importances = surrogate_gb.feature_importances_\n",
        "\n",
        "                    plt.figure(figsize=(12, 8))\n",
        "                    plt.title('Feature Importances (Gradient Boosting)')\n",
        "                    importances = pd.Series(feature_importances, index=X_train.columns)\n",
        "                    importances = importances.sort_values(ascending=False)\n",
        "                    importances.head(20).plot(kind='barh')\n",
        "                    plt.xlabel('Relative Importance')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f\"{VIZ_DIR}/feature_importance_gb.png\", dpi=200, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "\n",
        "                    log(\"Created feature importance visualization\")\n",
        "                except Exception as e:\n",
        "                    log(f\"Error creating feature importance visualization: {str(e)}\")\n",
        "\n",
        "                # Create SHAP visualizations for explainability\n",
        "                if USE_EXPLAINERS and SHAP_AVAILABLE:\n",
        "                    try:\n",
        "                        shap_values = create_shap_visualizations(\n",
        "                            gb_model,\n",
        "                            X_test_scaled_df,\n",
        "                            X_test.columns,\n",
        "                            VIZ_DIR\n",
        "                        )\n",
        "                        log(\"Created SHAP visualizations\")\n",
        "                    except Exception as e:\n",
        "                        log(f\"Error creating SHAP visualizations: {str(e)}\")\n",
        "\n",
        "                # Create LIME explanations\n",
        "                if USE_EXPLAINERS and LIME_AVAILABLE:\n",
        "                    try:\n",
        "                        create_lime_explanations(\n",
        "                            gb_model,\n",
        "                            X_train_scaled_df,\n",
        "                            X_test_scaled_df,\n",
        "                            X_test.columns,\n",
        "                            VIZ_DIR\n",
        "                        )\n",
        "                        log(\"Created LIME explanations\")\n",
        "                    except Exception as e:\n",
        "                        log(f\"Error creating LIME explanations: {str(e)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log(f\"Error in model explainability: {str(e)}\")\n",
        "\n",
        "        # Ridge coefficient visualization\n",
        "        if 'Ridge' in trained_models:\n",
        "            try:\n",
        "                ridge_pipeline, _ = trained_models['Ridge']\n",
        "\n",
        "                # Get the Ridge model from the pipeline\n",
        "                ridge_model = ridge_pipeline.named_steps.get('ridge')\n",
        "\n",
        "                if ridge_model is not None and hasattr(ridge_model, 'coef_'):\n",
        "                    plt.figure(figsize=(12, 8))\n",
        "                    plt.title('Feature Coefficients (Ridge)')\n",
        "                    coef = pd.Series(ridge_model.coef_, index=X_train.columns)\n",
        "                    coef = coef.reindex(coef.abs().sort_values(ascending=False).index)\n",
        "                    coef.head(20).plot(kind='barh')\n",
        "                    plt.xlabel('Coefficient Value')\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(f\"{VIZ_DIR}/coefficients_ridge.png\", dpi=200, bbox_inches='tight')\n",
        "                    plt.close()\n",
        "                    log(\"Created Ridge coefficient visualization\")\n",
        "                else:\n",
        "                    log(\"Could not access Ridge coefficients\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error creating Ridge coefficient visualization: {str(e)}\")\n",
        "\n",
        "        # Actual vs Predicted plot for best model\n",
        "        if best_model_name in trained_models:\n",
        "            _, best_pred = trained_models[best_model_name]\n",
        "\n",
        "            try:\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                plt.scatter(y_test, best_pred, alpha=0.6)\n",
        "                plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n",
        "                plt.xlabel('Actual')\n",
        "                plt.ylabel('Predicted')\n",
        "                plt.title(f'Actual vs Predicted ({best_model_name})')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{VIZ_DIR}/actual_vs_predicted.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                log(\"Created actual vs predicted plot\")\n",
        "\n",
        "                # Residual plot\n",
        "                residuals = y_test - best_pred\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                plt.scatter(best_pred, residuals, alpha=0.6)\n",
        "                plt.hlines(y=0, xmin=best_pred.min(), xmax=best_pred.max(), colors='k', linestyles='--')\n",
        "                plt.xlabel('Predicted')\n",
        "                plt.ylabel('Residuals')\n",
        "                plt.title(f'Residual Plot ({best_model_name})')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(f\"{VIZ_DIR}/residual_plot.png\", dpi=200, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                log(\"Created residual plot\")\n",
        "\n",
        "                # Create a heatmap visualization of predictions by community area\n",
        "                if FOLIUM_AVAILABLE and 'community_area' in test_df.columns:\n",
        "                    try:\n",
        "                        # Get community area predictions\n",
        "                        community_predictions = pd.DataFrame({\n",
        "                            'community_area': test_df['community_area'],\n",
        "                            'actual': y_test,\n",
        "                            'predicted': best_pred,\n",
        "                            'error': np.abs(y_test - best_pred)\n",
        "                        })\n",
        "\n",
        "                        # Aggregate by community area\n",
        "                        community_results = community_predictions.groupby('community_area').agg({\n",
        "                            'actual': 'mean',\n",
        "                            'predicted': 'mean',\n",
        "                            'error': 'mean'\n",
        "                        }).reset_index()\n",
        "\n",
        "                        try:\n",
        "                            # Check if coordinate columns exist in the community data\n",
        "                            if 'center_lat' not in comm.columns or 'center_lon' not in comm.columns:\n",
        "                                log(\"Missing coordinate columns in community data. Creating them from crime data.\")\n",
        "\n",
        "                                # Create centroid coordinates from crime data\n",
        "                                crime_coords = crime.dropna(subset=['latitude', 'longitude'])\n",
        "\n",
        "                                # Group by community area and calculate mean coordinates\n",
        "                                centroids = crime_coords.groupby('community_area').agg({\n",
        "                                    'latitude': 'mean',\n",
        "                                    'longitude': 'mean'\n",
        "                                }).reset_index()\n",
        "\n",
        "                                # Rename columns to match expected names\n",
        "                                centroids.rename(columns={\n",
        "                                    'latitude': 'center_lat',\n",
        "                                    'longitude': 'center_lon'\n",
        "                                }, inplace=True)\n",
        "\n",
        "                                # Merge with community data\n",
        "                                comm = pd.merge(\n",
        "                                    comm,\n",
        "                                    centroids,\n",
        "                                    on='community_area',\n",
        "                                    how='left'\n",
        "                                )\n",
        "\n",
        "                                log(f\"Created centroid coordinates for {centroids.shape[0]} community areas\")\n",
        "\n",
        "                            # Merge with community data for coordinates\n",
        "                            prediction_map_data = pd.merge(\n",
        "                                community_results,\n",
        "                                comm[['community_area', 'center_lat', 'center_lon']],\n",
        "                                on='community_area',\n",
        "                                how='left'\n",
        "                            )\n",
        "\n",
        "                            # Convert coordinates to numeric\n",
        "                            prediction_map_data['center_lat'] = pd.to_numeric(prediction_map_data['center_lat'], errors='coerce')\n",
        "                            prediction_map_data['center_lon'] = pd.to_numeric(prediction_map_data['center_lon'], errors='coerce')\n",
        "\n",
        "                            # Filter out NaN coordinates\n",
        "                            prediction_map_data = prediction_map_data.dropna(subset=['center_lat', 'center_lon'])\n",
        "\n",
        "                            # Create map with predictions\n",
        "                            m_pred = folium.Map(location=[41.8781, -87.6298], zoom_start=10,\n",
        "                                            tiles='CartoDB positron')\n",
        "\n",
        "                            # Add circles for predicted values\n",
        "                            for _, row in prediction_map_data.iterrows():\n",
        "                                try:\n",
        "                                    ca_lat = float(row['center_lat'])\n",
        "                                    ca_lon = float(row['center_lon'])\n",
        "                                    predicted_val = float(row['predicted'])\n",
        "                                    actual_val = float(row['actual'])\n",
        "                                    error_val = float(row['error'])\n",
        "\n",
        "                                    # Scale prediction for circle size (protect against negative values)\n",
        "                                    circle_radius = np.log1p(max(predicted_val, 0.1)) * 2\n",
        "\n",
        "                                    # Add circle for predicted value\n",
        "                                    folium.CircleMarker(\n",
        "                                        location=[ca_lat, ca_lon],\n",
        "                                        radius=circle_radius,\n",
        "                                        color='blue',\n",
        "                                        fill=True,\n",
        "                                        fill_color='blue',\n",
        "                                        fill_opacity=0.5,\n",
        "                                        popup=f\"Community Area: {row['community_area']}<br>\"\n",
        "                                            f\"Predicted: {predicted_val:.1f}<br>\"\n",
        "                                            f\"Actual: {actual_val:.1f}<br>\"\n",
        "                                            f\"Error: {error_val:.1f}\"\n",
        "                                    ).add_to(m_pred)\n",
        "                                except (ValueError, TypeError) as e:\n",
        "                                    # Skip problematic rows\n",
        "                                    continue\n",
        "\n",
        "                            # Save map\n",
        "                            pred_map_path = f\"{VIZ_DIR}/predicted_crime_map.html\"\n",
        "                            m_pred.save(pred_map_path)\n",
        "                            log(f\"Saved prediction map to {pred_map_path}\")\n",
        "\n",
        "                            # Create map of prediction errors\n",
        "                            m_error = folium.Map(location=[41.8781, -87.6298], zoom_start=10,\n",
        "                                            tiles='CartoDB positron')\n",
        "\n",
        "                            # Find max error for scaling\n",
        "                            max_error = prediction_map_data['error'].max()\n",
        "\n",
        "                            # Add circles for prediction error\n",
        "                            for _, row in prediction_map_data.iterrows():\n",
        "                                try:\n",
        "                                    ca_lat = float(row['center_lat'])\n",
        "                                    ca_lon = float(row['center_lon'])\n",
        "                                    predicted_val = float(row['predicted'])\n",
        "                                    actual_val = float(row['actual'])\n",
        "                                    error_val = float(row['error'])\n",
        "\n",
        "                                    # Scale error for circle size\n",
        "                                    circle_radius = (error_val / max_error) * 15 if max_error > 0 else 5\n",
        "\n",
        "                                    # Add circle with error\n",
        "                                    folium.CircleMarker(\n",
        "                                        location=[ca_lat, ca_lon],\n",
        "                                        radius=circle_radius,\n",
        "                                        color='red',\n",
        "                                        fill=True,\n",
        "                                        fill_color='red',\n",
        "                                        fill_opacity=0.5,\n",
        "                                        popup=f\"Community Area: {row['community_area']}<br>\"\n",
        "                                            f\"Error: {error_val:.1f}<br>\"\n",
        "                                            f\"Actual: {actual_val:.1f}<br>\"\n",
        "                                            f\"Predicted: {predicted_val:.1f}\"\n",
        "                                    ).add_to(m_error)\n",
        "                                except (ValueError, TypeError) as e:\n",
        "                                    # Skip problematic rows\n",
        "                                    continue\n",
        "\n",
        "                            # Save error map\n",
        "                            error_map_path = f\"{VIZ_DIR}/prediction_error_map.html\"\n",
        "                            m_error.save(error_map_path)\n",
        "                            log(f\"Saved error map to {error_map_path}\")\n",
        "                        except Exception as e:\n",
        "                            log(f\"Error creating prediction maps: {str(e)}\")\n",
        "                            import traceback\n",
        "                            log(f\"Detailed error: {traceback.format_exc()}\")\n",
        "                    except Exception as e:\n",
        "                        log(f\"Error creating prediction maps: {str(e)}\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error creating prediction plots: {str(e)}\")\n",
        "\n",
        "        # Fairness analysis\n",
        "        fairness_results = {}\n",
        "        if protected_attrs and best_model_name in trained_models:\n",
        "            try:\n",
        "                _, best_pred = trained_models[best_model_name]\n",
        "                fairness_results = perform_fairness_analysis(y_test, best_pred, protected_attrs, VIZ_DIR)\n",
        "                log(\"Completed fairness analysis\")\n",
        "            except Exception as e:\n",
        "                log(f\"Error performing fairness analysis: {str(e)}\")\n",
        "\n",
        "        # ---------- 8. save results and models -------------------------------\n",
        "        log(\"Saving results and model information\")\n",
        "\n",
        "        # Save model results\n",
        "        try:\n",
        "            results_data = {\n",
        "                'metadata': {\n",
        "                    'date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                    'target_variable': TARGET,\n",
        "                    'years': TIME_RANGE,\n",
        "                    'data_shape': panel_df.shape,\n",
        "                    'train_shape': X_train.shape,\n",
        "                    'val_shape': X_val.shape,\n",
        "                    'test_shape': X_test.shape,\n",
        "                    'best_model': best_model_name,\n",
        "                    'features_used': X_train.columns.tolist(),\n",
        "                    'fairness_mitigation_applied': APPLY_FAIRNESS and FAIRLEARN_AVAILABLE\n",
        "                },\n",
        "                'model_results': {\n",
        "                    model: {\n",
        "                        metric: convert_to_serializable(value)\n",
        "                        for metric, value in metrics.items()\n",
        "                    }\n",
        "                    for model, metrics in model_results.items()\n",
        "                },\n",
        "                'fairness_results': convert_to_serializable(fairness_results)\n",
        "            }\n",
        "\n",
        "            # Save results to JSON\n",
        "            results_path = f\"{MODELS_DIR}/results_{datetime.now():%Y%m%d_%H%M%S}.json\"\n",
        "            with open(results_path, 'w') as f:\n",
        "                json.dump(results_data, f, indent=2)\n",
        "            log(f\"Saved results to {results_path}\")\n",
        "\n",
        "            # Save best model using MLflow if available\n",
        "            if USE_MLFLOW and MLFLOW_AVAILABLE and best_model_name in trained_models:\n",
        "                with mlflow.start_run(run_name=f\"predictive_policing_{best_model_name}\"):\n",
        "                    # Log parameters\n",
        "                    mlflow.log_param(\"target_variable\", TARGET)\n",
        "                    mlflow.log_param(\"time_range\", TIME_RANGE)\n",
        "                    mlflow.log_param(\"feature_count\", X_train.shape[1])\n",
        "                    mlflow.log_param(\"fairness_mitigation\", APPLY_FAIRNESS and FAIRLEARN_AVAILABLE)\n",
        "\n",
        "                    # Log metrics\n",
        "                    for metric, value in model_results[best_model_name].items():\n",
        "                        mlflow.log_metric(metric, value)\n",
        "\n",
        "                    # Log feature importances if available for Gradient Boosting\n",
        "                    if best_model_name == 'Gradient Boosting':\n",
        "                        gb_model, _ = trained_models[best_model_name]\n",
        "                        feature_importances = None\n",
        "\n",
        "                        # Extract feature importances\n",
        "                        if hasattr(gb_model, 'feature_importances_'):\n",
        "                            feature_importances = pd.DataFrame({\n",
        "                                'feature': X_train.columns,\n",
        "                                'importance': gb_model.feature_importances_\n",
        "                            }).sort_values('importance', ascending=False)\n",
        "                        elif hasattr(gb_model, '_estimator') and hasattr(gb_model._estimator, 'feature_importances_'):\n",
        "                            feature_importances = pd.DataFrame({\n",
        "                                'feature': X_train.columns,\n",
        "                                'importance': gb_model._estimator.feature_importances_\n",
        "                            }).sort_values('importance', ascending=False)\n",
        "                        else:\n",
        "                            # Create a surrogate model\n",
        "                            surrogate_gb = GradientBoostingRegressor(n_estimators=100, random_state=RAND_STATE)\n",
        "                            surrogate_gb.fit(X_train_scaled_df, y_train)\n",
        "                            feature_importances = pd.DataFrame({\n",
        "                                'feature': X_train.columns,\n",
        "                                'importance': surrogate_gb.feature_importances_\n",
        "                            }).sort_values('importance', ascending=False)\n",
        "\n",
        "                        if feature_importances is not None:\n",
        "                            # Log as JSON artifact\n",
        "                            feature_imp_path = f\"{MODELS_DIR}/feature_importance_{datetime.now():%Y%m%d_%H%M%S}.json\"\n",
        "                            feature_importances.to_json(feature_imp_path, orient='records')\n",
        "                            mlflow.log_artifact(feature_imp_path)\n",
        "\n",
        "                    # Log model\n",
        "                    if best_model_name == 'Ridge':\n",
        "                        mlflow.sklearn.log_model(trained_models[best_model_name][0], \"ridge_model\")\n",
        "                    elif best_model_name == 'Gradient Boosting':\n",
        "                        mlflow.sklearn.log_model(trained_models[best_model_name][0], \"gb_model\")\n",
        "\n",
        "                    # Log environment info\n",
        "                    conda_env = {\n",
        "                        'channels': ['defaults', 'conda-forge'],\n",
        "                        'dependencies': [\n",
        "                            'python=3.9',\n",
        "                            'pandas',\n",
        "                            'numpy',\n",
        "                            'scikit-learn',\n",
        "                            'matplotlib',\n",
        "                            'seaborn',\n",
        "                            'mlflow',\n",
        "                            'folium',\n",
        "                            'shap',\n",
        "                            'lime',\n",
        "                            'fairlearn',\n",
        "                            'statsmodels'\n",
        "                        ],\n",
        "                        'name': 'predictive_policing_env'\n",
        "                    }\n",
        "\n",
        "                    # Save conda env\n",
        "                    conda_path = f\"{MODELS_DIR}/conda_env_{datetime.now():%Y%m%d_%H%M%S}.yaml\"\n",
        "                    with open(conda_path, 'w') as f:\n",
        "                        yaml.dump(conda_env, f)\n",
        "                    mlflow.log_artifact(conda_path)\n",
        "\n",
        "                    # Log fairness report if available\n",
        "                    if fairness_results:\n",
        "                        fairness_path = f\"{MODELS_DIR}/fairness_report_{datetime.now():%Y%m%d_%H%M%S}.json\"\n",
        "                        with open(fairness_path, 'w') as f:\n",
        "                            json.dump(convert_to_serializable(fairness_results), f, indent=2)\n",
        "                        mlflow.log_artifact(fairness_path)\n",
        "\n",
        "                    log(f\"Saved model artifacts to MLflow\")\n",
        "            else:\n",
        "                # Traditional model saving\n",
        "                import pickle\n",
        "                if best_model_name in trained_models:\n",
        "                    model_path = f\"{MODELS_DIR}/{best_model_name.lower().replace(' ', '_')}_{datetime.now():%Y%m%d_%H%M%S}.pkl\"\n",
        "                    with open(model_path, 'wb') as f:\n",
        "                        pickle.dump(trained_models[best_model_name][0], f)\n",
        "                    log(f\"Saved {best_model_name} model to {model_path}\")\n",
        "                else:\n",
        "                    log(\"No best model to save\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log(f\"Error saving results: {str(e)}\")\n",
        "\n",
        "        # ---------- 9. deployment preparation -----------------------------\n",
        "        log(\"Preparing for model deployment\")\n",
        "\n",
        "        # Export preprocessing steps and model for deployment\n",
        "        try:\n",
        "            # Create a simplified pipeline for deployment with MLflow if available\n",
        "            if USE_MLFLOW and MLFLOW_AVAILABLE and best_model_name in trained_models:\n",
        "                # Create a preprocessing function and model wrapper class\n",
        "                preprocessing_code = f\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlflow.pyfunc import PythonModel\n",
        "\n",
        "class CrimePredictionModel(PythonModel):\n",
        "    \\\"\\\"\\\"\n",
        "    MLflow wrapper for crime prediction model\n",
        "    \\\"\\\"\\\"\n",
        "    def __init__(self):\n",
        "        \\\"\\\"\\\"Initialize the model wrapper\\\"\\\"\\\"\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.scaler.mean_ = {scaler.mean_.tolist() if hasattr(scaler, 'mean_') else None}\n",
        "        self.scaler.scale_ = {scaler.scale_.tolist() if hasattr(scaler, 'scale_') else None}\n",
        "        self.features = {X_train.columns.tolist()}\n",
        "\n",
        "    def load_context(self, context):\n",
        "        \\\"\\\"\\\"Load the model from the MLflow artifact\\\"\\\"\\\"\n",
        "        import pickle\n",
        "        model_path = context.artifacts[\"model\"]\n",
        "        with open(model_path, \"rb\") as f:\n",
        "            self.model = pickle.load(f)\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        \\\"\\\"\\\"Preprocess the data for prediction\\\"\\\"\\\"\n",
        "        # Ensure all required features are present\n",
        "        missing_features = [f for f in self.features if f not in data.columns]\n",
        "        if missing_features:\n",
        "            for feature in missing_features:\n",
        "                data[feature] = 0  # Default value for missing features\n",
        "\n",
        "        # Select and order features\n",
        "        X = data[self.features]\n",
        "\n",
        "        # Fill missing values\n",
        "        for col in X.columns:\n",
        "            if X[col].isna().any():\n",
        "                X[col] = X[col].fillna(X[col].median() if X[col].dtype != 'object' else 'UNKNOWN')\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = pd.DataFrame(\n",
        "            self.scaler.transform(X),\n",
        "            columns=self.features,\n",
        "            index=X.index\n",
        "        )\n",
        "\n",
        "        return X_scaled\n",
        "\n",
        "    def predict(self, context, data):\n",
        "        \\\"\\\"\\\"Make predictions using the model\\\"\\\"\\\"\n",
        "        # Convert to DataFrame if necessary\n",
        "        if not isinstance(data, pd.DataFrame):\n",
        "            data = pd.DataFrame(data)\n",
        "\n",
        "        # Preprocess the data\n",
        "        X_processed = self.preprocess(data)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X_processed)\n",
        "\n",
        "        return predictions\n",
        "\"\"\"\n",
        "\n",
        "                # Save preprocessing code\n",
        "                model_wrapper_path = f\"{MODELS_DIR}/model_wrapper_{datetime.now():%Y%m%d_%H%M%S}.py\"\n",
        "                with open(model_wrapper_path, 'w') as f:\n",
        "                    f.write(preprocessing_code)\n",
        "                log(f\"Saved MLflow model wrapper to {model_wrapper_path}\")\n",
        "\n",
        "                # Create a conda.yaml file for the model\n",
        "                conda_env = {\n",
        "                    'channels': ['defaults', 'conda-forge'],\n",
        "                    'dependencies': [\n",
        "                        'python=3.9',\n",
        "                        'pandas',\n",
        "                        'numpy',\n",
        "                        'scikit-learn',\n",
        "                        'matplotlib',\n",
        "                        'seaborn',\n",
        "                        'mlflow',\n",
        "                        'shap',\n",
        "                        'lime',\n",
        "                        'fairlearn'\n",
        "                    ],\n",
        "                    'name': 'crime_prediction_env'\n",
        "                }\n",
        "\n",
        "                conda_path = f\"{MODELS_DIR}/conda_env_{datetime.now():%Y%m%d_%H%M%S}.yaml\"\n",
        "                with open(conda_path, 'w') as f:\n",
        "                    yaml.dump(conda_env, f)\n",
        "                log(f\"Created conda environment file for deployment\")\n",
        "\n",
        "            else:\n",
        "                # Create a traditional preprocessing script\n",
        "                preprocessing_code = f\"\"\"\n",
        "# Preprocessing code for deployment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def safe_median(s):\n",
        "    \\\"\\\"\\\"Safely compute median of a Series, handling NaN values and conversion issues\\\"\\\"\\\"\n",
        "    try:\n",
        "        median = s.dropna().median()\n",
        "        if hasattr(median, 'item'):\n",
        "            return float(median.item())\n",
        "        else:\n",
        "            return float(median) if pd.notna(median) else 0.0\n",
        "    except (TypeError, ValueError):\n",
        "        return 0.0\n",
        "\n",
        "def preprocess_for_prediction(raw_data):\n",
        "    \\\"\\\"\\\"\n",
        "    Preprocess raw data for prediction with trained model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    raw_data : dict or DataFrame\n",
        "        Raw input data with community area and time features\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    processed_features : DataFrame\n",
        "        Processed features ready for model prediction\n",
        "    \\\"\\\"\\\"\n",
        "    # Convert to DataFrame if dictionary\n",
        "    if isinstance(raw_data, dict):\n",
        "        data = pd.DataFrame([raw_data])\n",
        "    else:\n",
        "        data = raw_data.copy()\n",
        "\n",
        "    # Required features for the model\n",
        "    required_features = {X_train.columns.tolist()}\n",
        "\n",
        "    # Fill missing values\n",
        "    for feature in required_features:\n",
        "        if feature not in data.columns:\n",
        "            data[feature] = 0  # Default value\n",
        "\n",
        "    # Select and order features\n",
        "    X = data[required_features]\n",
        "\n",
        "    # Fill any remaining missing values\n",
        "    for col in X.columns:\n",
        "        if X[col].isna().any():\n",
        "            X[col] = X[col].fillna(safe_median(X[col]) if pd.api.types.is_numeric_dtype(X[col]) else 'UNKNOWN')\n",
        "\n",
        "    # Apply scaling\n",
        "    means = {scaler.mean_.tolist() if hasattr(scaler, 'mean_') else [0]*len(X_train.columns)}\n",
        "    stds = {scaler.scale_.tolist() if hasattr(scaler, 'scale_') else [1]*len(X_train.columns)}\n",
        "\n",
        "    # Apply scaling with error handling\n",
        "    X_scaled = np.zeros(X.shape)\n",
        "    for i, col in enumerate(X.columns):\n",
        "        try:\n",
        "            X_scaled[:, i] = (X[col].values - means[i]) / max(stds[i], 1e-10)  # Avoid division by zero\n",
        "        except Exception as e:\n",
        "            print(f\"Error scaling column {{col}}: {{str(e)}}\")\n",
        "            X_scaled[:, i] = X[col].values  # Use unscaled values as fallback\n",
        "\n",
        "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "    return X_scaled_df\n",
        "\n",
        "# Example usage:\n",
        "# import pickle\n",
        "# with open('model.pkl', 'rb') as f:\n",
        "#     model = pickle.load(f)\n",
        "#\n",
        "# sample_data = {{\n",
        "#     'community_area': '1',\n",
        "#     'month': 1,\n",
        "#     'year': 2024,\n",
        "#     # Add other required feature values\n",
        "# }}\n",
        "#\n",
        "# processed = preprocess_for_prediction(sample_data)\n",
        "# prediction = model.predict(processed)\n",
        "# print(f\"Predicted {TARGET}: {{prediction[0]:.2f}}\")\n",
        "\"\"\"\n",
        "\n",
        "                # Save preprocessing code\n",
        "                preproc_path = f\"{MODELS_DIR}/preprocessing_{datetime.now():%Y%m%d_%H%M%S}.py\"\n",
        "                with open(preproc_path, 'w') as f:\n",
        "                    f.write(preprocessing_code)\n",
        "                log(f\"Saved preprocessing code to {preproc_path}\")\n",
        "\n",
        "                # Create simple example prediction script\n",
        "                if best_model_name in trained_models:\n",
        "                    model_path = f\"{MODELS_DIR}/{best_model_name.lower().replace(' ', '_')}_{datetime.now():%Y%m%d_%H%M%S}.pkl\"\n",
        "                    prediction_code = f\"\"\"\n",
        "# Example prediction script\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Load the model\n",
        "model_path = \"{model_path}\"  # Update this path to where your model is saved\n",
        "\n",
        "# Load model with error handling\n",
        "try:\n",
        "    with open(model_path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {{str(e)}}\")\n",
        "    exit(1)\n",
        "\n",
        "# Import preprocessing function\n",
        "from preprocessing_{datetime.now():%Y%m%d_%H%M%S} import preprocess_for_prediction\n",
        "\n",
        "def predict_crime(input_data):\n",
        "    \\\"\\\"\\\"\n",
        "    Predict crime for the given input data\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_data : dict or DataFrame\n",
        "        Input data with community area and temporal features\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        Predicted crime {TARGET} value\n",
        "    \\\"\\\"\\\"\n",
        "    try:\n",
        "        # Preprocess the data\n",
        "        processed = preprocess_for_prediction(input_data)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(processed)\n",
        "\n",
        "        return float(prediction[0])\n",
        "    except Exception as e:\n",
        "        print(f\"Error making prediction: {{str(e)}}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example data for a community area in a specific month\n",
        "    sample_data = {{\n",
        "        'month': 1,\n",
        "        'year': 2024,\n",
        "        'community_area': '1',\n",
        "        # Add values for required features\n",
        "    }}\n",
        "\n",
        "    result = predict_crime(sample_data)\n",
        "    if result is not None:\n",
        "        print(f\"Predicted {TARGET}: {{result:.2f}}\")\n",
        "    else:\n",
        "        print(\"Prediction failed\")\n",
        "\"\"\"\n",
        "\n",
        "                    # Save prediction script\n",
        "                    predict_path = f\"{MODELS_DIR}/predict_{datetime.now():%Y%m%d_%H%M%S}.py\"\n",
        "                    with open(predict_path, 'w') as f:\n",
        "                        f.write(prediction_code)\n",
        "                    log(f\"Saved prediction script to {predict_path}\")\n",
        "                else:\n",
        "                    log(\"No best model available for creating prediction script\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log(f\"Error preparing deployment files: {str(e)}\")\n",
        "\n",
        "        # ---------- 10. final summary --------------------------------------\n",
        "        log(\"Generating final summary\")\n",
        "\n",
        "        # Create summary of model performance\n",
        "        summary = f\"\"\"\n",
        "# Improved Predictive Policing Model Summary\n",
        "Date: {datetime.now():%Y-%m-%d %H:%M:%S}\n",
        "\n",
        "## Dataset Information\n",
        "- Target Variable: {TARGET}\n",
        "- Panel Data Structure: {len(community_areas)} community areas × {12*len(TIME_RANGE)} months\n",
        "- Time Range: {min(TIME_RANGE)}-{max(TIME_RANGE)}\n",
        "- Total Records: {len(panel_df):,}\n",
        "- Features Used: {X_train.shape[1]}\n",
        "- Fairness Mitigation Applied: {APPLY_FAIRNESS and FAIRLEARN_AVAILABLE}\n",
        "- Explainability Tools Used: {USE_EXPLAINERS and (SHAP_AVAILABLE or LIME_AVAILABLE)}\n",
        "\n",
        "## Model Performance\n",
        "\"\"\"\n",
        "\n",
        "        # Add each model's performance\n",
        "        for model_name, metrics in model_results.items():\n",
        "            rmse = metrics.get('rmse', 'N/A')\n",
        "            r2 = metrics.get('r2', 'N/A')\n",
        "            mae = metrics.get('mae', 'N/A')\n",
        "\n",
        "            if not isinstance(rmse, str):\n",
        "                rmse = f\"{rmse:.3f}\"\n",
        "            if not isinstance(r2, str):\n",
        "                r2 = f\"{r2:.3f}\"\n",
        "            if not isinstance(mae, str):\n",
        "                mae = f\"{mae:.3f}\"\n",
        "\n",
        "            summary += f\"- {model_name}: RMSE = {rmse}, R² = {r2}, MAE = {mae}\\n\"\n",
        "\n",
        "        # Add best model info\n",
        "        if best_model_name:\n",
        "            summary += f\"\\n## Best Model: {best_model_name}\\n\"\n",
        "\n",
        "            # Add feature importance for Gradient Boosting model if available\n",
        "            if best_model_name == 'Gradient Boosting' and 'Gradient Boosting' in trained_models:\n",
        "                gb_model, _ = trained_models['Gradient Boosting']\n",
        "\n",
        "                # Extract feature importances\n",
        "                feature_importances = None\n",
        "                if hasattr(gb_model, 'feature_importances_'):\n",
        "                    feature_importances = pd.Series(gb_model.feature_importances_, index=X_train.columns)\n",
        "                elif hasattr(gb_model, '_estimator') and hasattr(gb_model._estimator, 'feature_importances_'):\n",
        "                    feature_importances = pd.Series(gb_model._estimator.feature_importances_, index=X_train.columns)\n",
        "                else:\n",
        "                    # Use surrogate model as fallback\n",
        "                    try:\n",
        "                        surrogate_gb = GradientBoostingRegressor(n_estimators=100, random_state=RAND_STATE)\n",
        "                        surrogate_gb.fit(X_train_scaled_df, y_train)\n",
        "                        feature_importances = pd.Series(surrogate_gb.feature_importances_, index=X_train.columns)\n",
        "                    except Exception as e:\n",
        "                        log(f\"Error creating surrogate model for feature importance: {str(e)}\")\n",
        "\n",
        "                if feature_importances is not None:\n",
        "                    importances = feature_importances.sort_values(ascending=False)\n",
        "\n",
        "                    summary += \"\\n### Most Important Features (Gradient Boosting):\\n\"\n",
        "                    for i, (col, imp) in enumerate(importances.items()):\n",
        "                        if i < 10:  # Show top 10\n",
        "                            summary += f\"{i+1}. {col}: {imp:.4f}\\n\"\n",
        "\n",
        "            # Add Ridge coefficients if available\n",
        "            if 'Ridge' in trained_models:\n",
        "                ridge_pipeline, _ = trained_models['Ridge']\n",
        "                ridge_model = ridge_pipeline.named_steps.get('ridge')\n",
        "\n",
        "                if ridge_model and hasattr(ridge_model, 'coef_'):\n",
        "                    coef = pd.Series(ridge_model.coef_, index=X_train.columns)\n",
        "                    coef = coef.reindex(coef.abs().sort_values(ascending=False).index)\n",
        "\n",
        "                    summary += \"\\n### Most Important Features (Ridge Coefficients):\\n\"\n",
        "                    for i, (col, coef_val) in enumerate(coef.items()):\n",
        "                        if i < 10:  # Show top 10\n",
        "                            summary += f\"{i+1}. {col}: {coef_val:.4f}\\n\"\n",
        "\n",
        "        # Add fairness analysis results if available\n",
        "        if fairness_results:\n",
        "            summary += \"\\n## Fairness Analysis\\n\"\n",
        "\n",
        "            for attr_name, attr_results in fairness_results.items():\n",
        "                summary += f\"\\n### Protected Attribute: {attr_name}\\n\"\n",
        "\n",
        "                # Handle different result formats\n",
        "                if 'disparities' in attr_results:\n",
        "                    summary += \"Disparity Metrics:\\n\"\n",
        "                    for metric, value in attr_results['disparities'].items():\n",
        "                        if not isinstance(value, str) and not np.isnan(value):\n",
        "                            summary += f\"- {metric}: {value:.3f}\\n\"\n",
        "                elif isinstance(attr_results, dict):\n",
        "                    summary += \"Disparity Metrics:\\n\"\n",
        "                    for metric, value in attr_results.items():\n",
        "                        if not isinstance(value, str) and not np.isnan(value):\n",
        "                            summary += f\"- {metric}: {value:.3f}\\n\"\n",
        "\n",
        "        # Add ethical considerations\n",
        "        summary += \"\"\"\n",
        "## Ethical Considerations\n",
        "- The model predictions should be interpreted as risk factors, not as deterministic outcomes.\n",
        "- Fairness analysis should be regularly repeated to ensure the model does not systematically disadvantage protected groups.\n",
        "- Deployment should include human oversight and routine auditing for bias or unintended consequences.\n",
        "- Transparency about the model's limitations and confidence intervals should be maintained.\n",
        "- The model should be periodically retrained as socioeconomic conditions and crime patterns evolve.\n",
        "\n",
        "## Next Steps & Recommendations\n",
        "1. Incorporate additional datasets (weather, economic indicators, etc.) to improve prediction accuracy.\n",
        "2. Explore more sophisticated fairness mitigation techniques.\n",
        "3. Develop an interactive dashboard for interpretable results by non-technical stakeholders.\n",
        "4. Establish a continuous monitoring system for model drift and bias detection.\n",
        "5. Conduct community engagement to ensure the system aligns with community needs and priorities.\n",
        "\"\"\"\n",
        "\n",
        "        # Save summary to file\n",
        "        summary_path = f\"{MODELS_DIR}/model_summary_{datetime.now():%Y%m%d_%H%M%S}.md\"\n",
        "        with open(summary_path, 'w') as f:\n",
        "            f.write(summary)\n",
        "        log(f\"Saved model summary to {summary_path}\")\n",
        "\n",
        "        log(\"Analysis complete!\")\n",
        "\n",
        "        return {\n",
        "            'model_results': model_results,\n",
        "            'best_model': best_model_name,\n",
        "            'fairness_results': fairness_results,\n",
        "            'summary_path': summary_path\n",
        "        }\n",
        "    else:\n",
        "        log(\"Panel data approach not used. Implement alternative modeling here if needed.\")\n",
        "        return {}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        log(\"Starting predictive policing analysis\")\n",
        "        results = main()\n",
        "        log(\"Analysis completed successfully\")\n",
        "    except Exception as e:\n",
        "        log(f\"Critical error in main execution: {str(e)}\")\n",
        "        import traceback\n",
        "        log(traceback.format_exc())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A8e2xiuaeBHq",
        "outputId": "9019cc3e-d3a1-4277-9e54-18c0835042d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairlearn not available. Install with: pip install fairlearn\n",
            "18:05:44 | Starting predictive policing analysis\n",
            "18:05:50 | Loading cached crime data\n",
            "18:05:50 | Loading cached community data\n",
            "18:05:50 | Loading cached census data\n",
            "18:05:50 | Loading cached boundary data\n",
            "18:05:51 | Applied year filter [2022, 2023, 2024]: 760,811 rows (from 2,000,000)\n",
            "18:05:51 | Data quality check for Crime dataset:\n",
            "18:05:51 |   Shape: (760811, 31)\n",
            "18:05:52 |   Missing values: 119946\n",
            "18:05:52 |   Columns: id, case_number, date, block, iucr, primary_type, description, location_description, arrest, domestic, beat, district, ward, community_area, fbi_code, x_coordinate, y_coordinate, year, updated_on, latitude, longitude, location, :@computed_region_awaf_s7ux, :@computed_region_6mkv_f3dw, :@computed_region_vrxf_vc4k, :@computed_region_bdys_3d7i, :@computed_region_43wa_7qmu, :@computed_region_rpca_8um6, :@computed_region_d9mm_jgwp, :@computed_region_d3ds_rm58, :@computed_region_8hcu_yrd4\n",
            "18:05:53 | Data quality check for Community dataset:\n",
            "18:05:53 |   Shape: (77, 6)\n",
            "18:05:53 |   Missing values: 0\n",
            "18:05:53 |   Columns: the_geom, area_numbe, community, area_num_1, shape_area, shape_len\n",
            "18:05:53 | Data quality check for Census dataset:\n",
            "18:05:53 |   Shape: (78, 9)\n",
            "18:05:53 |   Missing values: 2\n",
            "18:05:53 |   Columns: ca, community_area_name, percent_of_housing_crowded, percent_households_below_poverty, percent_aged_16_unemployed, percent_aged_25_without_high_school_diploma, percent_aged_under_18_or_over_64, per_capita_income_, hardship_index\n",
            "18:05:54 | Converting unhashable values in column location to strings\n",
            "18:05:55 | Converting unhashable values in column the_geom to strings\n",
            "18:05:58 | Converting unhashable values in column the_geom to strings\n",
            "18:05:59 | Standardizing community area columns across datasets\n",
            "18:06:00 | Starting enhanced feature engineering\n",
            "18:06:00 | Extracting temporal features\n",
            "18:06:01 | Created temporal features\n",
            "18:06:01 | Creating spatial features with individual row processing\n",
            "18:06:02 | Created basic spatial features\n",
            "18:06:03 | Processing 753988 rows with valid coordinates\n",
            "18:07:30 | Successfully calculated 753988 distances to downtown\n",
            "18:07:31 | Added minimum distance to downtown by community area\n",
            "18:07:31 | Added community area centroids\n",
            "18:07:32 | Creating simplified crime heatmap with maximum safeguards\n",
            "18:07:32 | Extracted 753988 valid coordinate pairs\n",
            "18:07:32 | Sampled to 50,000 coordinates\n",
            "18:07:32 | Created list with 50000 valid coordinates\n",
            "18:07:32 | Successfully saved simplified heatmap\n",
            "18:07:32 | Creating panel data structure (area × month)\n",
            "18:07:33 |   Column crime_count_lag1 has 113 missing values\n",
            "18:07:33 |   Column crime_count_lag2 has 190 missing values\n",
            "18:07:33 |   Column crime_count_lag3 has 267 missing values\n",
            "18:07:33 |   Column rolling_mean_3m has 35 missing values\n",
            "18:07:33 |   Column rolling_min_3m has 35 missing values\n",
            "18:07:33 |   Column rolling_max_3m has 35 missing values\n",
            "18:07:33 | Created panel dataset with shape (2808, 29)\n",
            "18:07:33 | Panel data missing values: 2844\n",
            "18:07:33 | Creating seasonal patterns visualization\n",
            "18:07:34 | Created monthly crime patterns visualization\n",
            "18:07:34 | Merged panel data with community data: (2808, 41)\n",
            "18:07:34 | Merged panel data with census data: (2808, 50)\n",
            "18:07:34 | Using crime_count as target variable\n",
            "18:07:34 | Converted hardship_index to numeric\n",
            "18:07:34 | Created hardship quartiles for fairness analysis\n",
            "18:07:34 | Identifying features with correlation > 0.8 (safe implementation)\n",
            "18:07:34 | Feature highly correlated with crime_count: crime_count_lag1 (corr=0.968)\n",
            "18:07:34 | Feature highly correlated with crime_count: crime_count_lag2 (corr=0.941)\n",
            "18:07:34 | Feature highly correlated with crime_count: crime_count_lag3 (corr=0.914)\n",
            "18:07:34 | Feature highly correlated with crime_count: rolling_mean_3m (corr=0.962)\n",
            "18:07:34 | Feature highly correlated with crime_count: rolling_min_3m (corr=0.960)\n",
            "18:07:34 | Feature highly correlated with crime_count: rolling_max_3m (corr=0.954)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 18:07:34,753] A new study created in memory with name: no-name-b135333a-97e7-4457-867a-e12e7a2d252a\n",
            "[I 2025-05-02 18:07:34,764] Trial 0 finished with value: 205.52245448432836 and parameters: {'alpha': 0.34077205250558684}. Best is trial 0 with value: 205.52245448432836.\n",
            "[I 2025-05-02 18:07:34,773] Trial 1 finished with value: 205.49649654005714 and parameters: {'alpha': 63.32653429889588}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,783] Trial 2 finished with value: 205.52250643482114 and parameters: {'alpha': 0.3296252095932128}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,792] Trial 3 finished with value: 205.5109348747338 and parameters: {'alpha': 2.931392055633154}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,801] Trial 4 finished with value: 205.52024205181064 and parameters: {'alpha': 0.8196562305184616}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,810] Trial 5 finished with value: 205.52321311514032 and parameters: {'alpha': 0.17843230665357854}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,819] Trial 6 finished with value: 205.52274626269914 and parameters: {'alpha': 0.2782233855576804}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,828] Trial 7 finished with value: 205.52385134934508 and parameters: {'alpha': 0.042576548044348586}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,839] Trial 8 finished with value: 205.52337214878332 and parameters: {'alpha': 0.14451891589595275}. Best is trial 1 with value: 205.49649654005714.\n",
            "[I 2025-05-02 18:07:34,848] Trial 9 finished with value: 205.4778152392475 and parameters: {'alpha': 12.407383124924198}. Best is trial 9 with value: 205.4778152392475.\n",
            "[I 2025-05-02 18:07:34,862] Trial 10 finished with value: 205.47789231847077 and parameters: {'alpha': 12.379662999694311}. Best is trial 9 with value: 205.4778152392475.\n",
            "[I 2025-05-02 18:07:34,875] Trial 11 finished with value: 205.4728939838226 and parameters: {'alpha': 14.269176908530154}. Best is trial 11 with value: 205.4728939838226.\n",
            "[I 2025-05-02 18:07:34,887] Trial 12 finished with value: 205.4847910384364 and parameters: {'alpha': 10.04496676622074}. Best is trial 11 with value: 205.4728939838226.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:07:34 | Excluding percent_aged_16_unemployed (corr=0.806) in favor of percent_households_below_poverty (corr=0.154)\n",
            "18:07:34 | Excluding hardship_index (corr=0.805) in favor of percent_households_below_poverty (corr=0.154)\n",
            "18:07:34 | Excluded 8 features with high correlation\n",
            "18:07:34 | Keeping 43 features\n",
            "18:07:34 | Panel data shape after correlation filtering: (2808, 43)\n",
            "18:07:34 | Train data: (936, 44) rows from 2022\n",
            "18:07:34 | Validation data: (936, 44) rows from 2023\n",
            "18:07:34 | Test data: (936, 44) rows from 2024\n",
            "18:07:34 | Selecting up to 30 important features\n",
            "18:07:34 | Selected 21 features with variance > 0.01\n",
            "18:07:34 | Final feature set: 21 features\n",
            "18:07:34 | Imputing missing values (before: 36 NaNs)\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_households_below_poverty' with median=18.900\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_aged_25_without_high_school_diploma' with median=18.800\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_aged_under_18_or_over_64' with median=38.100\n",
            "18:07:34 | After imputation: 0 NaNs remaining\n",
            "18:07:34 | Imputing missing values (before: 36 NaNs)\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_households_below_poverty' with median=18.900\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_aged_25_without_high_school_diploma' with median=18.800\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_aged_under_18_or_over_64' with median=38.100\n",
            "18:07:34 | After imputation: 0 NaNs remaining\n",
            "18:07:34 | Imputing missing values (before: 36 NaNs)\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_households_below_poverty' with median=18.900\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_aged_25_without_high_school_diploma' with median=18.800\n",
            "18:07:34 |   Imputed 12 missing values in 'percent_aged_under_18_or_over_64' with median=38.100\n",
            "18:07:34 | After imputation: 0 NaNs remaining\n",
            "18:07:34 | Data shapes after imputation - Train: (936, 21), Val: (936, 21), Test: (936, 21)\n",
            "18:07:34 | Top feature correlations with target:\n",
            "18:07:34 |   1. rolling_std_3m: 0.296\n",
            "18:07:34 |   2. percent_households_below_poverty: 0.161\n",
            "18:07:34 |   3. quarter_1: 0.121\n",
            "18:07:34 |   4. month: 0.116\n",
            "18:07:34 |   5. month_2: 0.085\n",
            "18:07:34 |   6. quarter_3: 0.083\n",
            "18:07:34 |   7. month_1: 0.077\n",
            "18:07:34 |   8. percent_aged_25_without_high_school_diploma: 0.071\n",
            "18:07:34 |   9. month_10: 0.059\n",
            "18:07:34 |   10. percent_aged_under_18_or_over_64: 0.056\n",
            "18:07:34 | Protected attributes for fairness analysis: ['hardship_quartile']\n",
            "18:07:34 | Training models with time-based validation\n",
            "18:07:34 | Training Ridge Regression model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 18:07:34,901] Trial 13 finished with value: 205.56335857161022 and parameters: {'alpha': 81.25605744665289}. Best is trial 11 with value: 205.4728939838226.\n",
            "[I 2025-05-02 18:07:34,914] Trial 14 finished with value: 205.47192048817834 and parameters: {'alpha': 14.661599177666869}. Best is trial 14 with value: 205.47192048817834.\n",
            "[I 2025-05-02 18:07:34,928] Trial 15 finished with value: 205.51330155081055 and parameters: {'alpha': 2.378472854063141}. Best is trial 14 with value: 205.47192048817834.\n",
            "[I 2025-05-02 18:07:34,941] Trial 16 finished with value: 205.449860007722 and parameters: {'alpha': 31.08276131594949}. Best is trial 16 with value: 205.449860007722.\n",
            "[I 2025-05-02 18:07:34,955] Trial 17 finished with value: 205.44949459063412 and parameters: {'alpha': 33.50181864900946}. Best is trial 17 with value: 205.44949459063412.\n",
            "[I 2025-05-02 18:07:34,969] Trial 18 finished with value: 205.52399248845052 and parameters: {'alpha': 0.012621349178344173}. Best is trial 17 with value: 205.44949459063412.\n",
            "[I 2025-05-02 18:07:34,982] Trial 19 finished with value: 205.44977626906262 and parameters: {'alpha': 35.76803236117235}. Best is trial 17 with value: 205.44949459063412.\n",
            "[I 2025-05-02 18:07:34,993] A new study created in memory with name: no-name-46dd3e92-f3ec-4394-9901-dd942030334f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:07:34 | Best Ridge alpha: 33.5018\n",
            "18:07:34 | Ridge - RMSE: 213.208, R²: 0.112, MAE: 155.876\n",
            "18:07:34 | Top Ridge coefficients:\n",
            "18:07:34 |   1. rolling_std_3m: 87.0713\n",
            "18:07:34 |   2. percent_households_below_poverty: 46.2021\n",
            "18:07:34 |   3. month_2: 41.3476\n",
            "18:07:34 |   4. percent_aged_25_without_high_school_diploma: 23.0141\n",
            "18:07:34 |   5. percent_aged_under_18_or_over_64: 18.9821\n",
            "18:07:34 | Training Gradient Boosting model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 18:07:35,254] Trial 0 finished with value: 100.92637294630924 and parameters: {'max_iter': 236, 'max_depth': 5, 'learning_rate': 0.045278810185636766, 'max_bins': 95}. Best is trial 0 with value: 100.92637294630924.\n",
            "[I 2025-05-02 18:07:35,531] Trial 1 finished with value: 81.73518662286752 and parameters: {'max_iter': 208, 'max_depth': 6, 'learning_rate': 0.08495372656900747, 'max_bins': 214}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:35,785] Trial 2 finished with value: 138.61361456331218 and parameters: {'max_iter': 186, 'max_depth': 5, 'learning_rate': 0.015732618554731657, 'max_bins': 210}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:35,913] Trial 3 finished with value: 126.90897108034788 and parameters: {'max_iter': 202, 'max_depth': 2, 'learning_rate': 0.06571943613601643, 'max_bins': 219}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:36,148] Trial 4 finished with value: 96.37000375809795 and parameters: {'max_iter': 252, 'max_depth': 4, 'learning_rate': 0.057719117140522316, 'max_bins': 220}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:36,408] Trial 5 finished with value: 115.00559507644151 and parameters: {'max_iter': 182, 'max_depth': 6, 'learning_rate': 0.026802536637453456, 'max_bins': 185}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:36,697] Trial 6 finished with value: 84.24664705781818 and parameters: {'max_iter': 261, 'max_depth': 5, 'learning_rate': 0.07291864451829892, 'max_bins': 160}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:36,940] Trial 7 finished with value: 138.98451358150868 and parameters: {'max_iter': 241, 'max_depth': 4, 'learning_rate': 0.016868203548035563, 'max_bins': 168}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:37,293] Trial 8 finished with value: 85.43652418892259 and parameters: {'max_iter': 275, 'max_depth': 6, 'learning_rate': 0.05338359266128294, 'max_bins': 66}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:37,475] Trial 9 finished with value: 156.4826583646585 and parameters: {'max_iter': 229, 'max_depth': 3, 'learning_rate': 0.01054874757706951, 'max_bins': 196}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:37,659] Trial 10 finished with value: 126.40159855544522 and parameters: {'max_iter': 99, 'max_depth': 6, 'learning_rate': 0.03393071980632581, 'max_bins': 107}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:37,819] Trial 11 finished with value: 98.10304920560694 and parameters: {'max_iter': 125, 'max_depth': 5, 'learning_rate': 0.09613425938674641, 'max_bins': 248}. Best is trial 1 with value: 81.73518662286752.\n",
            "[I 2025-05-02 18:07:38,167] Trial 12 finished with value: 74.33931332217561 and parameters: {'max_iter': 300, 'max_depth': 5, 'learning_rate': 0.09900954680172297, 'max_bins': 143}. Best is trial 12 with value: 74.33931332217561.\n",
            "[I 2025-05-02 18:07:38,559] Trial 13 finished with value: 74.36391508038218 and parameters: {'max_iter': 300, 'max_depth': 6, 'learning_rate': 0.08640257951228268, 'max_bins': 130}. Best is trial 12 with value: 74.33931332217561.\n",
            "[I 2025-05-02 18:07:38,923] Trial 14 finished with value: 73.51254893924836 and parameters: {'max_iter': 300, 'max_depth': 5, 'learning_rate': 0.09674346907422421, 'max_bins': 132}. Best is trial 14 with value: 73.51254893924836.\n",
            "[I 2025-05-02 18:07:39,155] Trial 15 finished with value: 115.31769352882249 and parameters: {'max_iter': 297, 'max_depth': 3, 'learning_rate': 0.039487386183339974, 'max_bins': 134}. Best is trial 14 with value: 73.51254893924836.\n",
            "[I 2025-05-02 18:07:39,335] Trial 16 finished with value: 137.1035529633988 and parameters: {'max_iter': 155, 'max_depth': 4, 'learning_rate': 0.027445894747951224, 'max_bins': 39}. Best is trial 14 with value: 73.51254893924836.\n",
            "[I 2025-05-02 18:07:39,619] Trial 17 finished with value: 79.31331383618279 and parameters: {'max_iter': 277, 'max_depth': 4, 'learning_rate': 0.09920097478214181, 'max_bins': 103}. Best is trial 14 with value: 73.51254893924836.\n",
            "[I 2025-05-02 18:07:39,740] Trial 18 finished with value: 121.84981635911237 and parameters: {'max_iter': 73, 'max_depth': 5, 'learning_rate': 0.06937899755801946, 'max_bins': 155}. Best is trial 14 with value: 73.51254893924836.\n",
            "[I 2025-05-02 18:07:39,823] Trial 19 finished with value: 156.31795031277017 and parameters: {'max_iter': 50, 'max_depth': 3, 'learning_rate': 0.04745003896800573, 'max_bins': 79}. Best is trial 14 with value: 73.51254893924836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:07:39 | Best GB parameters: {'max_iter': 300, 'max_depth': 5, 'learning_rate': 0.09674346907422421, 'max_bins': 132, 'random_state': 42, 'early_stopping': True}\n",
            "18:07:40 | Gradient Boosting - RMSE: 78.108, R²: 0.881, MAE: 53.165\n",
            "18:07:40 | Creating surrogate GradientBoostingRegressor for feature importance\n",
            "18:07:40 | Top GB feature importances:\n",
            "18:07:40 |   1. rolling_std_3m: 0.4013\n",
            "18:07:40 |   2. percent_households_below_poverty: 0.2310\n",
            "18:07:40 |   3. percent_aged_under_18_or_over_64: 0.1431\n",
            "18:07:40 |   4. month: 0.1359\n",
            "18:07:40 |   5. percent_aged_25_without_high_school_diploma: 0.0814\n",
            "18:07:40 | Training Mixed Effects Model\n",
            "18:07:40 | Mixed Effects Model - RMSE: 225.154, R²: 0.009, MAE: 166.917\n",
            "18:07:40 | Mixed Effects Model Summary:\n",
            "18:07:40 |   Intercept: 206.6106\n",
            "18:07:40 |   rolling_std_3m: -0.0593\n",
            "18:07:40 |   percent_households_below_poverty: 3.0425\n",
            "18:07:40 |   quarter_1: -58.2789\n",
            "18:07:40 |   Group Var: 19.7901\n",
            "18:07:40 |   Group Var: 0.0000\n",
            "18:07:40 | Training baseline model\n",
            "18:07:40 | Baseline (Mean) - RMSE: 227.045, R²: -0.007, MAE: 168.969\n",
            "18:07:40 | Performing model evaluation and fairness analysis\n",
            "18:07:40 | Best model based on RMSE: Gradient Boosting (78.108)\n",
            "18:07:41 | Model comparison chart created successfully\n",
            "18:07:41 | Created model comparison visualization\n",
            "18:07:41 | Creating surrogate GradientBoostingRegressor for feature importance\n",
            "18:07:42 | Created feature importance visualization\n",
            "18:07:42 | Creating SHAP visualizations for model explainability\n",
            "18:07:42 | Creating SHAP explainer for HistGradientBoostingRegressor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PermutationExplainer explainer: 101it [00:20,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:08:02 | Generated SHAP values using modern API\n",
            "18:08:02 | Created SHAP bar summary plot\n",
            "18:08:03 | Created SHAP beeswarm plot\n",
            "18:08:03 | Created SHAP dependence plot for rolling_std_3m\n",
            "18:08:04 | Created SHAP dependence plot for percent_aged_under_18_or_over_64\n",
            "18:08:04 | Created SHAP dependence plot for percent_households_below_poverty\n",
            "18:08:04 | SHAP visualizations created successfully\n",
            "18:08:04 | Created SHAP visualizations\n",
            "18:08:04 | Creating LIME explanations for sample predictions\n",
            "18:08:04 | LIME explanation for instance 65:\n",
            "18:08:04 |   percent_households_below_poverty <= -0.74: -89.9617\n",
            "18:08:04 |   -0.45 < percent_aged_under_18_or_over_64 <= 0.31: 51.9652\n",
            "18:08:04 |   -0.13 < percent_aged_25_without_high_school_diploma <= 0.54: -35.0732\n",
            "18:08:04 |   -0.28 < rolling_std_3m <= 0.02: -16.9874\n",
            "18:08:04 |   month_10 <= -0.30: -15.0898\n",
            "18:08:04 |   quarter_3 <= -0.58: -14.0304\n",
            "18:08:04 |   month_5 <= -0.30: -12.8422\n",
            "18:08:04 |   month_4 <= -0.30: 12.0858\n",
            "18:08:04 |   month_7 <= -0.30: -10.4052\n",
            "18:08:04 |   -0.80 < month <= 0.00: 10.3740\n",
            "18:08:05 | LIME explanation for instance 763:\n",
            "18:08:05 |   percent_households_below_poverty > 0.66: 100.8407\n",
            "18:08:05 |   rolling_std_3m <= -0.46: -95.1176\n",
            "18:08:05 |   percent_aged_25_without_high_school_diploma > 0.54: -46.4050\n",
            "18:08:05 |   0.00 < month <= 0.80: 35.4214\n",
            "18:08:05 |   percent_aged_under_18_or_over_64 > 0.65: -31.2713\n",
            "18:08:05 |   month_10 <= -0.30: -21.8545\n",
            "18:08:05 |   month_3 <= -0.30: -6.5185\n",
            "18:08:05 |   month_2 <= -0.30: -5.4463\n",
            "18:08:05 |   quarter_2 <= -0.58: 5.3865\n",
            "18:08:05 |   month_8 > -0.30: -5.2429\n",
            "18:08:05 | LIME explanation for instance 675:\n",
            "18:08:05 |   percent_aged_25_without_high_school_diploma > 0.54: -47.2953\n",
            "18:08:05 |   -0.24 < percent_households_below_poverty <= 0.66: 30.8325\n",
            "18:08:05 |   -0.28 < rolling_std_3m <= 0.02: -24.8690\n",
            "18:08:05 |   0.31 < percent_aged_under_18_or_over_64 <= 0.65: -22.7262\n",
            "18:08:05 |   month_10 <= -0.30: -19.3549\n",
            "18:08:05 |   -0.80 < month <= 0.00: 15.3913\n",
            "18:08:05 |   month_2 <= -0.30: 13.0939\n",
            "18:08:05 |   month_11 <= -0.30: -9.1416\n",
            "18:08:05 |   month_8 <= -0.30: -7.4277\n",
            "18:08:05 |   month_1 <= -0.30: 6.0748\n",
            "18:08:05 | LIME explanation for instance 578:\n",
            "18:08:05 |   percent_households_below_poverty > 0.66: 108.8913\n",
            "18:08:05 |   month <= -0.80: -70.4096\n",
            "18:08:05 |   percent_aged_25_without_high_school_diploma > 0.54: -47.5777\n",
            "18:08:05 |   percent_aged_under_18_or_over_64 > 0.65: -27.3012\n",
            "18:08:05 |   -0.28 < rolling_std_3m <= 0.02: -21.1695\n",
            "18:08:05 |   month_4 <= -0.30: 15.6586\n",
            "18:08:05 |   month_7 <= -0.30: -14.9602\n",
            "18:08:05 |   month_5 <= -0.30: 10.9642\n",
            "18:08:05 |   month_6 <= -0.30: 8.6505\n",
            "18:08:05 |   month_3 > -0.30: 7.4202\n",
            "18:08:06 | LIME explanation for instance 35:\n",
            "18:08:06 |   percent_households_below_poverty <= -0.74: -100.0825\n",
            "18:08:06 |   percent_aged_25_without_high_school_diploma <= -0.73: 99.5985\n",
            "18:08:06 |   rolling_std_3m <= -0.46: -87.2629\n",
            "18:08:06 |   month > 0.80: 33.5189\n",
            "18:08:06 |   month_10 <= -0.30: -25.5025\n",
            "18:08:06 |   0.31 < percent_aged_under_18_or_over_64 <= 0.65: -19.0181\n",
            "18:08:06 |   quarter_3 <= -0.58: -9.2925\n",
            "18:08:06 |   month_5 <= -0.30: 8.8525\n",
            "18:08:06 |   month_12 > -0.30: 8.7037\n",
            "18:08:06 |   month_1 <= -0.30: -7.4125\n",
            "18:08:06 | Created LIME explanations for 5 instances\n",
            "18:08:06 | Created LIME explanations\n",
            "18:08:06 | Created Ridge coefficient visualization\n",
            "18:08:07 | Created actual vs predicted plot\n",
            "18:08:07 | Created residual plot\n",
            "18:08:07 | Missing coordinate columns in community data. Creating them from crime data.\n",
            "18:08:07 | Created centroid coordinates for 78 community areas\n",
            "18:08:07 | Saved prediction map to /content/drive/MyDrive/predictive_policing/viz/predicted_crime_map.html\n",
            "18:08:07 | Saved error map to /content/drive/MyDrive/predictive_policing/viz/prediction_error_map.html\n",
            "18:08:07 | Conducting fairness analysis\n",
            "18:08:07 | Fairness analysis for hardship_quartile\n",
            "18:08:07 |   hardship_quartile=High:\n",
            "18:08:07 |     rmse: 96.470\n",
            "18:08:07 |     mae: 62.727\n",
            "18:08:07 |     mean_prediction: 303.859\n",
            "18:08:07 |     median_prediction: 268.873\n",
            "18:08:07 |   hardship_quartile=Low:\n",
            "18:08:07 |     rmse: 81.691\n",
            "18:08:07 |     mae: 60.320\n",
            "18:08:07 |     mean_prediction: 298.758\n",
            "18:08:07 |     median_prediction: 245.748\n",
            "18:08:07 |   hardship_quartile=Medium-High:\n",
            "18:08:07 |     rmse: 72.419\n",
            "18:08:07 |     mae: 48.949\n",
            "18:08:07 |     mean_prediction: 232.916\n",
            "18:08:07 |     median_prediction: 164.621\n",
            "18:08:07 |   hardship_quartile=Medium-Low:\n",
            "18:08:07 |     rmse: 57.413\n",
            "18:08:07 |     mae: 40.930\n",
            "18:08:07 |     mean_prediction: 176.800\n",
            "18:08:07 |     median_prediction: 141.570\n",
            "18:08:07 |   rmse disparity: 39.057\n",
            "18:08:07 |   mae disparity: 21.797\n",
            "18:08:07 |   mean_prediction disparity: 127.059\n",
            "18:08:07 |   median_prediction disparity: 127.303\n",
            "18:08:08 | Completed fairness analysis\n",
            "18:08:08 | Saving results and model information\n",
            "18:08:08 | Saved results to /content/drive/MyDrive/predictive_policing/models/results_20250502_180808.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[31m2025/05/02 18:08:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:08:11 | Saved model artifacts to MLflow\n",
            "18:08:11 | Preparing for model deployment\n",
            "18:08:11 | Saved MLflow model wrapper to /content/drive/MyDrive/predictive_policing/models/model_wrapper_20250502_180811.py\n",
            "18:08:11 | Created conda environment file for deployment\n",
            "18:08:11 | Generating final summary\n",
            "18:08:11 | Saved model summary to /content/drive/MyDrive/predictive_policing/models/model_summary_20250502_180811.md\n",
            "18:08:11 | Analysis complete!\n",
            "18:08:11 | Analysis completed successfully\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}