{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67fb2a03e4794ea0b069f5547decdb36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d4d95eef3914ca5a0cdb145cc9611c8",
              "IPY_MODEL_606f0973fd0049e783df75e268d47be3",
              "IPY_MODEL_c429d259cafc4abaa589c6ea22fca2f5"
            ],
            "layout": "IPY_MODEL_31bfa5c6662242ea97895053581e80c2"
          }
        },
        "4d4d95eef3914ca5a0cdb145cc9611c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3b503c1bbc40a88ced96f0180e4a16",
            "placeholder": "​",
            "style": "IPY_MODEL_09c76224838148a29d5f8d8709ecef67",
            "value": "Fetching Chicago Crime Data: 100%"
          }
        },
        "606f0973fd0049e783df75e268d47be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9698779de2746a2a7e3137e74d3849b",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f3ffdec2a984360a11c6b23e07e8a75",
            "value": 100000
          }
        },
        "c429d259cafc4abaa589c6ea22fca2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_754fe2d403be4fe59e5d312988a40b07",
            "placeholder": "​",
            "style": "IPY_MODEL_00ee158d5a1241c4b2a17dde8d2a1cbb",
            "value": " 100000/100000 [00:52&lt;00:00, 1911.17rec/s]"
          }
        },
        "31bfa5c6662242ea97895053581e80c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e3b503c1bbc40a88ced96f0180e4a16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09c76224838148a29d5f8d8709ecef67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9698779de2746a2a7e3137e74d3849b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f3ffdec2a984360a11c6b23e07e8a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "754fe2d403be4fe59e5d312988a40b07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00ee158d5a1241c4b2a17dde8d2a1cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ethical and Parallelized Predictive Policing System\n",
        "\n",
        "This implementation combines fairness-aware AI, model explainability,\n",
        "and parallelization techniques for ethical crime prediction.\n",
        "\n",
        "Key components:\n",
        "1. Data processing with parallelization (OpenMP/multiprocessing)\n",
        "2. Fairness-aware model training with adversarial debiasing\n",
        "3. Model explainability with SHAP and LIME\n",
        "4. Ethical warning system for bias detection\n",
        "5. Visualization dashboard\n",
        "\n",
        "Authors: Sai Rishi Kiran Mannava, Venkata Akash Reddy Kakunuri, Anthony Kwasi\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# ML libraries\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, VarianceThreshold\n",
        "\n",
        "# For working with large datasets\n",
        "try:\n",
        "    import dask.dataframe as dd\n",
        "    DASK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DASK_AVAILABLE = False\n",
        "\n",
        "# For model explainability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import lime\n",
        "    import lime.lime_tabular\n",
        "    LIME_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIME_AVAILABLE = False\n",
        "\n",
        "# For fairness-aware AI\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    TF_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TF_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import aif360\n",
        "    from aif360.algorithms.preprocessing import Reweighing\n",
        "    from aif360.datasets import BinaryLabelDataset\n",
        "    AIF360_AVAILABLE = True\n",
        "except ImportError:\n",
        "    AIF360_AVAILABLE = False\n",
        "\n",
        "# For GPU acceleration\n",
        "try:\n",
        "    import cupy as cp\n",
        "    CUPY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CUPY_AVAILABLE = False\n",
        "\n",
        "# For web dashboard\n",
        "try:\n",
        "    from flask import Flask, render_template, request, jsonify\n",
        "    FLASK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    FLASK_AVAILABLE = False\n",
        "\n",
        "# Import for Google Colab if available\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Configure warning handling\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------------------------------------------------------\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # Set base directory based on environment\n",
        "        if IN_COLAB:\n",
        "            # Mount Google Drive in Colab\n",
        "            drive.mount('/content/drive')\n",
        "            self.base_dir = \"/content/drive/MyDrive/predictive_policing\"\n",
        "        else:\n",
        "            # Use current directory if not in Colab\n",
        "            self.base_dir = os.path.join(os.getcwd(), \"predictive_policing\")\n",
        "\n",
        "        # Define subdirectories\n",
        "        self.data_dir = os.path.join(self.base_dir, \"data\")\n",
        "        self.results_dir = os.path.join(self.base_dir, \"results\")\n",
        "        self.logs_dir = os.path.join(self.base_dir, \"logs\")\n",
        "        self.cache_dir = os.path.join(self.base_dir, \"cache\")\n",
        "        self.viz_dir = os.path.join(self.results_dir, \"visualizations\")\n",
        "        self.models_dir = os.path.join(self.base_dir, \"models\")\n",
        "        self.templates_dir = os.path.join(self.base_dir, \"templates\")\n",
        "\n",
        "        # Create directories\n",
        "        for directory in [self.base_dir, self.data_dir, self.results_dir, self.logs_dir,\n",
        "                          self.cache_dir, self.viz_dir, self.models_dir, self.templates_dir]:\n",
        "            os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "        # Data loading parameters\n",
        "        self.max_crime_records = 10000000\n",
        "        self.batch_size = 1000000\n",
        "        self.use_caching = True\n",
        "        self.use_dask = DASK_AVAILABLE\n",
        "        self.chunk_size = 500000\n",
        "\n",
        "        # Parallelization settings\n",
        "        self.use_parallel = True\n",
        "        self.num_workers = cpu_count()\n",
        "        self.use_gpu = CUPY_AVAILABLE\n",
        "\n",
        "        # Time period for analysis\n",
        "        self.years = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
        "        self.time_split_method = 'by_year'  # 'by_year' or 'random'\n",
        "\n",
        "        # Model parameters\n",
        "        self.random_state = 42\n",
        "        self.test_size = 0.2\n",
        "        self.cv_folds = 5\n",
        "        self.target = \"crime_count\"\n",
        "        self.stratify_target = False\n",
        "        self.scale_features = True\n",
        "        self.feature_selection = True\n",
        "        self.imputation_strategy = 'median'\n",
        "        self.max_features = 30  # Added to fix the missing attribute error\n",
        "\n",
        "        # Fairness settings\n",
        "        self.fairness_threshold = 0.8\n",
        "        self.reporting_bias_correction = True\n",
        "        self.use_adversarial_debiasing = TF_AVAILABLE\n",
        "        self.use_reweighting = AIF360_AVAILABLE\n",
        "        self.debiasing_protected_attrs = ['hardship_index', 'per_capita_income', 'pct_poverty']\n",
        "\n",
        "        # Protected attributes for fairness analysis\n",
        "        self.protected_attributes = [\n",
        "            \"hardship_index\", \"per_capita_income\", \"pct_poverty\",\n",
        "            \"pct_unemployed\", \"pct_no_hs_diploma\", \"pct_housing_crowded\",\n",
        "            \"percent_aged_16_unemployed\", \"percent_aged_25_without_high_school_diploma\",\n",
        "            \"percent_households_below_poverty\", \"per_capita_income\",\n",
        "            \"socioeconomic_score\"\n",
        "        ]\n",
        "\n",
        "        # Explainability settings\n",
        "        self.use_shap = SHAP_AVAILABLE\n",
        "        self.use_lime = LIME_AVAILABLE\n",
        "        self.num_features_explain = 10\n",
        "\n",
        "        # Performance metrics\n",
        "        self.metrics = [\"r2\", \"rmse\", \"mae\", \"mape\"]\n",
        "\n",
        "        # Uncertainty quantification parameters\n",
        "        self.enable_uncertainty = True\n",
        "        self.prediction_intervals = True\n",
        "        self.bootstrap_samples = 50\n",
        "\n",
        "        # Hyperparameter tuning\n",
        "        self.hyperparameter_tuning = True\n",
        "        self.hyperparameter_method = \"random\"  # 'random' or 'bayesian'\n",
        "        self.n_hyperopt_evals = 50\n",
        "\n",
        "        # API endpoints\n",
        "        self.chicago_crime_api = \"https://data.cityofchicago.org/resource/ijzp-q8t2.json\"\n",
        "        self.chicago_community_api = \"https://data.cityofchicago.org/resource/igwz-8jzy.json\"\n",
        "        self.chicago_census_api = \"https://data.cityofchicago.org/resource/kn9c-c2s2.json\"\n",
        "        self.api_timeout = 120\n",
        "        self.api_retries = 3\n",
        "        self.api_backoff = 2\n",
        "\n",
        "        # Dashboard settings\n",
        "        self.enable_dashboard = FLASK_AVAILABLE\n",
        "        self.dashboard_port = 5000\n",
        "        self.dashboard_host = \"127.0.0.1\"\n",
        "\n",
        "        # Column mapping for standardization\n",
        "        self.column_mapping = {\n",
        "            \"community_area\": \"community_area\",\n",
        "            \"block\": \"block\",\n",
        "            \"area_numbe\": \"community_area_id\",\n",
        "            \"area_number\": \"community_area_id\",\n",
        "            \"community\": \"community_name\",\n",
        "            \"communityname\": \"community_name\",\n",
        "            \"per_capita_income\": \"per_capita_income\",\n",
        "            \"hardship_index\": \"hardship_index\",\n",
        "            \"percent_households_below_poverty\": \"pct_poverty\",\n",
        "            \"percent_aged_16_unemployed\": \"pct_unemployed\",\n",
        "            \"percent_aged_25_without_high_school_diploma\": \"pct_no_hs_diploma\",\n",
        "            \"percent_housing_crowded\": \"pct_housing_crowded\",\n",
        "            \"ca\": \"community_area\",\n",
        "            \"community_area_name\": \"community_name\"\n",
        "        }\n",
        "\n",
        "# Global configuration\n",
        "CONFIG = Config()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# LOGGER SETUP\n",
        "# ---------------------------------------------------------------------\n",
        "def setup_logger():\n",
        "    \"\"\"Set up and configure the logging system\"\"\"\n",
        "    logger = logging.getLogger(\"predictive_policing\")\n",
        "    if logger.handlers:\n",
        "        return logger\n",
        "\n",
        "    logger.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Console handler\n",
        "    ch = logging.StreamHandler()\n",
        "    ch.setLevel(logging.INFO)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "    # File handler\n",
        "    log_file = os.path.join(CONFIG.logs_dir, f\"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "    fh = logging.FileHandler(log_file)\n",
        "    fh.setLevel(logging.DEBUG)\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    return logger\n",
        "\n",
        "logger = setup_logger()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# MEMORY MANAGEMENT\n",
        "# ---------------------------------------------------------------------\n",
        "def clear_memory():\n",
        "    \"\"\"Force garbage collection to free memory\"\"\"\n",
        "    gc.collect()\n",
        "\n",
        "def log_memory_usage():\n",
        "    \"\"\"Log current memory usage\"\"\"\n",
        "    try:\n",
        "        import psutil\n",
        "        process = psutil.Process(os.getpid())\n",
        "        memory_mb = process.memory_info().rss / (1024 * 1024)\n",
        "        logger.info(f\"Current memory usage: {memory_mb:.2f} MB\")\n",
        "    except ImportError:\n",
        "        logger.warning(\"psutil not available, skipping memory usage logging\")\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PARALLELIZATION UTILITIES\n",
        "# ---------------------------------------------------------------------\n",
        "def parallelize_dataframe(df, func, n_cores=None):\n",
        "    \"\"\"\n",
        "    Parallelize operations on a dataframe using Python's multiprocessing\n",
        "\n",
        "    Args:\n",
        "        df: Pandas DataFrame to process\n",
        "        func: Function to apply to each chunk\n",
        "        n_cores: Number of CPU cores to use (defaults to all available)\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame\n",
        "    \"\"\"\n",
        "    if n_cores is None:\n",
        "        n_cores = CONFIG.num_workers\n",
        "\n",
        "    # Ensure we don't create more partitions than we have rows\n",
        "    n_cores = min(n_cores, len(df))\n",
        "\n",
        "    if n_cores <= 1:\n",
        "        return func(df)\n",
        "\n",
        "    logger.info(f\"Parallelizing dataframe processing with {n_cores} cores\")\n",
        "\n",
        "    # Split dataframe into chunks\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "\n",
        "    # Process in parallel\n",
        "    with Pool(n_cores) as pool:\n",
        "        df_list = pool.map(func, df_split)\n",
        "\n",
        "    # Combine results\n",
        "    return pd.concat(df_list)\n",
        "\n",
        "def process_data_in_chunks_parallel(df, chunk_size=None, processor_func=None):\n",
        "    \"\"\"\n",
        "    Process a large dataframe in chunks with parallelization\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame to process\n",
        "        chunk_size: Size of each chunk\n",
        "        processor_func: Function to apply to each chunk\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame\n",
        "    \"\"\"\n",
        "    if df is None or processor_func is None:\n",
        "        return df\n",
        "\n",
        "    if chunk_size is None:\n",
        "        chunk_size = CONFIG.chunk_size\n",
        "\n",
        "    # If DataFrame is small enough, process directly with parallelization\n",
        "    if len(df) <= chunk_size:\n",
        "        return parallelize_dataframe(df, processor_func)\n",
        "\n",
        "    # For larger DataFrames, process in chunks\n",
        "    n_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
        "    logger.info(f\"Processing dataframe in {n_chunks} chunks with parallelization\")\n",
        "\n",
        "    results = []\n",
        "    for i in range(n_chunks):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min((i + 1) * chunk_size, len(df))\n",
        "\n",
        "        logger.info(f\"Processing chunk {i+1}/{n_chunks} (rows {start_idx}-{end_idx})\")\n",
        "        chunk = df.iloc[start_idx:end_idx].copy()\n",
        "\n",
        "        # Process chunk with parallelization\n",
        "        processed_chunk = parallelize_dataframe(chunk, processor_func)\n",
        "        results.append(processed_chunk)\n",
        "\n",
        "        # Clear memory\n",
        "        del chunk\n",
        "        clear_memory()\n",
        "\n",
        "    # Combine results\n",
        "    if results:\n",
        "        if isinstance(results[0], pd.DataFrame):\n",
        "            try:\n",
        "                return pd.concat(results, ignore_index=True)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error concatenating chunks: {e}\")\n",
        "                return results\n",
        "        else:\n",
        "            return results\n",
        "    return None\n",
        "\n",
        "def process_with_dask(df, processor_func=None, npartitions=None):\n",
        "    \"\"\"\n",
        "    Process data using Dask for better memory usage with large datasets\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame to process\n",
        "        processor_func: Function to apply to each partition\n",
        "        npartitions: Number of partitions to use\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame\n",
        "    \"\"\"\n",
        "    if df is None or processor_func is None or not DASK_AVAILABLE:\n",
        "        if not DASK_AVAILABLE:\n",
        "            logger.warning(\"Dask not available, falling back to chunk processing\")\n",
        "        return process_data_in_chunks_parallel(df, processor_func=processor_func)\n",
        "\n",
        "    # Determine number of partitions based on data size\n",
        "    if npartitions is None:\n",
        "        npartitions = max(1, len(df) // CONFIG.chunk_size)\n",
        "\n",
        "    logger.info(f\"Processing with Dask using {npartitions} partitions\")\n",
        "\n",
        "    try:\n",
        "        # Convert to Dask DataFrame\n",
        "        ddf = dd.from_pandas(df, npartitions=npartitions)\n",
        "\n",
        "        # Apply processing function\n",
        "        result_ddf = ddf.map_partitions(processor_func)\n",
        "\n",
        "        # Convert back to pandas\n",
        "        result = result_ddf.compute()\n",
        "        logger.info(f\"Dask processing complete, result shape: {result.shape}\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in Dask processing: {e}\")\n",
        "        # Fallback to chunk processing\n",
        "        logger.info(\"Falling back to chunk processing\")\n",
        "        return process_data_in_chunks_parallel(df, processor_func=processor_func)\n",
        "\n",
        "def gpu_accelerate(X_data, function_to_apply):\n",
        "    \"\"\"\n",
        "    Apply GPU acceleration to numerical operations if CUDA is available\n",
        "\n",
        "    Args:\n",
        "        X_data: NumPy array to process\n",
        "        function_to_apply: Function to apply using GPU\n",
        "\n",
        "    Returns:\n",
        "        Processed data\n",
        "    \"\"\"\n",
        "    if not CONFIG.use_gpu or not CUPY_AVAILABLE:\n",
        "        # Fall back to CPU\n",
        "        return function_to_apply(X_data)\n",
        "\n",
        "    try:\n",
        "        # Transfer data to GPU\n",
        "        X_gpu = cp.array(X_data)\n",
        "\n",
        "        # Apply function on GPU\n",
        "        result_gpu = function_to_apply(X_gpu)\n",
        "\n",
        "        # Transfer results back to CPU\n",
        "        result = cp.asnumpy(result_gpu)\n",
        "\n",
        "        # Free GPU memory\n",
        "        del X_gpu, result_gpu\n",
        "        cp.get_default_memory_pool().free_all_blocks()\n",
        "\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in GPU processing: {e}\")\n",
        "        # Fall back to CPU\n",
        "        return function_to_apply(X_data)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# DATA HELPERS\n",
        "# ---------------------------------------------------------------------\n",
        "def standardize_columns(df, mapping):\n",
        "    \"\"\"\n",
        "    Standardize column names using mapping dictionary\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "\n",
        "    # Apply mapping only for columns that exist in the DataFrame\n",
        "    rename_dict = {k: v for k, v in mapping.items() if k in df.columns}\n",
        "    if rename_dict:\n",
        "        df = df.rename(columns=rename_dict)\n",
        "\n",
        "    # Standardize all column names (lowercase and strip whitespace)\n",
        "    df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "def convert_numeric_columns(df, cols):\n",
        "    \"\"\"\n",
        "    Convert specified columns to numeric type\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def downcast_dtypes(df):\n",
        "    \"\"\"\n",
        "    Optimize memory usage by downcasting numeric types\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    result = df.copy()\n",
        "\n",
        "    # Downcast float columns\n",
        "    float_cols = result.select_dtypes(include=[\"float\"]).columns\n",
        "    for col in float_cols:\n",
        "        result[col] = pd.to_numeric(result[col], downcast=\"float\")\n",
        "\n",
        "    # Downcast integer columns\n",
        "    int_cols = result.select_dtypes(include=[\"int\"]).columns\n",
        "    for col in int_cols:\n",
        "        result[col] = pd.to_numeric(result[col], downcast=\"integer\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def handle_null_values(df, numeric_strategy='median', categorical_strategy='most_frequent'):\n",
        "    \"\"\"\n",
        "    Handle missing values with configurable strategies\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "\n",
        "    # Create a copy to avoid modifying the original DataFrame\n",
        "    result = df.copy()\n",
        "\n",
        "    # Handle numeric columns\n",
        "    num_cols = result.select_dtypes(include=['int', 'float']).columns\n",
        "    for col in num_cols:\n",
        "        if result[col].isnull().sum() > 0:\n",
        "            if numeric_strategy == 'median':\n",
        "                result[col] = result[col].fillna(result[col].median())\n",
        "            elif numeric_strategy == 'mean':\n",
        "                result[col] = result[col].fillna(result[col].mean())\n",
        "            elif numeric_strategy == 'zero':\n",
        "                result[col] = result[col].fillna(0)\n",
        "\n",
        "    # Handle categorical columns\n",
        "    cat_cols = result.select_dtypes(include=['object', 'category']).columns\n",
        "    for col in cat_cols:\n",
        "        if result[col].isnull().sum() > 0:\n",
        "            if categorical_strategy == 'most_frequent':\n",
        "                most_freq = result[col].mode()[0] if not result[col].mode().empty else \"Unknown\"\n",
        "                result[col] = result[col].fillna(most_freq)\n",
        "            elif categorical_strategy == 'new_category':\n",
        "                result[col] = result[col].fillna(\"Unknown\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# DATA LOADING WITH CACHING AND CHUNKING FOR LARGE DATASETS\n",
        "# ---------------------------------------------------------------------\n",
        "def make_api_request(url, timeout=None, max_retries=None, backoff_factor=None):\n",
        "    \"\"\"\n",
        "    Make an API request with retry logic\n",
        "    \"\"\"\n",
        "    # Use config defaults if not specified\n",
        "    timeout = timeout if timeout is not None else CONFIG.api_timeout\n",
        "    max_retries = max_retries if max_retries is not None else CONFIG.api_retries\n",
        "    backoff_factor = backoff_factor if backoff_factor is not None else CONFIG.api_backoff\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = requests.get(url, timeout=timeout)\n",
        "            resp.raise_for_status()\n",
        "            return resp.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            wait_time = backoff_factor ** attempt\n",
        "            logger.warning(f\"Request failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
        "            logger.info(f\"Waiting {wait_time:.1f} seconds before retry...\")\n",
        "\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                logger.error(f\"Max retries exceeded for URL: {url}\")\n",
        "                return None\n",
        "\n",
        "def fetch_sample_crime_data():\n",
        "    \"\"\"Generate sample crime data when API fails\"\"\"\n",
        "    logger.info(\"Generating synthetic sample crime data for testing\")\n",
        "\n",
        "    # Create a sample DataFrame with expected columns\n",
        "    sample_size = 10000\n",
        "    np.random.seed(CONFIG.random_state)\n",
        "\n",
        "    # Generate dates within the specified years\n",
        "    start_date = datetime(min(CONFIG.years), 1, 1)\n",
        "    end_date = datetime(max(CONFIG.years), 12, 31)\n",
        "    days_range = (end_date - start_date).days\n",
        "\n",
        "    # Generate random dates\n",
        "    random_days = np.random.randint(0, days_range, sample_size)\n",
        "    dates = [start_date + timedelta(days=days) for days in random_days]\n",
        "\n",
        "    # Generate community areas (1-77)\n",
        "    community_areas = np.random.randint(1, 78, sample_size).astype(str)\n",
        "\n",
        "    # Generate crime types\n",
        "    crime_types = np.random.choice(\n",
        "        ['THEFT', 'BATTERY', 'CRIMINAL DAMAGE', 'ASSAULT', 'BURGLARY',\n",
        "         'NARCOTICS', 'ROBBERY', 'MOTOR VEHICLE THEFT', 'HOMICIDE'],\n",
        "        sample_size,\n",
        "        p=[0.3, 0.2, 0.15, 0.1, 0.08, 0.07, 0.05, 0.04, 0.01]\n",
        "    )\n",
        "\n",
        "    # Generate arrests (boolean)\n",
        "    arrests = np.random.choice([True, False], sample_size, p=[0.2, 0.8])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'id': [f\"sample_{i}\" for i in range(sample_size)],\n",
        "        'case_number': [f\"HZ{100000+i}\" for i in range(sample_size)],\n",
        "        'date': dates,\n",
        "        'block': [f\"{np.random.randint(1, 9999)} BLOCK SAMPLE ST\" for _ in range(sample_size)],\n",
        "        'primary_type': crime_types,\n",
        "        'community_area': community_areas,\n",
        "        'arrest': arrests,\n",
        "        'year': [d.year for d in dates],\n",
        "        'latitude': np.random.uniform(41.6, 42.1, sample_size),\n",
        "        'longitude': np.random.uniform(-87.9, -87.5, sample_size)\n",
        "    })\n",
        "\n",
        "    # Add date-derived fields\n",
        "    df['month'] = df['date'].dt.month\n",
        "\n",
        "    logger.info(f\"Generated {len(df)} sample crime records\")\n",
        "    return df\n",
        "\n",
        "def fetch_chicago_crime_data(use_cache=True, use_full_dataset=True, years=None):\n",
        "    \"\"\"\n",
        "    Fetch Chicago crime data with caching and year filtering\n",
        "    \"\"\"\n",
        "    cache_file = os.path.join(CONFIG.cache_dir, \"chicago_crime_data.parquet\")\n",
        "\n",
        "    # Try to load from cache first\n",
        "    if use_cache and os.path.exists(cache_file):\n",
        "        try:\n",
        "            logger.info(f\"Loading Chicago crime data from cache\")\n",
        "            df = pd.read_parquet(cache_file)\n",
        "\n",
        "            # Check if the cache is empty or has too few records\n",
        "            if df is None or len(df) < 100:\n",
        "                logger.warning(f\"Cache appears to be empty or has too few records ({len(df)}), fetching from API\")\n",
        "                use_cache = False\n",
        "            else:\n",
        "                # Apply year filter if specified\n",
        "                if years is not None and 'year' in df.columns:\n",
        "                    df = df[df['year'].isin(years)]\n",
        "                    logger.info(f\"Filtered to years {years}: {len(df):,} records\")\n",
        "\n",
        "                logger.info(f\"Loaded {len(df):,} records from cache\")\n",
        "                return df\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load from cache: {e}\")\n",
        "            use_cache = False\n",
        "\n",
        "    # Fetch data from API in batches\n",
        "    all_data = []\n",
        "    offset = 0\n",
        "    max_records = CONFIG.max_crime_records if use_full_dataset else 100000\n",
        "\n",
        "    # Prepare year filter for API if specified\n",
        "    year_filter = \"\"\n",
        "    if years is not None:\n",
        "        # Correct syntax for SODA API filter\n",
        "        # Each condition is separated with AND/OR inside the $where parameter\n",
        "        year_conditions = []\n",
        "        for year in years:\n",
        "            year_conditions.append(f\"year={year}\")\n",
        "        year_filter = f\"&$where={' OR '.join(year_conditions)}\"\n",
        "\n",
        "    # Start with a simple test query to validate the API\n",
        "    test_url = f\"{CONFIG.chicago_crime_api}?$limit=1\"\n",
        "    if make_api_request(test_url) is None:\n",
        "        logger.warning(\"API test failed. Using sample data instead.\")\n",
        "        return fetch_sample_crime_data()\n",
        "\n",
        "    logger.info(f\"Fetching Chicago crime data from API with year filter: {year_filter}\")\n",
        "    pbar = tqdm(total=max_records, desc=\"Fetching Chicago Crime Data\", unit=\"rec\")\n",
        "\n",
        "    while True:\n",
        "        api_url = f\"{CONFIG.chicago_crime_api}?$limit={CONFIG.batch_size}&$offset={offset}{year_filter}\"\n",
        "\n",
        "        # Make API request with retry logic\n",
        "        batch = make_api_request(api_url)\n",
        "        if batch is None or not batch:\n",
        "            # If first attempt fails, try without year filter\n",
        "            if offset == 0 and years is not None:\n",
        "                logger.warning(\"API request with year filter failed. Trying without filter...\")\n",
        "                batch = make_api_request(f\"{CONFIG.chicago_crime_api}?$limit={CONFIG.batch_size}&$offset={offset}\")\n",
        "\n",
        "                if batch is None or not batch:\n",
        "                    # If still fails, use sample data\n",
        "                    logger.warning(\"API requests failed. Using sample data instead.\")\n",
        "                    return fetch_sample_crime_data()\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        all_data.extend(batch)\n",
        "        offset += CONFIG.batch_size\n",
        "        pbar.update(min(len(batch), max_records - len(all_data) + len(batch)))\n",
        "\n",
        "        if len(all_data) >= max_records:\n",
        "            all_data = all_data[:max_records]\n",
        "            break\n",
        "\n",
        "        # Periodically save to cache during long downloads\n",
        "        if len(all_data) % 1000000 == 0:\n",
        "            logger.info(f\"Downloaded {len(all_data):,} records so far...\")\n",
        "\n",
        "    pbar.close()\n",
        "    logger.info(f\"Fetched {len(all_data):,} Chicago crime records from API\")\n",
        "\n",
        "    # Check if we got any data\n",
        "    if not all_data:\n",
        "        logger.warning(\"No data retrieved from API. Using sample data instead.\")\n",
        "        return fetch_sample_crime_data()\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    # Process date column\n",
        "    if \"date\" in df.columns:\n",
        "        try:\n",
        "            df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "            # Extract year for filtering\n",
        "            df[\"year\"] = df[\"date\"].dt.year\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error parsing date: {e}\")\n",
        "            # Add year column if missing\n",
        "            if \"year\" not in df.columns:\n",
        "                df[\"year\"] = pd.NA\n",
        "\n",
        "    # Standardize columns\n",
        "    df = standardize_columns(df, CONFIG.column_mapping)\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = [\"community_area\", \"x_coordinate\", \"y_coordinate\", \"latitude\", \"longitude\"]\n",
        "    df = convert_numeric_columns(df, numeric_columns)\n",
        "\n",
        "    # Save to cache\n",
        "    if use_cache:\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
        "            df.to_parquet(cache_file, index=False, compression=\"snappy\")\n",
        "            logger.info(f\"Saved Chicago crime data to cache\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to save to cache: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def fetch_chicago_community_data(use_cache=True):\n",
        "    \"\"\"\n",
        "    Fetch Chicago community data with caching\n",
        "    \"\"\"\n",
        "    cache_file = os.path.join(CONFIG.cache_dir, \"chicago_community_data.parquet\")\n",
        "\n",
        "    # Try to load from cache first\n",
        "    if use_cache and os.path.exists(cache_file):\n",
        "        try:\n",
        "            logger.info(f\"Loading Chicago community data from cache\")\n",
        "            df = pd.read_parquet(cache_file)\n",
        "            logger.info(f\"Loaded {len(df):,} community records from cache\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load from cache: {e}\")\n",
        "\n",
        "    # Fetch data from API with retry logic\n",
        "    data = make_api_request(CONFIG.chicago_community_api)\n",
        "    if data is None:\n",
        "        logger.error(\"Failed to fetch community data from API\")\n",
        "        # Generate sample community data\n",
        "        df = pd.DataFrame({\n",
        "            'community_area': [str(i) for i in range(1, 78)],\n",
        "            'community_name': [f\"Community {i}\" for i in range(1, 78)],\n",
        "            'per_capita_income': np.random.uniform(15000, 60000, 77),\n",
        "            'hardship_index': np.random.uniform(10, 90, 77),\n",
        "            'pct_poverty': np.random.uniform(5, 40, 77),\n",
        "            'pct_unemployed': np.random.uniform(3, 25, 77),\n",
        "            'pct_no_hs_diploma': np.random.uniform(5, 35, 77),\n",
        "            'pct_housing_crowded': np.random.uniform(1, 15, 77)\n",
        "        })\n",
        "        logger.info(f\"Generated sample community data with {len(df)} records\")\n",
        "        return df\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Standardize columns\n",
        "    df = standardize_columns(df, CONFIG.column_mapping)\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = [\"per_capita_income\", \"hardship_index\", \"pct_poverty\", \"pct_unemployed\",\n",
        "                       \"pct_no_hs_diploma\", \"pct_housing_crowded\"]\n",
        "    df = convert_numeric_columns(df, numeric_columns)\n",
        "\n",
        "    # Save to cache\n",
        "    if use_cache:\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
        "            df.to_parquet(cache_file, index=False, compression=\"snappy\")\n",
        "            logger.info(f\"Saved Chicago community data to cache\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to save to cache: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "def fetch_chicago_census_data(use_cache=True):\n",
        "    \"\"\"\n",
        "    Fetch Chicago census data with caching\n",
        "    \"\"\"\n",
        "    cache_file = os.path.join(CONFIG.cache_dir, \"chicago_census_data.parquet\")\n",
        "\n",
        "    # Try to load from cache first\n",
        "    if use_cache and os.path.exists(cache_file):\n",
        "        try:\n",
        "            logger.info(f\"Loading Chicago census data from cache\")\n",
        "            df = pd.read_parquet(cache_file)\n",
        "            logger.info(f\"Loaded {len(df):,} census records from cache\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load census data from cache: {e}\")\n",
        "\n",
        "    # Fetch data from API with retry logic\n",
        "    data = make_api_request(CONFIG.chicago_census_api)\n",
        "    if data is None:\n",
        "        logger.error(\"Failed to fetch census data from API\")\n",
        "        # Generate sample census data\n",
        "        df = pd.DataFrame({\n",
        "            'community_area': [str(i) for i in range(1, 79)],\n",
        "            'community_name': [f\"Community {i}\" for i in range(1, 79)],\n",
        "            'population': np.random.uniform(15000, 80000, 78),\n",
        "            'per_capita_income': np.random.uniform(15000, 60000, 78),\n",
        "            'hardship_index': np.random.uniform(10, 90, 78),\n",
        "            'pct_poverty': np.random.uniform(5, 40, 78),\n",
        "            'pct_unemployed': np.random.uniform(3, 25, 78),\n",
        "            'pct_no_hs_diploma': np.random.uniform(5, 35, 78),\n",
        "            'pct_housing_crowded': np.random.uniform(1, 15, 78)\n",
        "        })\n",
        "        logger.info(f\"Generated sample census data with {len(df)} records\")\n",
        "        return df\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Standardize columns\n",
        "    df = standardize_columns(df, CONFIG.column_mapping)\n",
        "\n",
        "    # Convert numeric columns\n",
        "    numeric_columns = [\"per_capita_income\", \"hardship_index\", \"pct_poverty\", \"pct_unemployed\",\n",
        "                       \"pct_no_hs_diploma\", \"pct_housing_crowded\"]\n",
        "    df = convert_numeric_columns(df, numeric_columns)\n",
        "\n",
        "    # Save to cache\n",
        "    if use_cache:\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(cache_file), exist_ok=True)\n",
        "            df.to_parquet(cache_file, index=False, compression=\"snappy\")\n",
        "            logger.info(f\"Saved Chicago census data to cache\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to save census data to cache: {e}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# DATA CLEANING AND FEATURE ENGINEERING\n",
        "# ---------------------------------------------------------------------\n",
        "def clean_dataframe(df):\n",
        "    \"\"\"\n",
        "    Clean and optimize a DataFrame\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "\n",
        "    logger.info(\"Cleaning dataframe...\")\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    df_cleaned = df.copy()\n",
        "\n",
        "    # Standardize column names\n",
        "    df_cleaned = standardize_columns(df_cleaned, CONFIG.column_mapping)\n",
        "\n",
        "    # Convert dictionary values to strings\n",
        "    for col in df_cleaned.select_dtypes(include=[\"object\"]).columns:\n",
        "        df_cleaned[col] = df_cleaned[col].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
        "\n",
        "    # Drop duplicates\n",
        "    df_cleaned = df_cleaned.drop_duplicates()\n",
        "\n",
        "    # Handle missing values\n",
        "    df_cleaned = handle_null_values(df_cleaned)\n",
        "\n",
        "    # Optimize memory usage\n",
        "    df_cleaned = downcast_dtypes(df_cleaned)\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "def create_temporal_features(df):\n",
        "    \"\"\"\n",
        "    Create features from date column\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0 or 'date' not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    result = df.copy()\n",
        "\n",
        "    try:\n",
        "        # Ensure date column is datetime\n",
        "        result['date'] = pd.to_datetime(result['date'], errors='coerce')\n",
        "\n",
        "        # Extract basic time components\n",
        "        result['year'] = result['date'].dt.year\n",
        "        result['month'] = result['date'].dt.month\n",
        "        result['day'] = result['date'].dt.day\n",
        "        result['hour'] = result['date'].dt.hour\n",
        "        result['dayofweek'] = result['date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
        "        result['weekend'] = result['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
        "        result['quarter'] = result['date'].dt.quarter\n",
        "\n",
        "        # Extract week safely\n",
        "        try:\n",
        "            result['week'] = result['date'].dt.isocalendar().week\n",
        "        except AttributeError:\n",
        "            # Fallback for older pandas versions\n",
        "            result['week'] = result['date'].apply(\n",
        "                lambda x: x.isocalendar()[1] if not pd.isna(x) else pd.NA\n",
        "            )\n",
        "\n",
        "        # Create time of day categories\n",
        "        result['timeofday'] = pd.cut(\n",
        "            result['hour'],\n",
        "            bins=[0, 6, 12, 18, 24],\n",
        "            labels=['Night', 'Morning', 'Afternoon', 'Evening'],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "        # Create season indicators\n",
        "        result['season'] = pd.cut(\n",
        "            result['month'],\n",
        "            bins=[0, 3, 6, 9, 12],\n",
        "            labels=['Winter', 'Spring', 'Summer', 'Fall'],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "        # Create holiday flag (simplified, could be expanded)\n",
        "        holidays = [\n",
        "            # Christmas\n",
        "            (12, 25),\n",
        "            # New Year's\n",
        "            (1, 1),\n",
        "            # Independence Day\n",
        "            (7, 4),\n",
        "            # Thanksgiving (approximate)\n",
        "            (11, 26),\n",
        "            # Labor Day (approximate)\n",
        "            (9, 5),\n",
        "            # Memorial Day (approximate)\n",
        "            (5, 30)\n",
        "        ]\n",
        "\n",
        "        result['is_holiday'] = 0\n",
        "        for month, day in holidays:\n",
        "            holiday_mask = (result['month'] == month) & (result['day'] == day)\n",
        "            result.loc[holiday_mask, 'is_holiday'] = 1\n",
        "\n",
        "        # Create cyclical encoding for month and hour to preserve cyclical nature\n",
        "        result['month_sin'] = np.sin(2 * np.pi * result['month'] / 12)\n",
        "        result['month_cos'] = np.cos(2 * np.pi * result['month'] / 12)\n",
        "        result['hour_sin'] = np.sin(2 * np.pi * result['hour'] / 24)\n",
        "        result['hour_cos'] = np.cos(2 * np.pi * result['hour'] / 24)\n",
        "\n",
        "        logger.info(\"Created temporal features\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating temporal features: {e}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_crime_specific_features(df):\n",
        "    \"\"\"\n",
        "    Create crime-specific features\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0 or 'primary_type' not in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    result = df.copy()\n",
        "\n",
        "    try:\n",
        "        # Create flags for major crime categories\n",
        "        violent_crimes = ['HOMICIDE', 'ASSAULT', 'BATTERY', 'CRIMINAL SEXUAL ASSAULT', 'ROBBERY']\n",
        "        property_crimes = ['BURGLARY', 'THEFT', 'MOTOR VEHICLE THEFT', 'ARSON']\n",
        "        drug_crimes = ['NARCOTICS']\n",
        "        public_order_crimes = ['PUBLIC PEACE VIOLATION', 'CRIMINAL TRESPASS', 'CRIMINAL DAMAGE']\n",
        "        financial_crimes = ['DECEPTIVE PRACTICE', 'FRAUD', 'FORGERY', 'EMBEZZLEMENT']\n",
        "        weapon_crimes = ['WEAPONS VIOLATION']\n",
        "\n",
        "        # Convert primary_type to uppercase for consistent comparison\n",
        "        result['primary_type_upper'] = result['primary_type'].str.upper()\n",
        "\n",
        "        # Create binary indicators\n",
        "        result['violent_crime'] = result['primary_type_upper'].isin(violent_crimes).astype(int)\n",
        "        result['property_crime'] = result['primary_type_upper'].isin(property_crimes).astype(int)\n",
        "        result['drug_crime'] = result['primary_type_upper'].isin(drug_crimes).astype(int)\n",
        "        result['public_order_crime'] = result['primary_type_upper'].isin(public_order_crimes).astype(int)\n",
        "        result['financial_crime'] = result['primary_type_upper'].isin(financial_crimes).astype(int)\n",
        "        result['weapon_crime'] = result['primary_type_upper'].isin(weapon_crimes).astype(int)\n",
        "\n",
        "        # Create crime severity score (based on FBI UCR hierarchy)\n",
        "        severity_map = {\n",
        "            'HOMICIDE': 10,\n",
        "            'CRIMINAL SEXUAL ASSAULT': 9,\n",
        "            'ROBBERY': 8,\n",
        "            'AGGRAVATED ASSAULT': 7,\n",
        "            'AGGRAVATED BATTERY': 7,\n",
        "            'BURGLARY': 6,\n",
        "            'MOTOR VEHICLE THEFT': 5,\n",
        "            'THEFT': 4,\n",
        "            'ARSON': 6,\n",
        "            'NARCOTICS': 3,\n",
        "            'WEAPONS VIOLATION': 6,\n",
        "            'DECEPTIVE PRACTICE': 3,\n",
        "            'CRIMINAL DAMAGE': 3,\n",
        "            'BATTERY': 5,\n",
        "            'ASSAULT': 4,\n",
        "            'PUBLIC PEACE VIOLATION': 2,\n",
        "            'CRIMINAL TRESPASS': 2\n",
        "        }\n",
        "\n",
        "        # Create a default severity of 2 for unmapped crimes\n",
        "        result['crime_severity'] = result['primary_type_upper'].map(\n",
        "            lambda x: severity_map.get(x, 2)\n",
        "        )\n",
        "\n",
        "        # Create flags for arrests and domestic incidents if available\n",
        "        if 'arrest' in result.columns:\n",
        "            if result['arrest'].dtype == 'object':\n",
        "                result['arrest'] = result['arrest'].str.upper().isin(['TRUE', 'T', 'YES', 'Y', '1']).astype(int)\n",
        "\n",
        "        if 'domestic' in result.columns:\n",
        "            if result['domestic'].dtype == 'object':\n",
        "                result['domestic'] = result['domestic'].str.upper().isin(['TRUE', 'T', 'YES', 'Y', '1']).astype(int)\n",
        "\n",
        "        # Drop temporary column\n",
        "        result = result.drop(columns=['primary_type_upper'])\n",
        "\n",
        "        logger.info(\"Created crime-specific features\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating crime-specific features: {e}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_spatial_features(df):\n",
        "    \"\"\"\n",
        "    Create spatial features from location data\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        return df\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    result = df.copy()\n",
        "\n",
        "    try:\n",
        "        # Check if we have location data\n",
        "        has_lat_long = all(col in result.columns for col in ['latitude', 'longitude'])\n",
        "\n",
        "        if not has_lat_long:\n",
        "            logger.warning(\"Missing latitude/longitude columns for spatial features\")\n",
        "            return result\n",
        "\n",
        "        # Filter out invalid coordinates (must be numeric)\n",
        "        result['latitude'] = pd.to_numeric(result['latitude'], errors='coerce')\n",
        "        result['longitude'] = pd.to_numeric(result['longitude'], errors='coerce')\n",
        "\n",
        "        # Check if we have any valid coordinates\n",
        "        valid_coords = (\n",
        "            ~result['latitude'].isna() &\n",
        "            ~result['longitude'].isna() &\n",
        "            (result['latitude'] >= 41.6) &\n",
        "            (result['latitude'] <= 42.1) &\n",
        "            (result['longitude'] >= -88.0) &\n",
        "            (result['longitude'] <= -87.5)\n",
        "        )\n",
        "\n",
        "        valid_count = valid_coords.sum()\n",
        "\n",
        "        if valid_count == 0:\n",
        "            logger.warning(\"No valid coordinates found for spatial features\")\n",
        "            return result\n",
        "\n",
        "        # Downtown Chicago coordinates (The Loop)\n",
        "        downtown_lat, downtown_lon = 41.8781, -87.6298\n",
        "\n",
        "        # Define haversine distance function\n",
        "        def haversine(lat1, lon1, lat2, lon2):\n",
        "            # Convert decimal degrees to radians\n",
        "            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "            # Haversine formula\n",
        "            dlon = lon2 - lon1\n",
        "            dlat = lat2 - lat1\n",
        "            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
        "            c = 2 * np.arcsin(np.sqrt(a))\n",
        "            r = 6371  # Radius of earth in kilometers\n",
        "            return c * r\n",
        "\n",
        "        # Initialize dist_to_downtown column with NaN\n",
        "        result['dist_to_downtown'] = np.nan\n",
        "\n",
        "        # Calculate distances only for valid coordinates\n",
        "        valid_indices = result.index[valid_coords]\n",
        "        result.loc[valid_indices, 'dist_to_downtown'] = haversine(\n",
        "            result.loc[valid_indices, 'latitude'],\n",
        "            result.loc[valid_indices, 'longitude'],\n",
        "            downtown_lat, downtown_lon\n",
        "        )\n",
        "\n",
        "        # Fill invalid coordinates with median distance\n",
        "        if len(valid_indices) > 0:\n",
        "            median_dist = result.loc[valid_indices, 'dist_to_downtown'].median()\n",
        "            result['dist_to_downtown'] = result['dist_to_downtown'].fillna(median_dist)\n",
        "\n",
        "            # Create distance bins\n",
        "            result['distance_category'] = pd.cut(\n",
        "                result['dist_to_downtown'],\n",
        "                bins=[0, 5, 10, 15, 100],  # in km\n",
        "                labels=['Very Close', 'Close', 'Medium', 'Far'],\n",
        "                include_lowest=True\n",
        "            )\n",
        "\n",
        "        logger.info(\"Created spatial features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating spatial features: {e}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def create_socioeconomic_features(community_df):\n",
        "    \"\"\"\n",
        "    Create advanced socioeconomic features from community data\n",
        "    \"\"\"\n",
        "    if community_df is None or len(community_df) == 0:\n",
        "        return community_df\n",
        "\n",
        "    # Create a copy to avoid modifying the original\n",
        "    result = community_df.copy()\n",
        "\n",
        "    try:\n",
        "        # Convert to numeric if not already\n",
        "        numeric_cols = [\n",
        "            'per_capita_income', 'hardship_index', 'pct_poverty',\n",
        "            'pct_unemployed', 'pct_no_hs_diploma', 'pct_housing_crowded'\n",
        "        ]\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in result.columns:\n",
        "                result[col] = pd.to_numeric(result[col], errors='coerce')\n",
        "\n",
        "        # Create normalized versions (0-1 scale)\n",
        "        for col in numeric_cols:\n",
        "            if col in result.columns:\n",
        "                norm_col = f'{col}_norm'\n",
        "                # Get min and max, handling empty data\n",
        "                col_min = result[col].min()\n",
        "                col_max = result[col].max()\n",
        "\n",
        "                # Avoid division by zero\n",
        "                if pd.isna(col_min) or pd.isna(col_max) or col_min == col_max:\n",
        "                    result[norm_col] = 0.5\n",
        "                else:\n",
        "                    result[norm_col] = (result[col] - col_min) / (col_max - col_min)\n",
        "\n",
        "        # Create composite socioeconomic score (higher = better socioeconomic status)\n",
        "        if all(col in result.columns for col in ['per_capita_income', 'hardship_index']):\n",
        "            # Normalize income (higher is better)\n",
        "            if 'per_capita_income_norm' not in result.columns:\n",
        "                income_min = result['per_capita_income'].min()\n",
        "                income_max = result['per_capita_income'].max()\n",
        "\n",
        "                # Avoid division by zero\n",
        "                if pd.isna(income_min) or pd.isna(income_max) or income_min == income_max:\n",
        "                    result['per_capita_income_norm'] = 0.5\n",
        "                else:\n",
        "                    result['per_capita_income_norm'] = (result['per_capita_income'] - income_min) / (income_max - income_min)\n",
        "\n",
        "            # Normalize hardship (lower is better)\n",
        "            if 'hardship_index_norm' not in result.columns:\n",
        "                # Ensure hardship_index is not NaN and is within the expected range\n",
        "                valid_hardship = result['hardship_index'].fillna(50)  # Fill NaNs with midpoint\n",
        "\n",
        "                # Ensure hardship_index is in 0-100 range\n",
        "                valid_hardship = valid_hardship.clip(0, 100)\n",
        "\n",
        "                result['hardship_index_norm'] = valid_hardship / 100\n",
        "\n",
        "            # Create score (higher = better socioeconomic conditions)\n",
        "            result['socioeconomic_score'] = (\n",
        "                result['per_capita_income_norm'] * (1 - result['hardship_index_norm'])\n",
        "            )\n",
        "\n",
        "            # Create quartiles for grouping (safely handle duplicate values)\n",
        "            try:\n",
        "                result['socioeconomic_quartile'] = pd.qcut(\n",
        "                    result['socioeconomic_score'],\n",
        "                    q=4,\n",
        "                    labels=['Low', 'Medium-Low', 'Medium-High', 'High'],\n",
        "                    duplicates='drop'\n",
        "                )\n",
        "            except ValueError as e:\n",
        "                # Fallback if we can't create quartiles properly\n",
        "                logger.warning(f\"Could not create quartiles, using manual binning instead: {e}\")\n",
        "\n",
        "                # Use manual binning instead\n",
        "                result['socioeconomic_quartile'] = pd.cut(\n",
        "                    result['socioeconomic_score'],\n",
        "                    bins=[0, 0.25, 0.5, 0.75, 1],\n",
        "                    labels=['Low', 'Medium-Low', 'Medium-High', 'High'],\n",
        "                    include_lowest=True\n",
        "                )\n",
        "\n",
        "        logger.info(\"Created socioeconomic features\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating socioeconomic features: {e}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# REPORTING BIAS DETECTION AND CORRECTION\n",
        "# ---------------------------------------------------------------------\n",
        "def enhanced_reporting_bias_analysis(crime_df, community_df, census_df=None):\n",
        "    \"\"\"\n",
        "    Enhanced multi-factor reporting bias model considering socioeconomic factors\n",
        "    and demographic variables more comprehensively\n",
        "    \"\"\"\n",
        "    logger.info(\"Performing enhanced reporting bias analysis\")\n",
        "\n",
        "    # Create a copy to work with\n",
        "    community_df = community_df.copy()\n",
        "    crime_df = crime_df.copy()\n",
        "\n",
        "    # Ensure we have needed columns\n",
        "    if 'community_area' not in crime_df.columns or 'community_area' not in community_df.columns:\n",
        "        logger.warning(\"Missing community_area column for reporting bias analysis\")\n",
        "        community_df['reporting_factor'] = 1.0\n",
        "        return community_df\n",
        "\n",
        "    # Fix: Ensure community_area columns are the same type (string) in both DataFrames\n",
        "    crime_df['community_area'] = crime_df['community_area'].astype(str)\n",
        "    community_df['community_area'] = community_df['community_area'].astype(str)\n",
        "\n",
        "    # Count crimes by community area\n",
        "    crime_counts = crime_df.groupby('community_area').size().reset_index(name='crime_count')\n",
        "\n",
        "    # Merge with community data\n",
        "    analysis_df = pd.merge(\n",
        "        community_df,\n",
        "        crime_counts,\n",
        "        on='community_area',\n",
        "        how='left'\n",
        "    )\n",
        "    analysis_df['crime_count'] = analysis_df['crime_count'].fillna(0)\n",
        "\n",
        "    # Initialize with neutral factor\n",
        "    analysis_df['reporting_factor'] = 1.0\n",
        "\n",
        "    # Create multi-factor bias model using available socioeconomic indicators\n",
        "    socioeconomic_factors = [\n",
        "        ('hardship_index', 0.3),      # Higher hardship = more under-reporting (weight: 0.3)\n",
        "        ('per_capita_income', -0.25),  # Lower income = more under-reporting (weight: -0.25)\n",
        "        ('pct_poverty', 0.2),         # Higher poverty = more under-reporting (weight: 0.2)\n",
        "        ('pct_unemployed', 0.15),     # Higher unemployment = more under-reporting (weight: 0.15)\n",
        "        ('pct_no_hs_diploma', 0.1)    # Lower education = more under-reporting (weight: 0.1)\n",
        "    ]\n",
        "\n",
        "    # Apply multi-factor model\n",
        "    for factor, weight in socioeconomic_factors:\n",
        "        if factor in analysis_df.columns:\n",
        "            # Normalize factor to 0-1 range\n",
        "            factor_values = analysis_df[factor].fillna(analysis_df[factor].median())\n",
        "            factor_min = factor_values.min()\n",
        "            factor_max = factor_values.max()\n",
        "\n",
        "            if factor_max > factor_min:  # Avoid division by zero\n",
        "                factor_norm = (factor_values - factor_min) / (factor_max - factor_min)\n",
        "\n",
        "                # Apply weighted adjustment (centered around 0.5 to create balanced adjustments)\n",
        "                analysis_df['reporting_factor'] += weight * (0.5 - factor_norm)\n",
        "\n",
        "    # Add confidence bounds (simple approach)\n",
        "    analysis_df['reporting_factor_lower'] = analysis_df['reporting_factor'] * 0.9\n",
        "    analysis_df['reporting_factor_upper'] = analysis_df['reporting_factor'] * 1.1\n",
        "\n",
        "    # Cap extreme values\n",
        "    analysis_df['reporting_factor'] = analysis_df['reporting_factor'].clip(0.7, 1.3)\n",
        "    analysis_df['reporting_factor_lower'] = analysis_df['reporting_factor_lower'].clip(0.6, 1.2)\n",
        "    analysis_df['reporting_factor_upper'] = analysis_df['reporting_factor_upper'].clip(0.8, 1.4)\n",
        "\n",
        "    # Create community_df with reporting factors\n",
        "    community_with_factors = pd.merge(\n",
        "        community_df,\n",
        "        analysis_df[['community_area', 'reporting_factor', 'reporting_factor_lower', 'reporting_factor_upper']],\n",
        "        on='community_area',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Fill missing reporting factors with 1.0 (neutral)\n",
        "    community_with_factors['reporting_factor'] = community_with_factors['reporting_factor'].fillna(1.0)\n",
        "    community_with_factors['reporting_factor_lower'] = community_with_factors['reporting_factor_lower'].fillna(0.9)\n",
        "    community_with_factors['reporting_factor_upper'] = community_with_factors['reporting_factor_upper'].fillna(1.1)\n",
        "\n",
        "    logger.info(f\"Enhanced reporting factor statistics: Min={community_with_factors['reporting_factor'].min():.2f}, \"\n",
        "               f\"Max={community_with_factors['reporting_factor'].max():.2f}, \"\n",
        "               f\"Mean={community_with_factors['reporting_factor'].mean():.2f}\")\n",
        "\n",
        "    return community_with_factors\n",
        "\n",
        "def apply_reporting_bias_correction(df, target_col='crime_count'):\n",
        "    \"\"\"Apply reporting bias correction to a target variable\"\"\"\n",
        "    logger.info(f\"Applying reporting bias correction to {target_col}\")\n",
        "\n",
        "    try:\n",
        "        if df is None or len(df) == 0:\n",
        "            return df\n",
        "\n",
        "        # Create a copy to avoid modifying original\n",
        "        result = df.copy()\n",
        "\n",
        "        if 'reporting_factor' not in result.columns:\n",
        "            logger.warning(\"No reporting factors available for bias correction\")\n",
        "            # Add a placeholder adjusted column with same values as original\n",
        "            adjusted_col = f'{target_col}_adjusted'\n",
        "            if target_col in result.columns:\n",
        "                result[adjusted_col] = result[target_col]\n",
        "            return result\n",
        "\n",
        "        if target_col not in result.columns:\n",
        "            logger.warning(f\"Target column {target_col} not found for bias correction\")\n",
        "            return result\n",
        "\n",
        "        # Create an adjusted version of the target\n",
        "        adjusted_col = f'{target_col}_adjusted'\n",
        "\n",
        "        # Apply correction: divide by reporting factor\n",
        "        # Higher reporting factor -> over-reported -> reduce crime count\n",
        "        # Lower reporting factor -> under-reported -> increase crime count\n",
        "        # Add a small constant to prevent division by zero\n",
        "        result[adjusted_col] = result[target_col] / result['reporting_factor'].clip(lower=0.001)\n",
        "\n",
        "        # Fill NaN values with original values to prevent issues\n",
        "        result[adjusted_col] = result[adjusted_col].fillna(result[target_col])\n",
        "\n",
        "        # Verify results\n",
        "        logger.info(f\"Original {target_col} mean: {result[target_col].mean():.2f}\")\n",
        "        logger.info(f\"Adjusted {adjusted_col} mean: {result[adjusted_col].mean():.2f}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error applying reporting bias correction: {e}\")\n",
        "        # Return original with placeholder adjusted column\n",
        "        df = df.copy()\n",
        "        adjusted_col = f'{target_col}_adjusted'\n",
        "        if target_col in df.columns:\n",
        "            df[adjusted_col] = df[target_col]\n",
        "        return df\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ADVANCED FEATURE SELECTION\n",
        "# ---------------------------------------------------------------------\n",
        "def select_features(X, y=None, method='importance', max_features=None):\n",
        "    \"\"\"\n",
        "    Select most informative features using various methods\n",
        "    \"\"\"\n",
        "    # Fix: Set a default value for max_features if not provided\n",
        "    if max_features is None:\n",
        "        max_features = min(30, len(X.columns))\n",
        "\n",
        "    logger.info(f\"Selecting features using {method} method (max={max_features})\")\n",
        "\n",
        "    try:\n",
        "        # Check input data\n",
        "        if X is None or len(X) == 0:\n",
        "            logger.error(\"Invalid input data for feature selection\")\n",
        "            return X.columns.tolist()[:max_features]\n",
        "\n",
        "        # If no target variable is provided, skip feature importance and just return all features\n",
        "        if y is None:\n",
        "            logger.warning(\"No target variable provided for feature selection. Using all features.\")\n",
        "            return X.columns.tolist()[:max_features]\n",
        "\n",
        "        # Check for shape mismatch\n",
        "        if len(X) != len(y):\n",
        "            logger.error(f\"Shape mismatch in feature selection: X has {len(X)} samples, y has {len(y)} samples\")\n",
        "            return X.columns.tolist()[:max_features]\n",
        "\n",
        "        # Ensure we're working with numeric data\n",
        "        numeric_cols = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "        if len(numeric_cols) == 0:\n",
        "            logger.warning(\"No numeric columns found for feature selection\")\n",
        "            return X.columns.tolist()[:max_features]\n",
        "\n",
        "        X_numeric = X[numeric_cols].copy()\n",
        "\n",
        "        # Fill any missing values\n",
        "        for col in X_numeric.columns:\n",
        "            X_numeric[col] = X_numeric[col].fillna(X_numeric[col].median())\n",
        "\n",
        "        # Make sure y doesn't have NaN values\n",
        "        y_clean = y.fillna(y.median())\n",
        "\n",
        "        # Method 1: Feature importance from Random Forest\n",
        "        if method == 'importance':\n",
        "            try:\n",
        "                # Use Random Forest for feature importance\n",
        "                model = RandomForestRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=8,\n",
        "                    random_state=CONFIG.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "                model.fit(X_numeric, y_clean)\n",
        "\n",
        "                # Get importances\n",
        "                importances = model.feature_importances_\n",
        "\n",
        "                # Create feature importance DataFrame\n",
        "                feature_importance = pd.DataFrame({\n",
        "                    'feature': X_numeric.columns,\n",
        "                    'importance': importances\n",
        "                })\n",
        "\n",
        "                # Sort by importance\n",
        "                feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "                # Select top features\n",
        "                selected_features = feature_importance['feature'].tolist()[:max_features]\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in importance-based feature selection: {e}\")\n",
        "                # Fall back to using all numeric features\n",
        "                selected_features = X_numeric.columns.tolist()[:max_features]\n",
        "\n",
        "        # Ensure we have at least some features\n",
        "        if not selected_features:\n",
        "            logger.warning(\"No features selected, using all numeric columns\")\n",
        "            selected_features = X_numeric.columns.tolist()[:max_features]\n",
        "\n",
        "        logger.info(f\"Selected {len(selected_features)} features\")\n",
        "        return selected_features\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in feature selection: {e}\")\n",
        "        # Return top columns as fallback\n",
        "        return X.select_dtypes(include=np.number).columns.tolist()[:max_features]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# DATA PROCESSOR CLASS\n",
        "# ---------------------------------------------------------------------\n",
        "class DataProcessor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the DataProcessor\"\"\"\n",
        "        self.crime_data = None\n",
        "        self.community_data = None\n",
        "        self.census_data = None\n",
        "        self.merged_data = None\n",
        "        self.feature_pipeline = None\n",
        "        self.target_col = CONFIG.target\n",
        "\n",
        "    def load_datasets(self, use_cache=True, use_full_dataset=True, years=None):\n",
        "        \"\"\"\n",
        "        Load all datasets\n",
        "        \"\"\"\n",
        "        logger.info(f\"Loading datasets (full_dataset={use_full_dataset}, years={years})...\")\n",
        "\n",
        "        # Load crime data\n",
        "        years_to_use = years if years is not None else CONFIG.years\n",
        "        self.crime_data = fetch_chicago_crime_data(\n",
        "            use_cache=use_cache,\n",
        "            use_full_dataset=use_full_dataset,\n",
        "            years=years_to_use\n",
        "        )\n",
        "\n",
        "        # If crime data is empty, try fetching from API directly\n",
        "        if self.crime_data is None or len(self.crime_data) == 0:\n",
        "            logger.warning(\"Crime data cache is empty, fetching from API directly\")\n",
        "            use_cache = False\n",
        "            self.crime_data = fetch_chicago_crime_data(\n",
        "                use_cache=False,\n",
        "                use_full_dataset=use_full_dataset,\n",
        "                years=years_to_use\n",
        "            )\n",
        "\n",
        "        # Load community data (always load complete set)\n",
        "        self.community_data = fetch_chicago_community_data(use_cache=use_cache)\n",
        "\n",
        "        # Load census data (always load complete set)\n",
        "        self.census_data = fetch_chicago_census_data(use_cache=use_cache)\n",
        "\n",
        "        # Log dataset shapes\n",
        "        if self.crime_data is not None:\n",
        "            logger.info(f\"Crime data shape: {self.crime_data.shape}\")\n",
        "        if self.community_data is not None:\n",
        "            logger.info(f\"Community data shape: {self.community_data.shape}\")\n",
        "        if self.census_data is not None:\n",
        "            logger.info(f\"Census data shape: {self.census_data.shape}\")\n",
        "\n",
        "        return self.crime_data, self.community_data, self.census_data\n",
        "\n",
        "    def clean_data(self):\n",
        "        \"\"\"\n",
        "        Clean all loaded datasets\n",
        "        \"\"\"\n",
        "        logger.info(\"Cleaning all datasets...\")\n",
        "\n",
        "        # Clean crime data\n",
        "        if self.crime_data is not None and len(self.crime_data) > 0:\n",
        "            # For large datasets, process in chunks\n",
        "            if len(self.crime_data) > 1000000:\n",
        "                if CONFIG.use_dask and DASK_AVAILABLE:\n",
        "                    self.crime_data = process_with_dask(self.crime_data, clean_dataframe)\n",
        "                else:\n",
        "                    self.crime_data = process_data_in_chunks_parallel(\n",
        "                        self.crime_data,\n",
        "                        chunk_size=CONFIG.chunk_size,\n",
        "                        processor_func=clean_dataframe\n",
        "                    )\n",
        "            else:\n",
        "                self.crime_data = clean_dataframe(self.crime_data)\n",
        "\n",
        "        # Clean community data\n",
        "        if self.community_data is not None and len(self.community_data) > 0:\n",
        "            self.community_data = clean_dataframe(self.community_data)\n",
        "\n",
        "        # Clean census data\n",
        "        if self.census_data is not None and len(self.census_data) > 0:\n",
        "            self.census_data = clean_dataframe(self.census_data)\n",
        "\n",
        "        logger.info(\"Data cleaning completed\")\n",
        "        return self\n",
        "\n",
        "    def engineer_features(self):\n",
        "        \"\"\"\n",
        "        Apply feature engineering to datasets\n",
        "        \"\"\"\n",
        "        logger.info(\"Engineering features...\")\n",
        "\n",
        "        # Feature engineering for crime data\n",
        "        if self.crime_data is not None and len(self.crime_data) > 0:\n",
        "            # For large datasets, process in chunks\n",
        "            if len(self.crime_data) > 1000000:\n",
        "                if CONFIG.use_dask and DASK_AVAILABLE:\n",
        "                    # Create temporal features\n",
        "                    self.crime_data = process_with_dask(self.crime_data, create_temporal_features)\n",
        "                    # Create crime-specific features\n",
        "                    self.crime_data = process_with_dask(self.crime_data, create_crime_specific_features)\n",
        "                    # Create spatial features - use chunk processing to avoid Dask errors\n",
        "                    self.crime_data = process_data_in_chunks_parallel(\n",
        "                        self.crime_data,\n",
        "                        chunk_size=CONFIG.chunk_size,\n",
        "                        processor_func=create_spatial_features\n",
        "                    )\n",
        "                else:\n",
        "                    # Create temporal features\n",
        "                    self.crime_data = process_data_in_chunks_parallel(\n",
        "                        self.crime_data,\n",
        "                        chunk_size=CONFIG.chunk_size,\n",
        "                        processor_func=create_temporal_features\n",
        "                    )\n",
        "\n",
        "                    # Create crime-specific features\n",
        "                    self.crime_data = process_data_in_chunks_parallel(\n",
        "                        self.crime_data,\n",
        "                        chunk_size=CONFIG.chunk_size,\n",
        "                        processor_func=create_crime_specific_features\n",
        "                    )\n",
        "                    # Create spatial features\n",
        "                    self.crime_data = process_data_in_chunks_parallel(\n",
        "                        self.crime_data,\n",
        "                        chunk_size=CONFIG.chunk_size,\n",
        "                        processor_func=create_spatial_features\n",
        "                    )\n",
        "            else:\n",
        "                # Create temporal features\n",
        "                self.crime_data = create_temporal_features(self.crime_data)\n",
        "                # Create crime-specific features\n",
        "                self.crime_data = create_crime_specific_features(self.crime_data)\n",
        "                # Create spatial features\n",
        "                self.crime_data = create_spatial_features(self.crime_data)\n",
        "\n",
        "        # Feature engineering for community data\n",
        "        if self.community_data is not None and len(self.community_data) > 0:\n",
        "            # First, check if community_area column exists or map from another column\n",
        "            if 'community_area' not in self.community_data.columns:\n",
        "                # Check for alternative column names that might contain community area\n",
        "                for col_name in ['area_num', 'area_numbe', 'area_number', 'ca']:\n",
        "                    if col_name in self.community_data.columns:\n",
        "                        self.community_data['community_area'] = self.community_data[col_name].astype(str).str.strip()\n",
        "                        logger.info(f\"Created 'community_area' column from '{col_name}'\")\n",
        "                        break\n",
        "                # If still not found, create from index as last resort\n",
        "                if 'community_area' not in self.community_data.columns:\n",
        "                    self.community_data['community_area'] = self.community_data.index.astype(str).str.strip()\n",
        "                    logger.info(\"Created 'community_area' column from index\")\n",
        "            else:\n",
        "                # Ensure community area is string\n",
        "                self.community_data['community_area'] = self.community_data['community_area'].astype(str).str.strip()\n",
        "\n",
        "            # Merge with census data if available\n",
        "            if self.census_data is not None and not self.census_data.empty:\n",
        "                # Check if community_area exists in census data, or map from another column\n",
        "                if 'community_area' not in self.census_data.columns:\n",
        "                    for col_name in ['ca', 'area_num', 'area_numbe', 'area_number']:\n",
        "                        if col_name in self.census_data.columns:\n",
        "                            self.census_data['community_area'] = self.census_data[col_name].astype(str).str.strip()\n",
        "                            logger.info(f\"Created 'community_area' column in census data from '{col_name}'\")\n",
        "                            break\n",
        "\n",
        "                if 'community_area' in self.census_data.columns:\n",
        "                    # Ensure community area is string in census data\n",
        "                    self.census_data['community_area'] = self.census_data['community_area'].astype(str).str.strip()\n",
        "\n",
        "                    # Merge community and census data\n",
        "                    self.community_data = pd.merge(\n",
        "                        self.community_data,\n",
        "                        self.census_data,\n",
        "                        on='community_area',\n",
        "                        how='left'\n",
        "                    )\n",
        "\n",
        "                    logger.info(f\"Merged community and census data: {self.community_data.shape}\")\n",
        "                else:\n",
        "                    logger.warning(\"Could not merge census data: no 'community_area' column found\")\n",
        "\n",
        "            # Create advanced socioeconomic features\n",
        "            self.community_data = create_socioeconomic_features(self.community_data)\n",
        "\n",
        "            # Apply enhanced reporting bias analysis if requested and crime data exists\n",
        "            if CONFIG.reporting_bias_correction and self.crime_data is not None and len(self.crime_data) > 0:\n",
        "                self.community_data = enhanced_reporting_bias_analysis(\n",
        "                    self.crime_data,\n",
        "                    self.community_data,\n",
        "                    self.census_data if self.census_data is not None and len(self.census_data) > 0 else None\n",
        "                )\n",
        "\n",
        "        logger.info(\"Feature engineering completed\")\n",
        "        return self\n",
        "\n",
        "    def merge_datasets(self):\n",
        "        \"\"\"\n",
        "        Merge datasets for modeling\n",
        "        \"\"\"\n",
        "        logger.info(\"Merging datasets...\")\n",
        "\n",
        "        # Check for empty crime data\n",
        "        if self.crime_data is None or len(self.crime_data) == 0:\n",
        "            logger.error(\"No crime data available for merging\")\n",
        "\n",
        "            # If we have community data, return it instead of None\n",
        "            if self.community_data is not None and len(self.community_data) > 0:\n",
        "                logger.info(\"Returning community data since crime data is empty\")\n",
        "                self.merged_data = self.community_data.copy()\n",
        "                return self.merged_data\n",
        "            return None\n",
        "\n",
        "        if self.community_data is None or len(self.community_data) == 0:\n",
        "            logger.error(\"No community data available for merging\")\n",
        "            return None\n",
        "\n",
        "        # Create a copy of the DataFrames to avoid modifying originals\n",
        "        crime_data = self.crime_data.copy()\n",
        "        community_data = self.community_data.copy()\n",
        "\n",
        "        # Ensure community_area is available and is string type in both datasets\n",
        "        if \"community_area\" in crime_data.columns:\n",
        "            crime_data[\"community_area\"] = crime_data[\"community_area\"].astype(str).str.strip()\n",
        "        elif \"block\" in crime_data.columns:\n",
        "            # Try to extract community area from block\n",
        "            crime_data[\"community_area\"] = crime_data[\"block\"].str.extract(r'(\\d+)', expand=False)\n",
        "        else:\n",
        "            logger.error(\"Cannot extract community area from crime data.\")\n",
        "            return None\n",
        "\n",
        "        # Ensure community_area is string in community data\n",
        "        community_data[\"community_area\"] = community_data[\"community_area\"].astype(str).str.strip()\n",
        "\n",
        "        # Create aggregation by community area and time period\n",
        "        logger.info(\"Aggregating crime data...\")\n",
        "\n",
        "        # Define time period for aggregation (month-year)\n",
        "        if 'date' in crime_data.columns:\n",
        "            crime_data['month_year'] = crime_data['date'].dt.strftime('%Y-%m')\n",
        "            time_var = 'month_year'\n",
        "        elif all(col in crime_data.columns for col in ['year', 'month']):\n",
        "            # Create month-year string\n",
        "            crime_data['month_year'] = crime_data['year'].astype(str) + '-' + crime_data['month'].astype(str).str.zfill(2)\n",
        "            time_var = 'month_year'\n",
        "        else:\n",
        "            # Fall back to year only\n",
        "            time_var = 'year' if 'year' in crime_data.columns else None\n",
        "\n",
        "        # Define aggregation functions\n",
        "        if time_var:\n",
        "            logger.info(f\"Aggregating by community area and {time_var}\")\n",
        "\n",
        "            # Define aggregation functions\n",
        "            agg_functions = {\n",
        "                'id': 'count'  # Total crime count\n",
        "            }\n",
        "\n",
        "            # Add crime type aggregations if available\n",
        "            for col in ['violent_crime', 'property_crime', 'drug_crime', 'public_order_crime',\n",
        "                       'financial_crime', 'weapon_crime', 'crime_severity', 'arrest', 'domestic']:\n",
        "                if col in crime_data.columns:\n",
        "                    if col == 'crime_severity':\n",
        "                        agg_functions[col] = ['mean', 'max']\n",
        "                    else:\n",
        "                        agg_functions[col] = 'sum'\n",
        "\n",
        "            # Add temporal aggregations\n",
        "            for col in ['weekend', 'hour', 'is_holiday']:\n",
        "                if col in crime_data.columns:\n",
        "                    agg_functions[col] = 'mean'\n",
        "\n",
        "            # Group by community area and time period\n",
        "            # Use more efficient aggregation with Dask if data is large\n",
        "            try:\n",
        "                if len(crime_data) > 1000000 and CONFIG.use_dask and DASK_AVAILABLE:\n",
        "                    logger.info(\"Using Dask for large dataset aggregation\")\n",
        "                    ddf = dd.from_pandas(crime_data, npartitions=max(1, len(crime_data) // CONFIG.chunk_size))\n",
        "                    agg_crime = ddf.groupby(['community_area', time_var]).agg(agg_functions).compute()\n",
        "                else:\n",
        "                    agg_crime = crime_data.groupby(['community_area', time_var]).agg(agg_functions)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in aggregation: {e}\")\n",
        "                # Fallback to pandas\n",
        "                logger.info(\"Falling back to pandas aggregation\")\n",
        "                agg_crime = crime_data.groupby(['community_area', time_var]).agg(agg_functions)\n",
        "\n",
        "            # Flatten multi-level column names\n",
        "            if isinstance(agg_crime.columns, pd.MultiIndex):\n",
        "                agg_crime.columns = ['_'.join(col).strip() for col in agg_crime.columns.values]\n",
        "\n",
        "            # Rename count column\n",
        "            if 'id_count' in agg_crime.columns:\n",
        "                agg_crime = agg_crime.rename(columns={'id_count': 'crime_count'})\n",
        "\n",
        "            # Reset index to make community_area and time_var regular columns\n",
        "            agg_crime = agg_crime.reset_index()\n",
        "\n",
        "        else:\n",
        "            # If no time variable, aggregate by community area only\n",
        "            logger.info(\"No time variable available, aggregating by community area only\")\n",
        "\n",
        "            # Define aggregation functions\n",
        "            agg_functions = {\n",
        "                'id': 'count'  # Total crime count\n",
        "            }\n",
        "\n",
        "            # Add crime type aggregations if available\n",
        "            for col in ['violent_crime', 'property_crime', 'drug_crime', 'public_order_crime',\n",
        "                       'financial_crime', 'weapon_crime', 'crime_severity', 'arrest', 'domestic']:\n",
        "                if col in crime_data.columns:\n",
        "                    if col == 'crime_severity':\n",
        "                        agg_functions[col] = ['mean', 'max']\n",
        "                    else:\n",
        "                        agg_functions[col] = 'sum'\n",
        "\n",
        "            # Group by community area\n",
        "            agg_crime = crime_data.groupby('community_area').agg(agg_functions)\n",
        "\n",
        "            # Flatten multi-level column names\n",
        "            if isinstance(agg_crime.columns, pd.MultiIndex):\n",
        "                agg_crime.columns = ['_'.join(col).strip() for col in agg_crime.columns.values]\n",
        "\n",
        "            # Rename count column\n",
        "            if 'id_count' in agg_crime.columns:\n",
        "                agg_crime = agg_crime.rename(columns={'id_count': 'crime_count'})\n",
        "\n",
        "            # Reset index to make community_area a regular column\n",
        "            agg_crime = agg_crime.reset_index()\n",
        "\n",
        "        logger.info(f\"Aggregated crime data shape: {agg_crime.shape}\")\n",
        "\n",
        "        # Create lagged features if we have time series data\n",
        "        if time_var == 'month_year':\n",
        "            logger.info(\"Creating lagged features for time series analysis\")\n",
        "\n",
        "            # Sort data by community area and time\n",
        "            agg_crime = agg_crime.sort_values(['community_area', 'month_year'])\n",
        "\n",
        "            try:\n",
        "                # Create lagged features for crime count\n",
        "                for lag in range(1, 4):  # 1, 2, 3 month lags\n",
        "                    agg_crime[f'crime_count_lag{lag}'] = agg_crime.groupby('community_area')['crime_count'].shift(lag)\n",
        "\n",
        "                # Create rolling window features\n",
        "                agg_crime['rolling_mean_3m'] = agg_crime.groupby('community_area')['crime_count'].transform(\n",
        "                    lambda x: x.shift(1).rolling(window=3, min_periods=1).mean()\n",
        "                )\n",
        "\n",
        "                agg_crime['rolling_std_3m'] = agg_crime.groupby('community_area')['crime_count'].transform(\n",
        "                    lambda x: x.shift(1).rolling(window=3, min_periods=1).std()\n",
        "                )\n",
        "\n",
        "                # Fill missing lagged values\n",
        "                lag_cols = [col for col in agg_crime.columns if 'lag' in col or 'rolling' in col]\n",
        "                for col in lag_cols:\n",
        "                    agg_crime[col] = agg_crime[col].fillna(agg_crime['crime_count'].median())\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error creating time series features: {e}\")\n",
        "                # Continue without time series features if they fail\n",
        "\n",
        "        # Ensure consistent types for merge\n",
        "        agg_crime['community_area'] = agg_crime['community_area'].astype(str)\n",
        "        community_data['community_area'] = community_data['community_area'].astype(str)\n",
        "\n",
        "        # Merge crime and community data\n",
        "        merged = pd.merge(\n",
        "            agg_crime,\n",
        "            community_data,\n",
        "            on=\"community_area\",\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Merged dataset shape: {merged.shape}\")\n",
        "\n",
        "        # Apply reporting bias correction if available\n",
        "        if CONFIG.reporting_bias_correction and 'reporting_factor' in merged.columns:\n",
        "            merged = apply_reporting_bias_correction(merged, 'crime_count')\n",
        "\n",
        "            # Update target column if using adjusted version\n",
        "            if 'crime_count_adjusted' in merged.columns:\n",
        "                self.target_col = 'crime_count_adjusted'\n",
        "                logger.info(f\"Using bias-adjusted target: {self.target_col}\")\n",
        "\n",
        "        # Calculate crime rate if population available\n",
        "        if 'population' in merged.columns:\n",
        "            # Ensure population is positive to avoid division by zero\n",
        "            merged['population'] = merged['population'].replace({0: np.nan})\n",
        "            merged['population'] = merged['population'].fillna(merged['population'].median())\n",
        "\n",
        "            # Create crime rate per 1000 people\n",
        "            crime_rate_col = 'crime_rate'\n",
        "            merged[crime_rate_col] = (merged['crime_count'] / merged['population']) * 1000\n",
        "\n",
        "            # Also create adjusted crime rate if available\n",
        "            if 'crime_count_adjusted' in merged.columns:\n",
        "                adj_rate_col = 'crime_rate_adjusted'\n",
        "                merged[adj_rate_col] = (merged['crime_count_adjusted'] / merged['population']) * 1000\n",
        "\n",
        "                # Update target column to use adjusted rate\n",
        "                self.target_col = adj_rate_col\n",
        "                logger.info(f\"Using adjusted crime rate as target: {self.target_col}\")\n",
        "\n",
        "            elif self.target_col == 'crime_count':\n",
        "                # Use crime rate as target if no adjusted version\n",
        "                self.target_col = crime_rate_col\n",
        "                logger.info(f\"Using crime rate as target: {self.target_col}\")\n",
        "\n",
        "        # Create month and year features from month_year if needed\n",
        "        if 'month_year' in merged.columns and not ('month' in merged.columns and 'year' in merged.columns):\n",
        "            try:\n",
        "                # Extract month and year\n",
        "                merged['month'] = merged['month_year'].str.split('-').str[1].astype(int)\n",
        "                merged['year'] = merged['month_year'].str.split('-').str[0].astype(int)\n",
        "\n",
        "                # Create month one-hot encoding\n",
        "                for month in range(1, 13):\n",
        "                    merged[f'month_{month}'] = (merged['month'] == month).astype(int)\n",
        "\n",
        "                # Create seasonal indicators\n",
        "                merged['season_winter'] = ((merged['month'] == 12) | (merged['month'] < 3)).astype(int)\n",
        "                merged['season_spring'] = ((merged['month'] >= 3) & (merged['month'] < 6)).astype(int)\n",
        "                merged['season_summer'] = ((merged['month'] >= 6) & (merged['month'] < 9)).astype(int)\n",
        "                merged['season_fall'] = ((merged['month'] >= 9) & (merged['month'] < 12)).astype(int)\n",
        "\n",
        "                # Create year indicators if we have multiple years\n",
        "                years = merged['year'].unique()\n",
        "                if len(years) > 1:\n",
        "                    for year in years:\n",
        "                        merged[f'year_{year}'] = (merged['year'] == year).astype(int)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error creating time features from month_year: {e}\")\n",
        "\n",
        "        # Create interaction features\n",
        "        merged = self.create_interaction_features(merged)\n",
        "\n",
        "        # Optimize memory\n",
        "        self.merged_data = downcast_dtypes(merged.copy())\n",
        "        logger.info(f\"Final merged dataset shape: {self.merged_data.shape}\")\n",
        "\n",
        "        return self.merged_data\n",
        "\n",
        "    def create_interaction_features(self, df):\n",
        "        \"\"\"\n",
        "        Create interaction features for modeling\n",
        "        \"\"\"\n",
        "        if df is None or len(df) == 0:\n",
        "            return df\n",
        "\n",
        "        try:\n",
        "            # Create a copy to avoid modifying the original\n",
        "            result = df.copy()\n",
        "\n",
        "            # Create socioeconomic interactions if columns are available\n",
        "\n",
        "            # 1. Interaction between hardship and crime types\n",
        "            if 'hardship_index' in result.columns:\n",
        "                for crime_type in ['violent_crime_sum', 'property_crime_sum', 'drug_crime_sum']:\n",
        "                    if crime_type in result.columns:\n",
        "                        result[f'hardship_{crime_type}_interaction'] = result['hardship_index'] * result[crime_type]\n",
        "\n",
        "            # 2. Interaction between income and crime types\n",
        "            if 'per_capita_income' in result.columns:\n",
        "                for crime_type in ['violent_crime_sum', 'property_crime_sum', 'drug_crime_sum']:\n",
        "                    if crime_type in result.columns:\n",
        "                        result[f'income_{crime_type}_interaction'] = result['per_capita_income'] * result[crime_type]\n",
        "\n",
        "            # 3. Seasonal interactions\n",
        "            if 'season_summer' in result.columns:\n",
        "                for crime_type in ['violent_crime_sum', 'property_crime_sum']:\n",
        "                    if crime_type in result.columns:\n",
        "                        result[f'summer_{crime_type}_interaction'] = result['season_summer'] * result[crime_type]\n",
        "\n",
        "            logger.info(\"Created interaction features\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating interaction features: {e}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def prepare_train_test_split(self, split_method=None):\n",
        "        \"\"\"\n",
        "        Prepare data for model training\n",
        "        \"\"\"\n",
        "        if self.merged_data is None:\n",
        "            logger.error(\"Merged data is not available. Run merge_datasets() first.\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        try:\n",
        "            # Use specified split method or config default\n",
        "            split_method = split_method if split_method is not None else CONFIG.time_split_method\n",
        "            logger.info(f\"Preparing train/test split using {split_method} method\")\n",
        "\n",
        "            # Determine target column\n",
        "            if self.target_col in self.merged_data.columns:\n",
        "                target_col = self.target_col\n",
        "            elif \"crime_count\" in self.merged_data.columns:\n",
        "                target_col = \"crime_count\"\n",
        "                self.target_col = target_col\n",
        "            else:\n",
        "                logger.error(\"No target column found in merged data.\")\n",
        "                return None, None, None, None\n",
        "\n",
        "            # Extract features and target\n",
        "            X = self.merged_data.drop(columns=[target_col], errors=\"ignore\")\n",
        "            y = self.merged_data[target_col]\n",
        "\n",
        "            # For time-based split\n",
        "            if split_method == 'by_year' and 'year' in X.columns:\n",
        "                try:\n",
        "                    # Get unique years\n",
        "                    years = sorted(X['year'].unique())\n",
        "\n",
        "                    if len(years) < 2:\n",
        "                        logger.warning(\"Not enough years for time-based split, falling back to random\")\n",
        "                        split_method = 'random'\n",
        "                    else:\n",
        "                        # Use earlier years for training, latest year for testing\n",
        "                        train_years = years[:-1]\n",
        "                        test_year = years[-1]\n",
        "\n",
        "                        logger.info(f\"Time-based split: train years={train_years}, test year={test_year}\")\n",
        "\n",
        "                        # Create train/test masks\n",
        "                        train_mask = X['year'].isin(train_years)\n",
        "                        test_mask = X['year'] == test_year\n",
        "\n",
        "                        # Check if masks contain data\n",
        "                        if train_mask.sum() == 0 or test_mask.sum() == 0:\n",
        "                            logger.warning(\"One or both splits are empty, falling back to random split\")\n",
        "                            split_method = 'random'\n",
        "                        else:\n",
        "                            # Split data\n",
        "                            X_train = X[train_mask]\n",
        "                            X_test = X[test_mask]\n",
        "                            y_train = y[train_mask]\n",
        "                            y_test = y[test_mask]\n",
        "\n",
        "                            # Check for NaN values in target\n",
        "                            train_nan_count = y_train.isna().sum()\n",
        "                            test_nan_count = y_test.isna().sum()\n",
        "\n",
        "                            if train_nan_count > 0 or test_nan_count > 0:\n",
        "                                logger.warning(f\"Found {train_nan_count} NaN values in train target and {test_nan_count} in test target\")\n",
        "\n",
        "                                # Filter out NaN values\n",
        "                                train_valid = ~y_train.isna()\n",
        "                                test_valid = ~y_test.isna()\n",
        "\n",
        "                                X_train = X_train[train_valid]\n",
        "                                y_train = y_train[train_valid]\n",
        "                                X_test = X_test[test_valid]\n",
        "                                y_test = y_test[test_valid]\n",
        "\n",
        "                            logger.info(f\"Train-test split: {X_train.shape} train, {X_test.shape} test\")\n",
        "                            return X_train, X_test, y_train, y_test\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error in time-based split: {e}\")\n",
        "                    split_method = 'random'\n",
        "\n",
        "            # For random split (default fallback)\n",
        "            if split_method != 'by_year' or 'year' not in X.columns:\n",
        "                # Filter out NaN values in target\n",
        "                valid_mask = ~y.isna()\n",
        "                if valid_mask.sum() < len(y):\n",
        "                    logger.warning(f\"Filtering out {len(y) - valid_mask.sum()} NaN values in target\")\n",
        "                    X = X[valid_mask]\n",
        "                    y = y[valid_mask]\n",
        "\n",
        "                # Stratify by quartiles of target if requested\n",
        "                if CONFIG.stratify_target:\n",
        "                    # Create target quartiles\n",
        "                    try:\n",
        "                        target_bins = pd.qcut(y, q=4, duplicates='drop')\n",
        "                        logger.info(\"Using stratified random split\")\n",
        "                    except ValueError as e:\n",
        "                        # If not enough unique values, don't stratify\n",
        "                        target_bins = None\n",
        "                        logger.warning(f\"Not enough unique values for stratification: {e}\")\n",
        "                else:\n",
        "                    target_bins = None\n",
        "\n",
        "                # Perform split\n",
        "                X_train, X_test, y_train, y_test = train_test_split(\n",
        "                    X, y,\n",
        "                    test_size=CONFIG.test_size,\n",
        "                    stratify=target_bins,\n",
        "                    random_state=CONFIG.random_state\n",
        "                )\n",
        "\n",
        "                logger.info(f\"Random train-test split: {X_train.shape} train, {X_test.shape} test\")\n",
        "                return X_train, X_test, y_train, y_test\n",
        "\n",
        "            # If we reach here, something went wrong\n",
        "            logger.error(\"Failed to create train/test split\")\n",
        "            return None, None, None, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in train/test split: {e}\")\n",
        "            return None, None, None, None\n",
        "\n",
        "    def prepare_model_features(self, X_train, X_test, y_train=None, feature_selection=True):\n",
        "        \"\"\"\n",
        "        Prepare features for modeling with proper encoding and scaling\n",
        "\n",
        "        Args:\n",
        "            X_train: Training features\n",
        "            X_test: Testing features\n",
        "            y_train: Training target (needed for feature selection)\n",
        "            feature_selection: Whether to perform feature selection\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (processed X_train, processed X_test, selected feature names)\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing model features\")\n",
        "\n",
        "        try:\n",
        "            if X_train is None or X_test is None:\n",
        "                logger.error(\"Invalid input data for feature preparation\")\n",
        "                return None, None, []\n",
        "\n",
        "            # Get numeric columns only\n",
        "            numeric_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            logger.info(f\"Found {len(numeric_cols)} numeric columns available for modeling\")\n",
        "\n",
        "            # Fix: Set default max_features if not found in config\n",
        "            max_features = 30  # Default value\n",
        "            if hasattr(CONFIG, 'max_features'):\n",
        "                max_features = CONFIG.max_features\n",
        "\n",
        "            # Dynamic setting of max_features based on available columns\n",
        "            if max_features is None or max_features > len(numeric_cols):\n",
        "                max_features = len(numeric_cols)\n",
        "                logger.info(f\"Setting max_features to {max_features} based on available data\")\n",
        "\n",
        "            # Get basic features\n",
        "            X_train_features = X_train[numeric_cols].copy()\n",
        "            X_test_features = X_test[numeric_cols].copy()\n",
        "\n",
        "            # Handle NaN values column by column instead of using SimpleImputer\n",
        "            # This ensures the column count doesn't change\n",
        "            nan_columns = [col for col in numeric_cols if\n",
        "                          X_train_features[col].isna().any() or X_test_features[col].isna().any()]\n",
        "\n",
        "            if nan_columns:\n",
        "                logger.info(f\"Found {len(nan_columns)} columns with NaN values. Applying column-wise imputation.\")\n",
        "\n",
        "                # Process each column individually\n",
        "                for col in nan_columns:\n",
        "                    # Get the appropriate fill value (median by default)\n",
        "                    if CONFIG.imputation_strategy == 'median':\n",
        "                        fill_value = X_train_features[col].median()\n",
        "                    elif CONFIG.imputation_strategy == 'mean':\n",
        "                        fill_value = X_train_features[col].mean()\n",
        "                    else:\n",
        "                        fill_value = 0\n",
        "\n",
        "                    # Fill missing values in both train and test sets\n",
        "                    X_train_features[col] = X_train_features[col].fillna(fill_value)\n",
        "                    X_test_features[col] = X_test_features[col].fillna(fill_value)\n",
        "\n",
        "                # Verify no more NaNs remain\n",
        "                train_nans = X_train_features.isna().sum().sum()\n",
        "                test_nans = X_test_features.isna().sum().sum()\n",
        "\n",
        "                if train_nans > 0 or test_nans > 0:\n",
        "                    logger.warning(f\"NaNs still present after imputation: {train_nans} in train, {test_nans} in test\")\n",
        "                    # Apply fallback imputation to any remaining NaNs\n",
        "                    X_train_features = X_train_features.fillna(0)\n",
        "                    X_test_features = X_test_features.fillna(0)\n",
        "                    logger.info(\"Fallback imputation applied: remaining NaNs filled with 0\")\n",
        "                else:\n",
        "                    logger.info(\"All NaN values successfully imputed\")\n",
        "            else:\n",
        "                logger.info(\"No NaN values detected in features\")\n",
        "\n",
        "            # Apply feature selection if requested\n",
        "            if feature_selection and CONFIG.feature_selection and y_train is not None:\n",
        "                # Select features based on importance\n",
        "                selected_features = select_features(\n",
        "                    X_train_features,\n",
        "                    y_train,\n",
        "                    method='importance',\n",
        "                    max_features=max_features\n",
        "                )\n",
        "\n",
        "                # Filter features\n",
        "                X_train_features = X_train_features[selected_features]\n",
        "                X_test_features = X_test_features[selected_features]\n",
        "\n",
        "                logger.info(f\"Selected {len(selected_features)} features\")\n",
        "            else:\n",
        "                # Use all numeric features if no feature selection or missing y_train\n",
        "                if y_train is None and feature_selection:\n",
        "                    logger.warning(\"No y_train provided for feature selection. Using all numeric features.\")\n",
        "                selected_features = numeric_cols\n",
        "                logger.info(f\"Using all {len(selected_features)} numeric features\")\n",
        "\n",
        "            # Set final processed features\n",
        "            X_train_processed = X_train_features\n",
        "            X_test_processed = X_test_features\n",
        "\n",
        "            logger.info(f\"Prepared features: train={X_train_processed.shape}, test={X_test_processed.shape}\")\n",
        "            return X_train_processed, X_test_processed, selected_features\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error preparing model features: {e}\")\n",
        "            return None, None, []\n",
        "\n",
        "    def save_processed_data(self, file_prefix=\"processed_data\"):\n",
        "        \"\"\"\n",
        "        Save processed data to disk\n",
        "        \"\"\"\n",
        "        if self.merged_data is None:\n",
        "            logger.error(\"No processed data to save\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Save merged data\n",
        "            file_path = os.path.join(CONFIG.data_dir, f\"{file_prefix}.parquet\")\n",
        "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "            self.merged_data.to_parquet(file_path, compression=\"snappy\")\n",
        "            logger.info(f\"Saved processed data to {file_path}\")\n",
        "\n",
        "            # Save column info\n",
        "            columns_info = {\n",
        "                \"target_column\": self.target_col,\n",
        "                \"feature_columns\": list(self.merged_data.columns),\n",
        "                \"numeric_columns\": list(self.merged_data.select_dtypes(include=[np.number]).columns),\n",
        "                \"categorical_columns\": list(self.merged_data.select_dtypes(include=[\"object\", \"category\"]).columns),\n",
        "                \"processing_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "\n",
        "            # Save as JSON\n",
        "            info_path = os.path.join(CONFIG.data_dir, f\"{file_prefix}_info.json\")\n",
        "            with open(info_path, 'w') as f:\n",
        "                json.dump(columns_info, f, indent=4)\n",
        "\n",
        "            logger.info(f\"Saved column information to {info_path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving processed data: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_processed_data(self, file_prefix=\"processed_data\"):\n",
        "        \"\"\"\n",
        "        Load previously processed data\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load data\n",
        "            file_path = os.path.join(CONFIG.data_dir, f\"{file_prefix}.parquet\")\n",
        "            if not os.path.exists(file_path):\n",
        "                logger.error(f\"Processed data file not found: {file_path}\")\n",
        "                return None\n",
        "\n",
        "            self.merged_data = pd.read_parquet(file_path)\n",
        "\n",
        "            # Load column info\n",
        "            info_path = os.path.join(CONFIG.data_dir, f\"{file_prefix}_info.json\")\n",
        "            if os.path.exists(info_path):\n",
        "                with open(info_path, 'r') as f:\n",
        "                    columns_info = json.load(f)\n",
        "\n",
        "                # Set target column\n",
        "                self.target_col = columns_info.get(\"target_column\", CONFIG.target)\n",
        "            else:\n",
        "                logger.warning(f\"Column info file not found: {info_path}\")\n",
        "                # Set default target column\n",
        "                self.target_col = CONFIG.target\n",
        "\n",
        "            logger.info(f\"Loaded processed data with shape {self.merged_data.shape}\")\n",
        "            logger.info(f\"Using target column: {self.target_col}\")\n",
        "\n",
        "            return self.merged_data\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading processed data: {e}\")\n",
        "            return None\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# FAIRNESS-AWARE MODEL CLASSES\n",
        "# ---------------------------------------------------------------------\n",
        "class FairnessAwareModel:\n",
        "    \"\"\"Class implementing fairness-aware machine learning models\"\"\"\n",
        "\n",
        "    def __init__(self, base_model=None, protected_attributes=None):\n",
        "        \"\"\"\n",
        "        Initialize fairness-aware model\n",
        "\n",
        "        Args:\n",
        "            base_model: Base ML model to use\n",
        "            protected_attributes: List of protected attribute column names\n",
        "        \"\"\"\n",
        "        self.base_model = base_model\n",
        "        if self.base_model is None:\n",
        "            self.base_model = xgb.XGBRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=4,\n",
        "                learning_rate=0.03,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=1.0,\n",
        "                random_state=CONFIG.random_state\n",
        "            )\n",
        "\n",
        "        self.protected_attributes = protected_attributes\n",
        "        if self.protected_attributes is None:\n",
        "            self.protected_attributes = CONFIG.protected_attributes\n",
        "\n",
        "        self.reweighing_transformer = None\n",
        "        self.adversarial_model = None\n",
        "        self.fairness_metrics = {}\n",
        "        self.group_thresholds = {}\n",
        "\n",
        "        # Set flags for available methods\n",
        "        self.use_reweighting = CONFIG.use_reweighting and AIF360_AVAILABLE\n",
        "        self.use_adversarial = CONFIG.use_adversarial_debiasing and TF_AVAILABLE\n",
        "\n",
        "    def reweighing_fit_transform(self, X, y, protected_attribute):\n",
        "        \"\"\"\n",
        "        Apply the reweighing technique to reduce bias\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Target\n",
        "            protected_attribute: Protected attribute column name\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (transformed X, transformed y, sample weights)\n",
        "        \"\"\"\n",
        "        if not AIF360_AVAILABLE:\n",
        "            logger.warning(\"AIF360 not available, skipping reweighing\")\n",
        "            return X, y, np.ones(len(y))\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Applying reweighing for protected attribute: {protected_attribute}\")\n",
        "\n",
        "            # Prepare dataset for AIF360\n",
        "            feature_names = X.columns.tolist()\n",
        "            protected_idx = feature_names.index(protected_attribute)\n",
        "\n",
        "            # Convert to binary for simplification in this example\n",
        "            # In practice, you'd bin the values properly\n",
        "            if not pd.api.types.is_categorical_dtype(X[protected_attribute]):\n",
        "                median_val = X[protected_attribute].median()\n",
        "                protected_values = (X[protected_attribute] > median_val).astype(int).values\n",
        "            else:\n",
        "                protected_values = X[protected_attribute].cat.codes.values\n",
        "\n",
        "            # For regression, convert to binary classification temporarily\n",
        "            if isinstance(y, pd.Series) or isinstance(y, np.ndarray):\n",
        "                median_target = np.median(y)\n",
        "                binary_y = (y > median_target).astype(int).values\n",
        "            else:\n",
        "                binary_y = y\n",
        "\n",
        "            # Create aif360 dataset\n",
        "            dataset = BinaryLabelDataset(\n",
        "                df=pd.concat([X, pd.Series(binary_y, name='target')], axis=1),\n",
        "                label_names=['target'],\n",
        "                protected_attribute_names=[protected_attribute]\n",
        "            )\n",
        "\n",
        "            # Apply reweighing\n",
        "            RW = Reweighing(unprivileged_groups=[{protected_attribute: 0}],\n",
        "                           privileged_groups=[{protected_attribute: 1}])\n",
        "            self.reweighing_transformer = RW\n",
        "            transf_dataset = RW.fit_transform(dataset)\n",
        "\n",
        "            # Extract weights\n",
        "            instance_weights = transf_dataset.instance_weights\n",
        "\n",
        "            return X, y, instance_weights\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in reweighing: {e}\")\n",
        "            return X, y, np.ones(len(y))\n",
        "\n",
        "    def train_adversarial_model(self, X, y, protected_attribute, epochs=50):\n",
        "        \"\"\"\n",
        "        Train adversarial debiasing model\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Target\n",
        "            protected_attribute: Protected attribute to debias against\n",
        "            epochs: Number of training epochs\n",
        "\n",
        "        Returns:\n",
        "            Trained model\n",
        "        \"\"\"\n",
        "        if not TF_AVAILABLE:\n",
        "            logger.warning(\"TensorFlow not available, skipping adversarial debiasing\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Training adversarial debiasing model for {protected_attribute}\")\n",
        "\n",
        "            # Extract protected attribute\n",
        "            if protected_attribute in X.columns:\n",
        "                # Normalize to 0-1 range for the adversary\n",
        "                protected_values = X[protected_attribute].values\n",
        "                protected_values = (protected_values - protected_values.min()) / \\\n",
        "                                   (protected_values.max() - protected_values.min() + 1e-10)\n",
        "                protected_values = protected_values.reshape(-1, 1)\n",
        "            else:\n",
        "                logger.warning(f\"Protected attribute {protected_attribute} not found\")\n",
        "                return None\n",
        "\n",
        "            # Prepare data\n",
        "            X_tensor = X.copy()\n",
        "            X_numeric = X_tensor.select_dtypes(include=['number'])\n",
        "\n",
        "            # Normalize features\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "            # Build adversarial debiasing model with TensorFlow\n",
        "            # Main prediction model\n",
        "            main_input = tf.keras.layers.Input(shape=(X_scaled.shape[1],))\n",
        "            main_hidden1 = tf.keras.layers.Dense(64, activation='relu')(main_input)\n",
        "            main_hidden2 = tf.keras.layers.Dense(32, activation='relu')(main_hidden1)\n",
        "            main_output = tf.keras.layers.Dense(1, activation='linear', name='main_output')(main_hidden2)\n",
        "\n",
        "            # Adversarial model to predict protected attribute\n",
        "            adv_hidden1 = tf.keras.layers.Dense(32, activation='relu',\n",
        "                                               trainable=False)(main_hidden2)\n",
        "            adv_output = tf.keras.layers.Dense(1, activation='sigmoid',\n",
        "                                             name='adversarial_output')(adv_hidden1)\n",
        "\n",
        "            # Create combined model\n",
        "            combined_model = tf.keras.Model(\n",
        "                inputs=main_input,\n",
        "                outputs=[main_output, adv_output]\n",
        "            )\n",
        "\n",
        "            # Custom loss to maximize adversarial loss\n",
        "            def adversarial_loss(y_true, y_pred):\n",
        "                # We want to maximize this loss (minimize negative)\n",
        "                return -tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "\n",
        "            # Compile model with multiple outputs and losses\n",
        "            combined_model.compile(\n",
        "                optimizer='adam',\n",
        "                loss={\n",
        "                    'main_output': 'mse',  # For regression\n",
        "                    'adversarial_output': adversarial_loss\n",
        "                },\n",
        "                loss_weights={\n",
        "                    'main_output': 1.0,\n",
        "                    'adversarial_output': 0.2  # Weight of the adversarial component\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # Train model\n",
        "            combined_model.fit(\n",
        "                X_scaled,\n",
        "                {\n",
        "                    'main_output': y,\n",
        "                    'adversarial_output': protected_values\n",
        "                },\n",
        "                epochs=epochs,\n",
        "                batch_size=32,\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Save model and preprocessing information\n",
        "            self.adversarial_model = {\n",
        "                'model': combined_model,\n",
        "                'scaler': scaler,\n",
        "                'feature_names': X_numeric.columns.tolist()\n",
        "            }\n",
        "\n",
        "            logger.info(\"Adversarial debiasing model trained successfully\")\n",
        "            return combined_model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in adversarial model training: {e}\")\n",
        "            return None\n",
        "\n",
        "    def predict_with_adversarial(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the adversarial model\n",
        "\n",
        "        Args:\n",
        "            X: Features for prediction\n",
        "\n",
        "        Returns:\n",
        "            Predictions\n",
        "        \"\"\"\n",
        "        if self.adversarial_model is None:\n",
        "            logger.warning(\"No adversarial model available, using base model instead\")\n",
        "            return self.base_model.predict(X)\n",
        "\n",
        "        try:\n",
        "            # Prepare data\n",
        "            X_numeric = X[self.adversarial_model['feature_names']]\n",
        "            X_scaled = self.adversarial_model['scaler'].transform(X_numeric)\n",
        "\n",
        "            # Get predictions (main output)\n",
        "            predictions = self.adversarial_model['model'].predict(X_scaled)[0]\n",
        "\n",
        "            return predictions.flatten()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in adversarial prediction: {e}\")\n",
        "            return self.base_model.predict(X)\n",
        "\n",
        "    def apply_threshold_moving(self, X, predictions, protected_attribute):\n",
        "        \"\"\"\n",
        "        Apply threshold adjustments to balance error rates across groups\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            predictions: Model predictions\n",
        "            protected_attribute: Protected attribute column name\n",
        "\n",
        "        Returns:\n",
        "            Adjusted predictions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Applying threshold moving for {protected_attribute}\")\n",
        "\n",
        "            # Get protected attribute values\n",
        "            if protected_attribute not in X.columns:\n",
        "                logger.warning(f\"Protected attribute {protected_attribute} not found\")\n",
        "                return predictions\n",
        "\n",
        "            protected_values = X[protected_attribute]\n",
        "\n",
        "            # Create groups (using quartiles)\n",
        "            try:\n",
        "                groups = pd.qcut(protected_values, 4, labels=False)\n",
        "            except ValueError:\n",
        "                # If too few unique values, use median split\n",
        "                groups = (protected_values > protected_values.median()).astype(int)\n",
        "\n",
        "            unique_groups = np.unique(groups)\n",
        "            adjusted_predictions = predictions.copy()\n",
        "\n",
        "            # Calculate group-specific thresholds\n",
        "            for group in unique_groups:\n",
        "                group_mask = (groups == group)\n",
        "                group_preds = predictions[group_mask]\n",
        "                group_mean = np.mean(group_preds)\n",
        "                group_std = np.std(group_preds)\n",
        "\n",
        "                # Store thresholds for later use\n",
        "                self.group_thresholds[f\"group_{group}\"] = {\n",
        "                    'mean': group_mean,\n",
        "                    'std': group_std,\n",
        "                    'count': np.sum(group_mask)\n",
        "                }\n",
        "\n",
        "                # Adjust predictions based on group statistics\n",
        "                # Implementation depends on the fairness criteria\n",
        "                # For this example, we normalize the predictions within each group\n",
        "                if group_std > 0:\n",
        "                    adjusted_predictions[group_mask] = (\n",
        "                        (predictions[group_mask] - group_mean) / group_std\n",
        "                    )\n",
        "\n",
        "            # Rescale back to original range\n",
        "            overall_mean = np.mean(predictions)\n",
        "            overall_std = np.std(predictions)\n",
        "\n",
        "            adjusted_predictions = adjusted_predictions * overall_std + overall_mean\n",
        "\n",
        "            logger.info(\"Threshold moving applied successfully\")\n",
        "            return adjusted_predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in threshold moving: {e}\")\n",
        "            return predictions\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"\n",
        "        Train the fairness-aware model\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: Target\n",
        "            sample_weight: Optional sample weights\n",
        "\n",
        "        Returns:\n",
        "            Self\n",
        "        \"\"\"\n",
        "        logger.info(\"Training fairness-aware model\")\n",
        "\n",
        "        try:\n",
        "            # Apply fairness techniques one by one\n",
        "            # 1. Reweighing for first protected attribute\n",
        "            if self.use_reweighting and self.protected_attributes:\n",
        "                primary_attr = self.protected_attributes[0]\n",
        "                _, _, sample_weight = self.reweighing_fit_transform(X, y, primary_attr)\n",
        "\n",
        "            # 2. Train base model with sample weights\n",
        "            logger.info(\"Training base model\")\n",
        "            self.base_model.fit(X, y, sample_weight=sample_weight)\n",
        "\n",
        "            # 3. Train adversarial model for debiasing\n",
        "            if self.use_adversarial and self.protected_attributes:\n",
        "                for attr in self.protected_attributes[:1]:  # Start with one attribute\n",
        "                    self.train_adversarial_model(X, y, attr)\n",
        "\n",
        "            return self\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in fairness-aware model training: {e}\")\n",
        "            # Fall back to base model\n",
        "            self.base_model.fit(X, y)\n",
        "            return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions with the fairness-aware model\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "\n",
        "        Returns:\n",
        "            Predictions\n",
        "        \"\"\"\n",
        "        logger.info(\"Making fairness-aware predictions\")\n",
        "\n",
        "        try:\n",
        "            # Use adversarial model if available\n",
        "            if self.use_adversarial and self.adversarial_model is not None:\n",
        "                predictions = self.predict_with_adversarial(X)\n",
        "            else:\n",
        "                # Otherwise use base model\n",
        "                predictions = self.base_model.predict(X)\n",
        "\n",
        "            # Apply threshold moving if protected attributes available\n",
        "            if self.protected_attributes:\n",
        "                for attr in self.protected_attributes[:1]:  # Start with one attribute\n",
        "                    predictions = self.apply_threshold_moving(X, predictions, attr)\n",
        "\n",
        "            return predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in fairness-aware prediction: {e}\")\n",
        "            # Fall back to base model\n",
        "            return self.base_model.predict(X)\n",
        "\n",
        "    def evaluate_fairness(self, X, y, predictions):\n",
        "        \"\"\"\n",
        "        Evaluate model fairness across protected groups\n",
        "\n",
        "        Args:\n",
        "            X: Features\n",
        "            y: True target values\n",
        "            predictions: Model predictions\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of fairness metrics\n",
        "        \"\"\"\n",
        "        logger.info(\"Evaluating model fairness\")\n",
        "\n",
        "        try:\n",
        "            fairness_results = {}\n",
        "\n",
        "            # Evaluate fairness for each protected attribute\n",
        "            for attr in self.protected_attributes:\n",
        "                if attr not in X.columns:\n",
        "                    continue\n",
        "\n",
        "                attr_values = X[attr]\n",
        "\n",
        "                # Create groups (using quartiles for numeric attributes)\n",
        "                try:\n",
        "                    if pd.api.types.is_numeric_dtype(attr_values):\n",
        "                        groups = pd.qcut(attr_values, 4, labels=False)\n",
        "                    else:\n",
        "                        groups = attr_values.astype('category').cat.codes\n",
        "                except ValueError:\n",
        "                    # If too few unique values, use median split\n",
        "                    groups = (protected_values > protected_values.median()).astype(int)\n",
        "\n",
        "                unique_groups = np.unique(groups)\n",
        "\n",
        "                # Calculate metrics per group\n",
        "                group_metrics = {}\n",
        "\n",
        "                for group in unique_groups:\n",
        "                    group_mask = (groups == group)\n",
        "                    if np.sum(group_mask) < 10:  # Skip groups that are too small\n",
        "                        continue\n",
        "\n",
        "                    # Calculate error metrics\n",
        "                    group_y = y[group_mask]\n",
        "                    group_pred = predictions[group_mask]\n",
        "\n",
        "                    mse = mean_squared_error(group_y, group_pred)\n",
        "                    rmse = np.sqrt(mse)\n",
        "                    r2 = r2_score(group_y, group_pred)\n",
        "                    mae = mean_absolute_error(group_y, group_pred)\n",
        "\n",
        "                    # Prediction ratio\n",
        "                    pred_ratio = np.mean(group_pred) / np.mean(group_y) if np.mean(group_y) > 0 else 1.0\n",
        "\n",
        "                    group_metrics[f\"group_{group}\"] = {\n",
        "                        'count': np.sum(group_mask),\n",
        "                        'mse': mse,\n",
        "                        'rmse': rmse,\n",
        "                        'r2': r2,\n",
        "                        'mae': mae,\n",
        "                        'mean_pred': np.mean(group_pred),\n",
        "                        'mean_actual': np.mean(group_y),\n",
        "                        'pred_ratio': pred_ratio\n",
        "                    }\n",
        "\n",
        "                # Calculate disparities between groups\n",
        "                if len(group_metrics) >= 2:\n",
        "                    # RMSE disparity\n",
        "                    rmse_values = [metrics['rmse'] for metrics in group_metrics.values()]\n",
        "                    min_rmse, max_rmse = min(rmse_values), max(rmse_values)\n",
        "                    rmse_ratio = min_rmse / max_rmse if max_rmse > 0 else 0\n",
        "\n",
        "                    # Prediction ratio disparity\n",
        "                    pred_ratios = [metrics['pred_ratio'] for metrics in group_metrics.values()]\n",
        "                    min_ratio, max_ratio = min(pred_ratios), max(pred_ratios)\n",
        "                    pred_ratio_disparity = min_ratio / max_ratio if max_ratio > 0 else 0\n",
        "\n",
        "                    fairness_results[attr] = {\n",
        "                        'rmse_min': min_rmse,\n",
        "                        'rmse_max': max_rmse,\n",
        "                        'rmse_ratio': rmse_ratio,\n",
        "                        'pred_ratio_disparity': pred_ratio_disparity,\n",
        "                        'group_metrics': group_metrics,\n",
        "                        'disparate_impact': pred_ratio_disparity\n",
        "                    }\n",
        "\n",
        "                    logger.info(f\"Fairness metrics for {attr}:\")\n",
        "                    logger.info(f\"  RMSE ratio: {rmse_ratio:.4f} (closer to 1 = more fair)\")\n",
        "                    logger.info(f\"  Prediction ratio disparity: {pred_ratio_disparity:.4f}\")\n",
        "\n",
        "            self.fairness_metrics = fairness_results\n",
        "            return fairness_results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in fairness evaluation: {e}\")\n",
        "            return {}\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# COMPREHENSIVE FAIRNESS METRICS\n",
        "# ---------------------------------------------------------------------\n",
        "def comprehensive_fairness_analysis(model, X_test, y_test, y_pred, protected_attrs=None):\n",
        "    \"\"\"\n",
        "    Comprehensive fairness analysis with multiple metrics and better grouping\n",
        "    \"\"\"\n",
        "    logger.info(\"Performing comprehensive fairness analysis\")\n",
        "\n",
        "    if protected_attrs is None:\n",
        "        protected_attrs = CONFIG.protected_attributes\n",
        "\n",
        "    # Find available protected attributes in test data\n",
        "    available_attrs = [attr for attr in protected_attrs if attr in X_test.columns]\n",
        "\n",
        "    if not available_attrs:\n",
        "        logger.warning(\"No protected attributes found in test data\")\n",
        "        return {}\n",
        "\n",
        "    fairness_results = {}\n",
        "\n",
        "    # Analyze each attribute\n",
        "    for attr in available_attrs:\n",
        "        try:\n",
        "            logger.info(f\"Analyzing fairness for attribute: {attr}\")\n",
        "\n",
        "            # Ensure attribute is numeric\n",
        "            if not pd.api.types.is_numeric_dtype(X_test[attr]):\n",
        "                logger.info(f\"{attr} is not numeric; converting to numeric if possible\")\n",
        "                try:\n",
        "                    X_test[attr] = pd.to_numeric(X_test[attr], errors='coerce')\n",
        "                except:\n",
        "                    logger.warning(f\"Could not convert {attr} to numeric; skipping\")\n",
        "                    continue\n",
        "\n",
        "            # Improved group creation using quantiles (more robust)\n",
        "            attr_values = X_test[attr].dropna()\n",
        "\n",
        "            # Skip if not enough unique values\n",
        "            if attr_values.nunique() < 2:\n",
        "                logger.warning(f\"Not enough unique values in {attr} for fairness analysis\")\n",
        "                continue\n",
        "\n",
        "            # Create more robust groups - try quantiles first\n",
        "            try:\n",
        "                # Use percentiles rather than quartiles for more granular analysis\n",
        "                percentiles = [0, 25, 50, 75, 100]\n",
        "                bins = np.percentile(attr_values, percentiles)\n",
        "\n",
        "                # If there are repeated values, try fewer bins\n",
        "                if len(np.unique(bins)) < len(bins):\n",
        "                    percentiles = [0, 33, 66, 100]\n",
        "                    bins = np.percentile(attr_values, percentiles)\n",
        "\n",
        "                # If still issues, use min/max approach\n",
        "                if len(np.unique(bins)) < len(bins):\n",
        "                    raise ValueError(\"Repeated bin edges\")\n",
        "\n",
        "                labels = [f\"Group {i+1}\" for i in range(len(percentiles)-1)]\n",
        "                groups = pd.cut(attr_values, bins=bins, labels=labels, include_lowest=True)\n",
        "                group_labels = sorted(groups.unique())\n",
        "\n",
        "                # Map values to groups\n",
        "                value_to_group = {}\n",
        "                for group in group_labels:\n",
        "                    mask = groups == group\n",
        "                    values = attr_values[mask].unique()\n",
        "                    for value in values:\n",
        "                        value_to_group[value] = group\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error creating groups for {attr}: {e}\")\n",
        "\n",
        "                # Fallback: use binning based on value range with fewer bins\n",
        "                try:\n",
        "                    min_val = attr_values.min()\n",
        "                    max_val = attr_values.max()\n",
        "                    bins = np.linspace(min_val, max_val, 3)  # Try just 2 bins if we're having issues\n",
        "                    labels = ['Low', 'High']\n",
        "                    groups = pd.cut(attr_values, bins=bins, labels=labels)\n",
        "                    group_labels = labels\n",
        "\n",
        "                    # Create value to group mapping\n",
        "                    value_to_group = {}\n",
        "                    for i, (lower, upper) in enumerate(zip(bins[:-1], bins[1:])):\n",
        "                        for value in attr_values[(attr_values >= lower) & (attr_values < upper)].unique():\n",
        "                            value_to_group[value] = labels[i]\n",
        "\n",
        "                    # Handle values at the maximum\n",
        "                    for value in attr_values[attr_values == max_val].unique():\n",
        "                        value_to_group[value] = labels[-1]\n",
        "\n",
        "                except Exception as e2:\n",
        "                    logger.warning(f\"Error creating fallback groups for {attr}: {e2}\")\n",
        "                    continue\n",
        "\n",
        "            # Calculate metrics by group\n",
        "            group_metrics = {}\n",
        "\n",
        "            for group in group_labels:\n",
        "                # Find indices for this group\n",
        "                group_indices = []\n",
        "                for i, val in enumerate(X_test[attr]):\n",
        "                    if val in value_to_group and value_to_group[val] == group:\n",
        "                        group_indices.append(i)\n",
        "\n",
        "                # Skip if too few samples\n",
        "                if len(group_indices) < 5:\n",
        "                    logger.info(f\"Group {group} has too few samples ({len(group_indices)}); skipping\")\n",
        "                    continue\n",
        "\n",
        "                # Get predictions and actual values for this group\n",
        "                group_y_test = y_test.iloc[group_indices]\n",
        "                group_y_pred = y_pred[group_indices]\n",
        "\n",
        "                # Calculate error metrics\n",
        "                mse = mean_squared_error(group_y_test, group_y_pred)\n",
        "                rmse = np.sqrt(mse)\n",
        "                r2 = r2_score(group_y_test, group_y_pred)\n",
        "                mae = mean_absolute_error(group_y_test, group_y_pred)\n",
        "\n",
        "                # Calculate MAPE (avoiding zero denominator)\n",
        "                mape = np.mean(np.abs((group_y_test - group_y_pred) / np.maximum(np.abs(group_y_test), 0.0001))) * 100\n",
        "\n",
        "                # Calculate prediction ratio (predicted/actual) to assess over/under-prediction\n",
        "                pred_ratio = np.mean(group_y_pred) / np.mean(group_y_test) if np.mean(group_y_test) > 0 else 1.0\n",
        "\n",
        "                # Calculate calibration - correlation between predictions and actual values\n",
        "                calibration = np.corrcoef(group_y_test, group_y_pred)[0, 1] if len(group_y_test) > 1 else 0\n",
        "\n",
        "                # Store metrics\n",
        "                group_metrics[str(group)] = {\n",
        "                    'count': len(group_indices),\n",
        "                    'mse': mse,\n",
        "                    'rmse': rmse,\n",
        "                    'r2': r2,\n",
        "                    'mae': mae,\n",
        "                    'mape': mape,\n",
        "                    'mean_pred': np.mean(group_y_pred),\n",
        "                    'mean_actual': np.mean(group_y_test),\n",
        "                    'pred_ratio': pred_ratio,\n",
        "                    'calibration': calibration\n",
        "                }\n",
        "\n",
        "                logger.info(f\"Group {group} metrics - RMSE: {rmse:.4f}, R²: {r2:.4f}, Count: {len(group_indices)}\")\n",
        "\n",
        "            # Calculate disparity metrics\n",
        "            if len(group_metrics) >= 2:\n",
        "                # Get metrics across groups\n",
        "                rmse_values = [metrics['rmse'] for metrics in group_metrics.values()]\n",
        "                mae_values = [metrics['mae'] for metrics in group_metrics.values()]\n",
        "                mape_values = [metrics['mape'] for metrics in group_metrics.values()]\n",
        "                pred_ratios = [metrics['pred_ratio'] for metrics in group_metrics.values()]\n",
        "\n",
        "                # Calculate min/max and ratios\n",
        "                min_rmse, max_rmse = min(rmse_values), max(rmse_values)\n",
        "                # Avoid division by zero\n",
        "                rmse_ratio = min_rmse / max_rmse if max_rmse > 0 else 0\n",
        "\n",
        "                min_mae, max_mae = min(mae_values), max(mae_values)\n",
        "                # Avoid division by zero\n",
        "                mae_ratio = min_mae / max_mae if max_mae > 0 else 0\n",
        "\n",
        "                min_mape, max_mape = min(mape_values), max(mape_values)\n",
        "                # Avoid division by zero\n",
        "                mape_ratio = min_mape / max_mape if max_mape > 0 else 0\n",
        "\n",
        "                # Calculate equalized odds approximation (for regression)\n",
        "                # How consistent is the error ratio across groups\n",
        "                pred_ratio_min, pred_ratio_max = min(pred_ratios), max(pred_ratios)\n",
        "                pred_ratio_disparity = pred_ratio_min / pred_ratio_max if pred_ratio_max > 0 else 0\n",
        "\n",
        "                # Disparity metrics (higher ratio = more fair)\n",
        "                fairness_results[attr] = {\n",
        "                    'rmse_min': min_rmse,\n",
        "                    'rmse_max': max_rmse,\n",
        "                    'rmse_ratio': rmse_ratio,\n",
        "                    'rmse_range': max_rmse - min_rmse,\n",
        "                    'mae_min': min_mae,\n",
        "                    'mae_max': max_mae,\n",
        "                    'mae_ratio': mae_ratio,\n",
        "                    'mae_range': max_mae - min_mae,\n",
        "                    'mape_min': min_mape,\n",
        "                    'mape_max': max_mape,\n",
        "                    'mape_ratio': mape_ratio,\n",
        "                    'mape_range': max_mape - min_mape,\n",
        "                    'pred_ratio_disparity': pred_ratio_disparity,\n",
        "                    'group_metrics': group_metrics\n",
        "                }\n",
        "\n",
        "                logger.info(f\"Fairness metrics for {attr}:\")\n",
        "                logger.info(f\"  RMSE ratio: {rmse_ratio:.4f} (closer to 1 = more fair)\")\n",
        "                logger.info(f\"  MAE ratio: {mae_ratio:.4f} (closer to 1 = more fair)\")\n",
        "                logger.info(f\"  Prediction ratio disparity: {pred_ratio_disparity:.4f} (closer to 1 = more fair)\")\n",
        "            else:\n",
        "                logger.warning(f\"Not enough groups for {attr} to calculate fairness metrics\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error analyzing fairness for {attr}: {e}\")\n",
        "\n",
        "    return fairness_results\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# UNCERTAINTY QUANTIFICATION\n",
        "# ---------------------------------------------------------------------\n",
        "def enhanced_prediction_uncertainty(model, X, model_name=None, method='ensemble'):\n",
        "    \"\"\"\n",
        "    Generate predictions with comprehensive uncertainty estimates\n",
        "    \"\"\"\n",
        "    logger.info(f\"Generating enhanced predictions with uncertainty ({method} method)\")\n",
        "\n",
        "    # Use specified model or best model\n",
        "    if model_name is not None and hasattr(model, 'models') and model_name in model.models:\n",
        "        model_to_use = model.models[model_name]\n",
        "    else:\n",
        "        model_to_use = model\n",
        "\n",
        "    if model_to_use is None:\n",
        "        logger.error(\"No model available for prediction\")\n",
        "        return None\n",
        "\n",
        "    # Base predictions\n",
        "    base_predictions = model_to_use.predict(X)\n",
        "\n",
        "    # Create DataFrame for results\n",
        "    results = pd.DataFrame()\n",
        "    results['prediction'] = base_predictions\n",
        "\n",
        "    if method == 'ensemble':\n",
        "        try:\n",
        "            # Use a combination of methods for more robust intervals\n",
        "\n",
        "            # 1. Bootstrap method\n",
        "            bootstrap_models = []\n",
        "            bootstrap_seed = CONFIG.random_state\n",
        "\n",
        "            for i in range(CONFIG.bootstrap_samples):\n",
        "                # Set different seed for each bootstrap\n",
        "                bootstrap_seed += 1\n",
        "\n",
        "                # For more robust uncertainty, use a mixture of:\n",
        "                # - Parameter perturbation\n",
        "                # - Data subsampling\n",
        "                # - Feature subsampling\n",
        "\n",
        "                # A. Parameter perturbation (add noise to predictions)\n",
        "                noise_factor = 0.05 + 0.1 * np.random.random()  # Between 5-15% noise\n",
        "                noise = np.random.RandomState(bootstrap_seed).normal(\n",
        "                    0, base_predictions.std() * noise_factor, size=len(base_predictions)\n",
        "                )\n",
        "\n",
        "                # B. Feature subsampling - add small perturbation based on feature importance\n",
        "                if hasattr(model_to_use, 'feature_importances_'):\n",
        "                    feature_importances = model_to_use.feature_importances_\n",
        "                    feature_indices = np.random.RandomState(bootstrap_seed+100).choice(\n",
        "                        range(len(feature_importances)),\n",
        "                        size=int(0.8*len(feature_importances)),  # Use 80% of features\n",
        "                        replace=False,\n",
        "                        p=feature_importances/feature_importances.sum()  # Weight by importance\n",
        "                    )\n",
        "\n",
        "                    # Create subsampled feature matrix and predict\n",
        "                    X_subsample = X.copy()\n",
        "                    excluded_cols = [col for i, col in enumerate(X.columns) if i not in feature_indices]\n",
        "\n",
        "                    if excluded_cols:\n",
        "                        # Replace excluded columns with their median values\n",
        "                        for col in excluded_cols:\n",
        "                            X_subsample[col] = X_subsample[col].median()\n",
        "\n",
        "                        try:\n",
        "                            # Get predictions with subsampled features\n",
        "                            subsample_preds = model_to_use.predict(X_subsample)\n",
        "                            # Add this perturbation to our noise\n",
        "                            sub_noise = (subsample_preds - base_predictions) * 0.5  # Weight at 50%\n",
        "                            noise += sub_noise\n",
        "                        except:\n",
        "                            pass  # If prediction fails, just skip this component\n",
        "\n",
        "                # Combine all noise sources with base predictions\n",
        "                bootstrap_predictions = base_predictions + noise\n",
        "                bootstrap_models.append(bootstrap_predictions)\n",
        "\n",
        "            # Calculate enhanced statistics\n",
        "            bootstrap_mean = np.mean(bootstrap_models, axis=0)\n",
        "            bootstrap_std = np.std(bootstrap_models, axis=0)\n",
        "\n",
        "            # Calculate prediction intervals at different confidence levels\n",
        "            results['bootstrap_mean'] = bootstrap_mean\n",
        "            results['bootstrap_std'] = bootstrap_std\n",
        "\n",
        "            # 68% confidence interval (≈1 std)\n",
        "            results['lower_68'] = bootstrap_mean - 1.0 * bootstrap_std\n",
        "            results['upper_68'] = bootstrap_mean + 1.0 * bootstrap_std\n",
        "\n",
        "            # 95% confidence interval (≈2 std)\n",
        "            results['lower_95'] = bootstrap_mean - 2.0 * bootstrap_std\n",
        "            results['upper_95'] = bootstrap_mean + 2.0 * bootstrap_std\n",
        "\n",
        "            # 99% confidence interval (≈2.5 std)\n",
        "            results['lower_99'] = bootstrap_mean - 2.5 * bootstrap_std\n",
        "            results['upper_99'] = bootstrap_mean + 2.5 * bootstrap_std\n",
        "\n",
        "            # Ensure no negative values for count/rate data\n",
        "            results['lower_68'] = results['lower_68'].clip(0)\n",
        "            results['lower_95'] = results['lower_95'].clip(0)\n",
        "            results['lower_99'] = results['lower_99'].clip(0)\n",
        "\n",
        "            # Calculate coefficient of variation as uncertainty measure (higher = more uncertain)\n",
        "            results['uncertainty'] = results['bootstrap_std'] / results['bootstrap_mean']\n",
        "            results['uncertainty'] = results['uncertainty'].fillna(0).clip(0, 1)\n",
        "\n",
        "            logger.info(\"Successfully generated enhanced uncertainty estimates\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating enhanced uncertainty estimates: {e}\")\n",
        "            # Return base predictions without uncertainty\n",
        "            return results\n",
        "\n",
        "    return results\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# ETHICAL WARNING SYSTEM\n",
        "# ---------------------------------------------------------------------\n",
        "def ethical_warning_system(predictions, fairness_results, uncertainty_threshold=0.3, fairness_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Generate ethical usage warnings based on model predictions, fairness metrics, and uncertainty\n",
        "    \"\"\"\n",
        "    logger.info(\"Analyzing predictions for ethical concerns\")\n",
        "\n",
        "    warnings = []\n",
        "\n",
        "    # Check for high uncertainty predictions\n",
        "    if 'uncertainty' in predictions.columns:\n",
        "        high_uncertainty = predictions['uncertainty'] > uncertainty_threshold\n",
        "        if high_uncertainty.any():\n",
        "            pct_uncertain = (high_uncertainty.sum() / len(predictions)) * 100\n",
        "            warnings.append({\n",
        "                'type': 'uncertainty',\n",
        "                'severity': 'high' if pct_uncertain > 20 else 'medium',\n",
        "                'message': f\"{pct_uncertain:.1f}% of predictions have high uncertainty (>0.3 coefficient of variation)\",\n",
        "                'recommendation': \"Areas with high uncertainty should not be used for consequential decisions. Consider gathering more data or using alternative methods for these areas.\"\n",
        "            })\n",
        "\n",
        "    # Check fairness metrics\n",
        "    if fairness_results:\n",
        "        for attr, metrics in fairness_results.items():\n",
        "            # Check for disparities in RMSE ratio\n",
        "            if 'rmse_ratio' in metrics and metrics['rmse_ratio'] < fairness_threshold:\n",
        "                warnings.append({\n",
        "                    'type': 'fairness',\n",
        "                    'attribute': attr,\n",
        "                    'severity': 'high' if metrics['rmse_ratio'] < 0.5 else 'medium',\n",
        "                    'message': f\"Significant prediction error disparity found for {attr} (RMSE ratio: {metrics['rmse_ratio']:.2f})\",\n",
        "                    'recommendation': \"Model predictions show different error rates across different groups. Use caution when applying these predictions, particularly for groups with higher error rates.\"\n",
        "                })\n",
        "\n",
        "            # Check for disparities in prediction ratios\n",
        "            if 'pred_ratio_disparity' in metrics and metrics['pred_ratio_disparity'] < fairness_threshold:\n",
        "                warnings.append({\n",
        "                    'type': 'fairness',\n",
        "                    'attribute': attr,\n",
        "                    'severity': 'high' if metrics['pred_ratio_disparity'] < 0.5 else 'medium',\n",
        "                    'message': f\"Potential systematic bias detected for {attr} (prediction ratio disparity: {metrics['pred_ratio_disparity']:.2f})\",\n",
        "                    'recommendation': \"Model may systematically over-predict or under-predict for certain groups. Consider using a fairness-aware model or post-processing corrections.\"\n",
        "                })\n",
        "\n",
        "    # Check for extreme predictions\n",
        "    if 'prediction' in predictions.columns:\n",
        "        mean_pred = predictions['prediction'].mean()\n",
        "        std_pred = predictions['prediction'].std()\n",
        "\n",
        "        # Flag extremely high predictions (3+ standard deviations)\n",
        "        extreme_high = predictions['prediction'] > (mean_pred + 3 * std_pred)\n",
        "        if extreme_high.any():\n",
        "            pct_extreme = (extreme_high.sum() / len(predictions)) * 100\n",
        "            if pct_extreme > 1:  # Only warn if more than 1% of predictions are extreme\n",
        "                warnings.append({\n",
        "                    'type': 'extremity',\n",
        "                    'severity': 'medium',\n",
        "                    'message': f\"{pct_extreme:.1f}% of predictions are extremely high (3+ standard deviations above mean)\",\n",
        "                    'recommendation': \"Review these extreme predictions carefully before using them for decision-making.\"\n",
        "                })\n",
        "\n",
        "    # Generate overall ethical assessment\n",
        "    if not warnings:\n",
        "        ethical_assessment = {\n",
        "            'status': 'green',\n",
        "            'message': \"No significant ethical concerns detected in model predictions.\",\n",
        "            'warnings': []\n",
        "        }\n",
        "    else:\n",
        "        # Count warnings by severity\n",
        "        high_severity = sum(1 for w in warnings if w['severity'] == 'high')\n",
        "\n",
        "        if high_severity > 0:\n",
        "            status = 'red'\n",
        "            message = \"Significant ethical concerns detected. Use caution when applying these predictions.\"\n",
        "        else:\n",
        "            status = 'yellow'\n",
        "            message = \"Some ethical concerns detected. Review warnings before using predictions.\"\n",
        "\n",
        "        ethical_assessment = {\n",
        "            'status': status,\n",
        "            'message': message,\n",
        "            'warnings': warnings\n",
        "        }\n",
        "\n",
        "    logger.info(f\"Ethical assessment: {ethical_assessment['status']} with {len(warnings)} warnings\")\n",
        "    return ethical_assessment\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# MODEL EXPLAINABILITY\n",
        "# ---------------------------------------------------------------------\n",
        "def generate_shap_explanations(model, X_test, feature_names=None):\n",
        "    \"\"\"\n",
        "    Generate SHAP values to explain model predictions\n",
        "    \"\"\"\n",
        "    if not SHAP_AVAILABLE:\n",
        "        logger.warning(\"SHAP not available, skipping explanation generation\")\n",
        "        return None\n",
        "\n",
        "    logger.info(\"Generating SHAP explanations\")\n",
        "\n",
        "    try:\n",
        "        # Use feature names if provided\n",
        "        if feature_names is None and isinstance(X_test, pd.DataFrame):\n",
        "            feature_names = X_test.columns.tolist()\n",
        "\n",
        "        # Create explainer based on model type\n",
        "        if isinstance(model, xgb.XGBRegressor):\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "        elif isinstance(model, lgb.LGBMRegressor):\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "        elif isinstance(model, RandomForestRegressor):\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "        else:\n",
        "            # Use KernelExplainer for other model types\n",
        "            # Use only a subset for speed if data is large\n",
        "            if len(X_test) > 100:\n",
        "                background = shap.sample(X_test, 100)\n",
        "            else:\n",
        "                background = X_test\n",
        "\n",
        "            explainer = shap.KernelExplainer(model.predict, background)\n",
        "\n",
        "        # Calculate SHAP values\n",
        "        # For large datasets, use a sample\n",
        "        if len(X_test) > 500:\n",
        "            # Sample 500 rows for SHAP calculation\n",
        "            indices = np.random.choice(len(X_test), 500, replace=False)\n",
        "            X_sample = X_test.iloc[indices] if isinstance(X_test, pd.DataFrame) else X_test[indices]\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "            # Store indices for reference\n",
        "            shap_indices = indices\n",
        "        else:\n",
        "            shap_values = explainer.shap_values(X_test)\n",
        "            shap_indices = np.arange(len(X_test))\n",
        "\n",
        "        # Create SHAP explanation object\n",
        "        explanation = {\n",
        "            'shap_values': shap_values,\n",
        "            'indices': shap_indices,\n",
        "            'feature_names': feature_names,\n",
        "            'explainer': explainer\n",
        "        }\n",
        "\n",
        "        logger.info(\"SHAP explanations generated successfully\")\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating SHAP explanations: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_lime_explanations(model, X_train, X_test, feature_names=None, num_samples=5):\n",
        "    \"\"\"\n",
        "    Generate LIME explanations for selected instances\n",
        "    \"\"\"\n",
        "    if not LIME_AVAILABLE:\n",
        "        logger.warning(\"LIME not available, skipping explanation generation\")\n",
        "        return None\n",
        "\n",
        "    logger.info(f\"Generating LIME explanations for {num_samples} instances\")\n",
        "\n",
        "    try:\n",
        "        # Use feature names if provided\n",
        "        if feature_names is None and isinstance(X_train, pd.DataFrame):\n",
        "            feature_names = X_train.columns.tolist()\n",
        "\n",
        "        # Create LIME explainer\n",
        "        explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "            training_data=X_train.values if isinstance(X_train, pd.DataFrame) else X_train,\n",
        "            feature_names=feature_names,\n",
        "            mode='regression',\n",
        "            random_state=CONFIG.random_state\n",
        "        )\n",
        "\n",
        "        # Select a few instances to explain\n",
        "        if len(X_test) <= num_samples:\n",
        "            indices = range(len(X_test))\n",
        "        else:\n",
        "            indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
        "\n",
        "        # Generate explanations\n",
        "        explanations = []\n",
        "        for i in indices:\n",
        "            instance = X_test.iloc[i].values if isinstance(X_test, pd.DataFrame) else X_test[i]\n",
        "            exp = explainer.explain_instance(\n",
        "                data_row=instance,\n",
        "                predict_fn=model.predict,\n",
        "                num_features=min(10, len(feature_names)),\n",
        "            )\n",
        "            explanations.append({\n",
        "                'index': i,\n",
        "                'explanation': exp\n",
        "            })\n",
        "\n",
        "        # Create LIME explanation object\n",
        "        explanation = {\n",
        "            'explanations': explanations,\n",
        "            'indices': indices,\n",
        "            'feature_names': feature_names\n",
        "        }\n",
        "\n",
        "        logger.info(\"LIME explanations generated successfully\")\n",
        "        return explanation\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error generating LIME explanations: {e}\")\n",
        "        return None\n",
        "\n",
        "def combined_model_explanations(model, X_train, X_test, feature_names=None):\n",
        "    \"\"\"\n",
        "    Generate comprehensive model explanations using multiple techniques\n",
        "    \"\"\"\n",
        "    logger.info(\"Generating comprehensive model explanations\")\n",
        "\n",
        "    explanations = {}\n",
        "\n",
        "    # Feature importance from model\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            logger.info(\"Extracting built-in feature importances\")\n",
        "\n",
        "            importances = model.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "\n",
        "            # Convert to dictionary for better usability\n",
        "            importance_dict = {}\n",
        "            for i in indices:\n",
        "                feature_name = feature_names[i] if feature_names is not None else f\"feature_{i}\"\n",
        "                importance_dict[feature_name] = float(importances[i])\n",
        "\n",
        "            explanations['feature_importance'] = importance_dict\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting feature importances: {e}\")\n",
        "\n",
        "    # Generate SHAP explanations\n",
        "    if CONFIG.use_shap and SHAP_AVAILABLE:\n",
        "        try:\n",
        "            shap_explanation = generate_shap_explanations(model, X_test, feature_names)\n",
        "            if shap_explanation is not None:\n",
        "                explanations['shap'] = shap_explanation\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in SHAP explanation: {e}\")\n",
        "\n",
        "    # Generate LIME explanations\n",
        "    if CONFIG.use_lime and LIME_AVAILABLE:\n",
        "        try:\n",
        "            lime_explanation = generate_lime_explanations(model, X_train, X_test, feature_names)\n",
        "            if lime_explanation is not None:\n",
        "                explanations['lime'] = lime_explanation\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in LIME explanation: {e}\")\n",
        "\n",
        "    # Add partial dependence plots information\n",
        "    explanations['partial_dependence'] = {\n",
        "        'supported': True,\n",
        "        'recommended_features': feature_names[:10] if feature_names is not None else None\n",
        "    }\n",
        "\n",
        "    return explanations\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# MODEL TRAINER CLASS\n",
        "# ---------------------------------------------------------------------\n",
        "class ModelTrainer:\n",
        "    def __init__(self, config=None):\n",
        "        \"\"\"Initialize ModelTrainer with configuration\"\"\"\n",
        "        self.config = config if config is not None else CONFIG\n",
        "        self.models = {}\n",
        "        self.best_model = None\n",
        "        self.best_model_name = None\n",
        "        self.feature_names = None\n",
        "        self.metrics = {}\n",
        "        self.fairness_metrics = {}\n",
        "        self.explanations = None\n",
        "        self.uncertainty = None\n",
        "        self.ethical_assessment = None\n",
        "\n",
        "    def train_base_model(self, X_train, y_train, X_test=None, y_test=None, model_name=\"xgboost\"):\n",
        "        \"\"\"Train a single base model\"\"\"\n",
        "        logger.info(f\"Training {model_name} model\")\n",
        "\n",
        "        try:\n",
        "            model = None\n",
        "\n",
        "            # Select model type\n",
        "            if model_name == \"xgboost\":\n",
        "                model = xgb.XGBRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=self.config.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "            elif model_name == \"lightgbm\":\n",
        "                model = lgb.LGBMRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=6,\n",
        "                    learning_rate=0.1,\n",
        "                    subsample=0.8,\n",
        "                    colsample_bytree=0.8,\n",
        "                    random_state=self.config.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "            elif model_name == \"random_forest\":\n",
        "                model = RandomForestRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=8,\n",
        "                    random_state=self.config.random_state,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "            elif model_name == \"gradient_boosting\":\n",
        "                model = GradientBoostingRegressor(\n",
        "                    n_estimators=100,\n",
        "                    max_depth=4,\n",
        "                    random_state=self.config.random_state\n",
        "                )\n",
        "            elif model_name == \"hist_gradient_boosting\":\n",
        "                model = HistGradientBoostingRegressor(\n",
        "                    max_iter=100,\n",
        "                    max_depth=8,\n",
        "                    random_state=self.config.random_state\n",
        "                )\n",
        "            else:\n",
        "                logger.error(f\"Unknown model type: {model_name}\")\n",
        "                return None\n",
        "\n",
        "            # Fit model\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Store feature names\n",
        "            if isinstance(X_train, pd.DataFrame):\n",
        "                self.feature_names = X_train.columns.tolist()\n",
        "\n",
        "            # Evaluate model if test data is provided\n",
        "            metrics = {}\n",
        "            if X_test is not None and y_test is not None:\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "                # Calculate metrics\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                rmse = np.sqrt(mse)\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "                # Calculate MAPE (handle zero values)\n",
        "                y_test_safe = np.maximum(np.abs(y_test), 0.0001)\n",
        "                mape = np.mean(np.abs((y_test - y_pred) / y_test_safe)) * 100\n",
        "\n",
        "                metrics = {\n",
        "                    'mse': mse,\n",
        "                    'rmse': rmse,\n",
        "                    'r2': r2,\n",
        "                    'mae': mae,\n",
        "                    'mape': mape\n",
        "                }\n",
        "\n",
        "                logger.info(f\"{model_name} model metrics - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "            # Store model and metrics\n",
        "            self.models[model_name] = model\n",
        "            self.metrics[model_name] = metrics\n",
        "\n",
        "            return model, metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training {model_name} model: {e}\")\n",
        "            return None, {}\n",
        "\n",
        "    def train_fairness_aware_model(self, X_train, y_train, X_test=None, y_test=None,\n",
        "                                  protected_attributes=None, model_name=\"fairness_model\"):\n",
        "        \"\"\"Train a fairness-aware model\"\"\"\n",
        "        logger.info(f\"Training fairness-aware model\")\n",
        "\n",
        "        try:\n",
        "            # Set protected attributes\n",
        "            if protected_attributes is None:\n",
        "                protected_attributes = self.config.protected_attributes\n",
        "\n",
        "            # Create base model (XGBoost)\n",
        "            base_model = xgb.XGBRegressor(\n",
        "                n_estimators=100,\n",
        "                max_depth=4,\n",
        "                learning_rate=0.05,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                reg_alpha=0.1,\n",
        "                reg_lambda=1.0,\n",
        "                random_state=self.config.random_state\n",
        "            )\n",
        "\n",
        "            # Initialize fairness-aware model\n",
        "            fairness_model = FairnessAwareModel(\n",
        "                base_model=base_model,\n",
        "                protected_attributes=protected_attributes\n",
        "            )\n",
        "\n",
        "            # Fit model\n",
        "            fairness_model.fit(X_train, y_train)\n",
        "\n",
        "            # Evaluate model if test data is provided\n",
        "            metrics = {}\n",
        "            if X_test is not None and y_test is not None:\n",
        "                y_pred = fairness_model.predict(X_test)\n",
        "\n",
        "                # Calculate metrics\n",
        "                mse = mean_squared_error(y_test, y_pred)\n",
        "                rmse = np.sqrt(mse)\n",
        "                r2 = r2_score(y_test, y_pred)\n",
        "                mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "                # Calculate MAPE (handle zero values)\n",
        "                y_test_safe = np.maximum(np.abs(y_test), 0.0001)\n",
        "                mape = np.mean(np.abs((y_test - y_pred) / y_test_safe)) * 100\n",
        "\n",
        "                metrics = {\n",
        "                    'mse': mse,\n",
        "                    'rmse': rmse,\n",
        "                    'r2': r2,\n",
        "                    'mae': mae,\n",
        "                    'mape': mape\n",
        "                }\n",
        "\n",
        "                logger.info(f\"Fairness model metrics - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "                # Evaluate fairness\n",
        "                fairness_metrics = fairness_model.evaluate_fairness(X_test, y_test, y_pred)\n",
        "                self.fairness_metrics[model_name] = fairness_metrics\n",
        "\n",
        "            # Store model and metrics\n",
        "            self.models[model_name] = fairness_model\n",
        "            self.metrics[model_name] = metrics\n",
        "\n",
        "            return fairness_model, metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training fairness model: {e}\")\n",
        "            return None, {}\n",
        "\n",
        "    def train_all_models(self, X_train, y_train, X_test=None, y_test=None, include_fairness=True):\n",
        "        \"\"\"Train all configured models\"\"\"\n",
        "        logger.info(\"Training all models\")\n",
        "\n",
        "        # List of models to train\n",
        "        model_types = [\"xgboost\", \"random_forest\", \"hist_gradient_boosting\"]\n",
        "\n",
        "        # Train all base models\n",
        "        for model_type in model_types:\n",
        "            self.train_base_model(X_train, y_train, X_test, y_test, model_type)\n",
        "\n",
        "        # Train fairness-aware model if requested\n",
        "        if include_fairness:\n",
        "            self.train_fairness_aware_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # Select best model\n",
        "        self.select_best_model()\n",
        "\n",
        "        # Generate explanations for best model\n",
        "        if X_train is not None and X_test is not None and self.best_model is not None:\n",
        "            self.explanations = combined_model_explanations(\n",
        "                self.best_model, X_train, X_test, self.feature_names\n",
        "            )\n",
        "\n",
        "        # Generate uncertainty estimates\n",
        "        if X_test is not None and self.best_model is not None:\n",
        "            self.uncertainty = enhanced_prediction_uncertainty(\n",
        "                self.best_model, X_test, self.best_model_name\n",
        "            )\n",
        "\n",
        "        # Generate ethical assessment\n",
        "        if self.uncertainty is not None and self.fairness_metrics:\n",
        "            for model_name, fairness_metrics in self.fairness_metrics.items():\n",
        "                self.ethical_assessment = ethical_warning_system(\n",
        "                    self.uncertainty, fairness_metrics\n",
        "                )\n",
        "                break\n",
        "\n",
        "        return self.models\n",
        "\n",
        "    def select_best_model(self):\n",
        "        \"\"\"Select the best model based on metrics\"\"\"\n",
        "        if not self.metrics:\n",
        "            logger.warning(\"No models to select from\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Compare RMSE (lower is better)\n",
        "            rmse_values = {name: metrics.get('rmse', float('inf'))\n",
        "                         for name, metrics in self.metrics.items()}\n",
        "\n",
        "            # Find model with lowest RMSE\n",
        "            best_model_name = min(rmse_values, key=rmse_values.get)\n",
        "            best_model = self.models[best_model_name]\n",
        "\n",
        "            logger.info(f\"Selected best model: {best_model_name} (RMSE: {rmse_values[best_model_name]:.4f})\")\n",
        "\n",
        "            self.best_model = best_model\n",
        "            self.best_model_name = best_model_name\n",
        "\n",
        "            return best_model, best_model_name\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error selecting best model: {e}\")\n",
        "\n",
        "            # Fall back to first model if available\n",
        "            if self.models:\n",
        "                first_model_name = list(self.models.keys())[0]\n",
        "                logger.info(f\"Falling back to model: {first_model_name}\")\n",
        "\n",
        "                self.best_model = self.models[first_model_name]\n",
        "                self.best_model_name = first_model_name\n",
        "\n",
        "                return self.best_model, self.best_model_name\n",
        "\n",
        "            return None, None\n",
        "\n",
        "    def evaluate_model(self, model, X_test, y_test):\n",
        "        \"\"\"Evaluate a trained model on test data\"\"\"\n",
        "        logger.info(\"Evaluating model performance\")\n",
        "\n",
        "        try:\n",
        "            # Generate predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mse)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "            # Calculate MAPE (handle zero values)\n",
        "            y_test_safe = np.maximum(np.abs(y_test), 0.0001)\n",
        "            mape = np.mean(np.abs((y_test - y_pred) / y_test_safe)) * 100\n",
        "\n",
        "            metrics = {\n",
        "                'mse': mse,\n",
        "                'rmse': rmse,\n",
        "                'r2': r2,\n",
        "                'mae': mae,\n",
        "                'mape': mape\n",
        "            }\n",
        "\n",
        "            logger.info(f\"Model evaluation metrics:\")\n",
        "            logger.info(f\"  RMSE: {rmse:.4f}\")\n",
        "            logger.info(f\"  R²: {r2:.4f}\")\n",
        "            logger.info(f\"  MAE: {mae:.4f}\")\n",
        "            logger.info(f\"  MAPE: {mape:.4f}%\")\n",
        "\n",
        "            return metrics, y_pred\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error evaluating model: {e}\")\n",
        "            return {}, None\n",
        "\n",
        "    def save_model(self, model, filename=None):\n",
        "        \"\"\"Save a trained model to disk\"\"\"\n",
        "        if model is None:\n",
        "            logger.error(\"No model to save\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Create default filename if not provided\n",
        "            if filename is None:\n",
        "                if self.best_model_name:\n",
        "                    filename = f\"{self.best_model_name}_model.pkl\"\n",
        "                else:\n",
        "                    filename = \"model.pkl\"\n",
        "\n",
        "            # Create full path\n",
        "            file_path = os.path.join(self.config.models_dir, filename)\n",
        "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "            # Save model\n",
        "            with open(file_path, 'wb') as f:\n",
        "                import pickle\n",
        "                pickle.dump(model, f)\n",
        "\n",
        "            logger.info(f\"Model saved to {file_path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_model(self, filename=None):\n",
        "        \"\"\"Load a trained model from disk\"\"\"\n",
        "        try:\n",
        "            # Create default filename if not provided\n",
        "            if filename is None:\n",
        "                if self.best_model_name:\n",
        "                    filename = f\"{self.best_model_name}_model.pkl\"\n",
        "                else:\n",
        "                    filename = \"model.pkl\"\n",
        "\n",
        "            # Create full path\n",
        "            file_path = os.path.join(self.config.models_dir, filename)\n",
        "\n",
        "            if not os.path.exists(file_path):\n",
        "                logger.error(f\"Model file not found: {file_path}\")\n",
        "                return None\n",
        "\n",
        "            # Load model\n",
        "            with open(file_path, 'rb') as f:\n",
        "                import pickle\n",
        "                model = pickle.load(f)\n",
        "\n",
        "            logger.info(f\"Model loaded from {file_path}\")\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading model: {e}\")\n",
        "            return None\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# VISUALIZATION FUNCTIONS\n",
        "# ---------------------------------------------------------------------\n",
        "def plot_fairness_metrics(fairness_metrics, filename=None):\n",
        "    \"\"\"\n",
        "    Create visualizations of fairness metrics\n",
        "    \"\"\"\n",
        "    if not fairness_metrics:\n",
        "        logger.warning(\"No fairness metrics to visualize\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Collect data for plotting\n",
        "        attributes = []\n",
        "        rmse_ratios = []\n",
        "        pred_disparities = []\n",
        "\n",
        "        for attr, metrics in fairness_metrics.items():\n",
        "            if 'rmse_ratio' in metrics and 'pred_ratio_disparity' in metrics:\n",
        "                attributes.append(attr)\n",
        "                rmse_ratios.append(metrics['rmse_ratio'])\n",
        "                pred_disparities.append(metrics['pred_ratio_disparity'])\n",
        "\n",
        "        # Create grouped bar chart\n",
        "        x = np.arange(len(attributes))\n",
        "        width = 0.35\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        rects1 = ax.bar(x - width/2, rmse_ratios, width, label='RMSE Ratio')\n",
        "        rects2 = ax.bar(x + width/2, pred_disparities, width, label='Prediction Ratio Disparity')\n",
        "\n",
        "        # Add fairness threshold line\n",
        "        ax.axhline(y=CONFIG.fairness_threshold, color='r', linestyle='--', alpha=0.7,\n",
        "                  label=f'Fairness Threshold ({CONFIG.fairness_threshold})')\n",
        "\n",
        "        # Add labels and legend\n",
        "        ax.set_xlabel('Protected Attributes')\n",
        "        ax.set_ylabel('Fairness Metrics (closer to 1 = more fair)')\n",
        "        ax.set_title('Fairness Metrics Comparison')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(attributes)\n",
        "        ax.legend()\n",
        "\n",
        "        # Add value labels on bars\n",
        "        def autolabel(rects):\n",
        "            for rect in rects:\n",
        "                height = rect.get_height()\n",
        "                ax.annotate(f'{height:.2f}',\n",
        "                           xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                           xytext=(0, 3),\n",
        "                           textcoords=\"offset points\",\n",
        "                           ha='center', va='bottom')\n",
        "\n",
        "        autolabel(rects1)\n",
        "        autolabel(rects2)\n",
        "\n",
        "        fig.tight_layout()\n",
        "\n",
        "        # Save plot if filename provided\n",
        "        if filename:\n",
        "            if not filename.endswith('.png'):\n",
        "                filename += '.png'\n",
        "            save_path = os.path.join(CONFIG.viz_dir, filename)\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            logger.info(f\"Fairness metrics plot saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating fairness metrics plot: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_feature_importance(model, feature_names=None, top_n=20, filename=None):\n",
        "    \"\"\"\n",
        "    Create visualization of feature importance\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        logger.warning(\"No model provided for feature importance plot\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Check if model has feature importances\n",
        "        if not hasattr(model, 'feature_importances_'):\n",
        "            logger.warning(\"Model does not have feature_importances_ attribute\")\n",
        "            return None\n",
        "\n",
        "        # Get feature importances\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Use default feature names if not provided\n",
        "        if feature_names is None:\n",
        "            feature_names = [f\"feature_{i}\" for i in range(len(importances))]\n",
        "\n",
        "        # Limit to top N features\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        top_indices = indices[:top_n]\n",
        "        top_importances = importances[top_indices]\n",
        "        top_features = [feature_names[i] for i in top_indices]\n",
        "\n",
        "        # Create plot\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.barh(range(len(top_importances)), top_importances, align='center')\n",
        "        plt.yticks(range(len(top_importances)), top_features)\n",
        "        plt.xlabel('Importance')\n",
        "        plt.ylabel('Feature')\n",
        "        plt.title('Feature Importance')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot if filename provided\n",
        "        if filename:\n",
        "            if not filename.endswith('.png'):\n",
        "                filename += '.png'\n",
        "            save_path = os.path.join(CONFIG.viz_dir, filename)\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            logger.info(f\"Feature importance plot saved to {save_path}\")\n",
        "\n",
        "        return plt.gcf()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating feature importance plot: {e}\")\n",
        "        return None\n",
        "\n",
        "def plot_prediction_map(predictions, community_data, attribute='hardship_index', filename=None):\n",
        "    \"\"\"\n",
        "    Create a map of predictions with community boundaries colored by predictions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if we have required data\n",
        "        if predictions is None or community_data is None:\n",
        "            logger.warning(\"Missing data for prediction map\")\n",
        "            return None\n",
        "\n",
        "        # Create a new figure\n",
        "        plt.figure(figsize=(14, 10))\n",
        "\n",
        "        # Create a merged dataset\n",
        "        if isinstance(predictions, pd.DataFrame) and 'prediction' in predictions.columns:\n",
        "            pred_values = predictions['prediction']\n",
        "        else:\n",
        "            pred_values = predictions\n",
        "\n",
        "        # Ensure community_area is string type for consistency\n",
        "        community_data['community_area'] = community_data['community_area'].astype(str)\n",
        "\n",
        "        # Create plot dataset\n",
        "        plot_data = pd.DataFrame({\n",
        "            'community_area': community_data['community_area'],\n",
        "            'prediction': pred_values,\n",
        "            attribute: community_data[attribute] if attribute in community_data.columns else None\n",
        "        })\n",
        "\n",
        "        # Create two subplots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 10))\n",
        "\n",
        "        # First plot: Predictions\n",
        "        pred_plot = plot_data.plot(\n",
        "            kind='bar',\n",
        "            x='community_area',\n",
        "            y='prediction',\n",
        "            title='Predictions by Community Area',\n",
        "            legend=True,\n",
        "            ax=ax1\n",
        "        )\n",
        "        ax1.set_xlabel('Community Area')\n",
        "        ax1.set_ylabel('Predicted Crime')\n",
        "        ax1.tick_params(axis='x', rotation=90)\n",
        "\n",
        "        # Second plot: Attribute comparison\n",
        "        if attribute in plot_data.columns:\n",
        "            # Normalize values for comparison\n",
        "            norm_pred = (plot_data['prediction'] - plot_data['prediction'].min()) / \\\n",
        "                       (plot_data['prediction'].max() - plot_data['prediction'].min())\n",
        "            norm_attr = (plot_data[attribute] - plot_data[attribute].min()) / \\\n",
        "                       (plot_data[attribute].max() - plot_data[attribute].min())\n",
        "\n",
        "            comparison = pd.DataFrame({\n",
        "                'community_area': plot_data['community_area'],\n",
        "                'Normalized Prediction': norm_pred,\n",
        "                f'Normalized {attribute}': norm_attr\n",
        "            })\n",
        "\n",
        "            comp_plot = comparison.plot(\n",
        "                kind='line',\n",
        "                x='community_area',\n",
        "                y=['Normalized Prediction', f'Normalized {attribute}'],\n",
        "                title=f'Prediction vs {attribute}',\n",
        "                ax=ax2\n",
        "            )\n",
        "            ax2.set_xlabel('Community Area')\n",
        "            ax2.set_ylabel('Normalized Value')\n",
        "            ax2.tick_params(axis='x', rotation=90)\n",
        "\n",
        "            # Calculate correlation\n",
        "            corr = plot_data['prediction'].corr(plot_data[attribute])\n",
        "            ax2.annotate(f'Correlation: {corr:.2f}',\n",
        "                        xy=(0.05, 0.95),\n",
        "                        xycoords='axes fraction',\n",
        "                        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot if filename provided\n",
        "        if filename:\n",
        "            if not filename.endswith('.png'):\n",
        "                filename += '.png'\n",
        "            save_path = os.path.join(CONFIG.viz_dir, filename)\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            logger.info(f\"Prediction map saved to {save_path}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating prediction map: {e}\")\n",
        "        return None\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# WEB DASHBOARD\n",
        "# ---------------------------------------------------------------------\n",
        "def create_dashboard_app(model_trainer, data_processor, predictions=None, fairness_metrics=None):\n",
        "    \"\"\"\n",
        "    Create a Flask web dashboard for the model\n",
        "    \"\"\"\n",
        "    if not FLASK_AVAILABLE:\n",
        "        logger.warning(\"Flask not available, skipping dashboard creation\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Initialize Flask app\n",
        "        app = Flask(__name__, template_folder=CONFIG.templates_dir)\n",
        "\n",
        "        # Store data in app context\n",
        "        app.config['MODEL_TRAINER'] = model_trainer\n",
        "        app.config['DATA_PROCESSOR'] = data_processor\n",
        "        app.config['PREDICTIONS'] = predictions\n",
        "        app.config['FAIRNESS_METRICS'] = fairness_metrics\n",
        "\n",
        "        # Create routes\n",
        "        @app.route('/')\n",
        "        def index():\n",
        "            return render_template('index.html', title=\"Ethical Predictive Policing Dashboard\")\n",
        "\n",
        "        @app.route('/metrics')\n",
        "        def metrics():\n",
        "            # Get model metrics\n",
        "            metrics = model_trainer.metrics if model_trainer else {}\n",
        "            best_model_name = model_trainer.best_model_name if model_trainer else None\n",
        "\n",
        "            return render_template(\n",
        "                'metrics.html',\n",
        "                title=\"Model Metrics\",\n",
        "                metrics=metrics,\n",
        "                best_model=best_model_name\n",
        "            )\n",
        "\n",
        "        @app.route('/fairness')\n",
        "        def fairness():\n",
        "            # Get fairness metrics\n",
        "            metrics = fairness_metrics if fairness_metrics else {}\n",
        "            if not metrics and model_trainer:\n",
        "                metrics = model_trainer.fairness_metrics\n",
        "\n",
        "            return render_template(\n",
        "                'fairness.html',\n",
        "                title=\"Fairness Metrics\",\n",
        "                metrics=metrics,\n",
        "                threshold=CONFIG.fairness_threshold\n",
        "            )\n",
        "\n",
        "        @app.route('/predictions')\n",
        "        def show_predictions():\n",
        "            # Get predictions\n",
        "            preds = predictions if predictions is not None else {}\n",
        "\n",
        "            # Limit to first 100 rows for display\n",
        "            if isinstance(preds, pd.DataFrame) and len(preds) > 100:\n",
        "                preds_display = preds.head(100)\n",
        "            else:\n",
        "                preds_display = preds\n",
        "\n",
        "            return render_template(\n",
        "                'predictions.html',\n",
        "                title=\"Model Predictions\",\n",
        "                predictions=preds_display\n",
        "            )\n",
        "\n",
        "        @app.route('/ethics')\n",
        "        def ethics():\n",
        "            # Get ethical assessment\n",
        "            assessment = None\n",
        "            if model_trainer and model_trainer.ethical_assessment:\n",
        "                assessment = model_trainer.ethical_assessment\n",
        "\n",
        "            return render_template(\n",
        "                'ethics.html',\n",
        "                title=\"Ethical Assessment\",\n",
        "                assessment=assessment\n",
        "            )\n",
        "\n",
        "        @app.route('/api/predictions', methods=['GET'])\n",
        "        def api_predictions():\n",
        "            # Return predictions as JSON\n",
        "            preds = predictions if predictions is not None else {}\n",
        "\n",
        "            if isinstance(preds, pd.DataFrame):\n",
        "                return jsonify(preds.to_dict(orient='records'))\n",
        "            return jsonify([])\n",
        "\n",
        "        @app.route('/api/fairness', methods=['GET'])\n",
        "        def api_fairness():\n",
        "            # Return fairness metrics as JSON\n",
        "            metrics = fairness_metrics if fairness_metrics else {}\n",
        "            if not metrics and model_trainer:\n",
        "                metrics = model_trainer.fairness_metrics\n",
        "\n",
        "            return jsonify(metrics)\n",
        "\n",
        "        logger.info(\"Flask dashboard created successfully\")\n",
        "        return app\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating dashboard: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_dashboard(app):\n",
        "    \"\"\"\n",
        "    Run the Flask dashboard\n",
        "    \"\"\"\n",
        "    if app is None:\n",
        "        logger.error(\"No app to run\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Run app\n",
        "        app.run(\n",
        "            host=CONFIG.dashboard_host,\n",
        "            port=CONFIG.dashboard_port,\n",
        "            debug=False\n",
        "        )\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error running dashboard: {e}\")\n",
        "        return False\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# MAIN PIPELINE FUNCTION\n",
        "# ---------------------------------------------------------------------\n",
        "def run_pipeline(use_full_dataset=False):\n",
        "    \"\"\"\n",
        "    Execute the complete ethical predictive policing pipeline\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting ethical predictive policing pipeline\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Set up data processor\n",
        "    data_processor = DataProcessor()\n",
        "\n",
        "    # Load data\n",
        "    data_processor.load_datasets(\n",
        "        use_cache=CONFIG.use_caching,\n",
        "        use_full_dataset=use_full_dataset,\n",
        "        years=CONFIG.years\n",
        "    )\n",
        "\n",
        "    # Clean data\n",
        "    data_processor.clean_data()\n",
        "\n",
        "    # Engineer features\n",
        "    data_processor.engineer_features()\n",
        "\n",
        "    # Merge datasets\n",
        "    merged_data = data_processor.merge_datasets()\n",
        "\n",
        "    if merged_data is None or len(merged_data) == 0:\n",
        "        logger.error(\"Failed to prepare merged dataset. Exiting pipeline.\")\n",
        "        return None\n",
        "\n",
        "    # Save processed data\n",
        "    data_processor.save_processed_data(file_prefix=\"processed_data\")\n",
        "\n",
        "    # Prepare train/test split\n",
        "    X_train, X_test, y_train, y_test = data_processor.prepare_train_test_split()\n",
        "\n",
        "    if X_train is None or y_train is None:\n",
        "        logger.error(\"Failed to prepare train/test data. Exiting pipeline.\")\n",
        "        return None\n",
        "\n",
        "    # Prepare features for modeling\n",
        "    # Pass y_train to enable feature selection\n",
        "    X_train_proc, X_test_proc, selected_features = data_processor.prepare_model_features(\n",
        "        X_train, X_test, y_train,\n",
        "        feature_selection=CONFIG.feature_selection\n",
        "    )\n",
        "\n",
        "    if X_train_proc is None or X_test_proc is None:\n",
        "        logger.error(\"Failed to prepare model features. Exiting pipeline.\")\n",
        "        return None\n",
        "\n",
        "    # Set up model trainer\n",
        "    model_trainer = ModelTrainer(CONFIG)\n",
        "\n",
        "    # Train models\n",
        "    model_trainer.train_all_models(\n",
        "        X_train_proc, y_train,\n",
        "        X_test_proc, y_test,\n",
        "        include_fairness=True\n",
        "    )\n",
        "\n",
        "    # Get best model\n",
        "    best_model = model_trainer.best_model\n",
        "    best_model_name = model_trainer.best_model_name\n",
        "\n",
        "    if best_model is None:\n",
        "        logger.error(\"No best model selected. Exiting pipeline.\")\n",
        "        return None\n",
        "\n",
        "    # Generate predictions with uncertainty\n",
        "    predictions = enhanced_prediction_uncertainty(\n",
        "        best_model, X_test_proc,\n",
        "        method='ensemble'\n",
        "    )\n",
        "\n",
        "    # Evaluate fairness\n",
        "    fairness_metrics = comprehensive_fairness_analysis(\n",
        "        best_model, X_test_proc, y_test,\n",
        "        predictions['prediction'] if predictions is not None else None\n",
        "    )\n",
        "\n",
        "    # Generate ethical assessment\n",
        "    ethical_assessment = ethical_warning_system(\n",
        "        predictions, fairness_metrics\n",
        "    )\n",
        "\n",
        "    # Save best model\n",
        "    model_trainer.save_model(best_model, f\"{best_model_name}_model.pkl\")\n",
        "\n",
        "    # Generate visualizations\n",
        "    plot_fairness_metrics(fairness_metrics, \"fairness_metrics.png\")\n",
        "    plot_feature_importance(best_model, selected_features, top_n=20, filename=\"feature_importance.png\")\n",
        "\n",
        "    # Create community level predictions if available\n",
        "    if 'community_area' in X_test.columns:\n",
        "        community_preds = pd.DataFrame({\n",
        "            'community_area': X_test['community_area'],\n",
        "            'prediction': predictions['prediction'] if predictions is not None else best_model.predict(X_test_proc)\n",
        "        })\n",
        "\n",
        "        # Plot prediction map if community data is available\n",
        "        if data_processor.community_data is not None:\n",
        "            plot_prediction_map(\n",
        "                community_preds,\n",
        "                data_processor.community_data,\n",
        "                attribute='hardship_index',\n",
        "                filename=\"community_predictions.png\"\n",
        "            )\n",
        "\n",
        "    # Create web dashboard if requested\n",
        "    app = None\n",
        "    if CONFIG.enable_dashboard:\n",
        "        app = create_dashboard_app(\n",
        "            model_trainer,\n",
        "            data_processor,\n",
        "            predictions,\n",
        "            fairness_metrics\n",
        "        )\n",
        "\n",
        "        if app is not None:\n",
        "            logger.info(f\"Dashboard ready to run at http://{CONFIG.dashboard_host}:{CONFIG.dashboard_port}\")\n",
        "            logger.info(\"Use run_dashboard(app) to start the server\")\n",
        "\n",
        "    # Calculate execution time\n",
        "    execution_time = time.time() - start_time\n",
        "    logger.info(f\"Pipeline completed in {execution_time:.2f} seconds\")\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        'model_trainer': model_trainer,\n",
        "        'data_processor': data_processor,\n",
        "        'best_model': best_model,\n",
        "        'best_model_name': best_model_name,\n",
        "        'predictions': predictions,\n",
        "        'fairness_metrics': fairness_metrics,\n",
        "        'ethical_assessment': ethical_assessment,\n",
        "        'execution_time': execution_time,\n",
        "        'app': app\n",
        "    }\n",
        "\n",
        "# Run pipeline when executed as script\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_pipeline(use_full_dataset=False)\n",
        "\n",
        "    # Run dashboard if available\n",
        "    if results is not None and CONFIG.enable_dashboard and 'app' in results and results['app'] is not None:\n",
        "        run_dashboard(results['app'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67fb2a03e4794ea0b069f5547decdb36",
            "4d4d95eef3914ca5a0cdb145cc9611c8",
            "606f0973fd0049e783df75e268d47be3",
            "c429d259cafc4abaa589c6ea22fca2f5",
            "31bfa5c6662242ea97895053581e80c2",
            "4e3b503c1bbc40a88ced96f0180e4a16",
            "09c76224838148a29d5f8d8709ecef67",
            "e9698779de2746a2a7e3137e74d3849b",
            "7f3ffdec2a984360a11c6b23e07e8a75",
            "754fe2d403be4fe59e5d312988a40b07",
            "00ee158d5a1241c4b2a17dde8d2a1cbb"
          ]
        },
        "id": "RsOEc8wPzMfP",
        "outputId": "0478e088-ab00-4fcc-ff8a-a00bf5928262"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-02 07:54:44,791 - predictive_policing - INFO - Starting ethical predictive policing pipeline\n",
            "INFO:predictive_policing:Starting ethical predictive policing pipeline\n",
            "2025-05-02 07:54:44,792 - predictive_policing - INFO - Loading datasets (full_dataset=False, years=[2018, 2019, 2020, 2021, 2022, 2023, 2024])...\n",
            "INFO:predictive_policing:Loading datasets (full_dataset=False, years=[2018, 2019, 2020, 2021, 2022, 2023, 2024])...\n",
            "2025-05-02 07:54:44,794 - predictive_policing - INFO - Loading Chicago crime data from cache\n",
            "INFO:predictive_policing:Loading Chicago crime data from cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-02 07:55:02,219 - predictive_policing - INFO - Filtered to years [2018, 2019, 2020, 2021, 2022, 2023, 2024]: 0 records\n",
            "INFO:predictive_policing:Filtered to years [2018, 2019, 2020, 2021, 2022, 2023, 2024]: 0 records\n",
            "2025-05-02 07:55:02,221 - predictive_policing - INFO - Loaded 0 records from cache\n",
            "INFO:predictive_policing:Loaded 0 records from cache\n",
            "2025-05-02 07:55:02,222 - predictive_policing - WARNING - Crime data cache is empty, fetching from API directly\n",
            "WARNING:predictive_policing:Crime data cache is empty, fetching from API directly\n",
            "2025-05-02 07:55:03,301 - predictive_policing - INFO - Fetching Chicago crime data from API with year filter: &$where=year=2018 OR year=2019 OR year=2020 OR year=2021 OR year=2022 OR year=2023 OR year=2024\n",
            "INFO:predictive_policing:Fetching Chicago crime data from API with year filter: &$where=year=2018 OR year=2019 OR year=2020 OR year=2021 OR year=2022 OR year=2023 OR year=2024\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching Chicago Crime Data:   0%|          | 0/100000 [00:00<?, ?rec/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67fb2a03e4794ea0b069f5547decdb36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-05-02 07:55:55,656 - predictive_policing - INFO - Fetched 100,000 Chicago crime records from API\n",
            "INFO:predictive_policing:Fetched 100,000 Chicago crime records from API\n",
            "2025-05-02 07:55:58,449 - predictive_policing - INFO - Crime data shape: (100000, 22)\n",
            "INFO:predictive_policing:Crime data shape: (100000, 22)\n",
            "2025-05-02 07:55:58,450 - predictive_policing - INFO - Community data shape: (77, 6)\n",
            "INFO:predictive_policing:Community data shape: (77, 6)\n",
            "2025-05-02 07:55:58,451 - predictive_policing - INFO - Census data shape: (78, 9)\n",
            "INFO:predictive_policing:Census data shape: (78, 9)\n",
            "2025-05-02 07:55:58,453 - predictive_policing - INFO - Cleaning all datasets...\n",
            "INFO:predictive_policing:Cleaning all datasets...\n",
            "2025-05-02 07:55:58,454 - predictive_policing - INFO - Cleaning dataframe...\n",
            "INFO:predictive_policing:Cleaning dataframe...\n",
            "2025-05-02 07:56:00,552 - predictive_policing - INFO - Cleaning dataframe...\n",
            "INFO:predictive_policing:Cleaning dataframe...\n",
            "2025-05-02 07:56:00,652 - predictive_policing - INFO - Cleaning dataframe...\n",
            "INFO:predictive_policing:Cleaning dataframe...\n",
            "2025-05-02 07:56:00,661 - predictive_policing - INFO - Data cleaning completed\n",
            "INFO:predictive_policing:Data cleaning completed\n",
            "2025-05-02 07:56:00,662 - predictive_policing - INFO - Engineering features...\n",
            "INFO:predictive_policing:Engineering features...\n",
            "2025-05-02 07:56:00,779 - predictive_policing - INFO - Created temporal features\n",
            "INFO:predictive_policing:Created temporal features\n",
            "2025-05-02 07:56:01,017 - predictive_policing - INFO - Created crime-specific features\n",
            "INFO:predictive_policing:Created crime-specific features\n",
            "2025-05-02 07:56:01,113 - predictive_policing - INFO - Created spatial features\n",
            "INFO:predictive_policing:Created spatial features\n",
            "2025-05-02 07:56:01,150 - predictive_policing - INFO - Created 'community_area' column from index\n",
            "INFO:predictive_policing:Created 'community_area' column from index\n",
            "2025-05-02 07:56:01,156 - predictive_policing - INFO - Merged community and census data: (78, 15)\n",
            "INFO:predictive_policing:Merged community and census data: (78, 15)\n",
            "2025-05-02 07:56:01,160 - predictive_policing - INFO - Created socioeconomic features\n",
            "INFO:predictive_policing:Created socioeconomic features\n",
            "2025-05-02 07:56:01,162 - predictive_policing - INFO - Performing enhanced reporting bias analysis\n",
            "INFO:predictive_policing:Performing enhanced reporting bias analysis\n",
            "2025-05-02 07:56:01,306 - predictive_policing - INFO - Enhanced reporting factor statistics: Min=0.70, Max=1.30, Mean=1.07\n",
            "INFO:predictive_policing:Enhanced reporting factor statistics: Min=0.70, Max=1.30, Mean=1.07\n",
            "2025-05-02 07:56:01,345 - predictive_policing - INFO - Feature engineering completed\n",
            "INFO:predictive_policing:Feature engineering completed\n",
            "2025-05-02 07:56:01,346 - predictive_policing - INFO - Merging datasets...\n",
            "INFO:predictive_policing:Merging datasets...\n",
            "2025-05-02 07:56:01,450 - predictive_policing - INFO - Aggregating crime data...\n",
            "INFO:predictive_policing:Aggregating crime data...\n",
            "2025-05-02 07:56:01,818 - predictive_policing - INFO - Aggregating by community area and month_year\n",
            "INFO:predictive_policing:Aggregating by community area and month_year\n",
            "2025-05-02 07:56:01,883 - predictive_policing - INFO - Aggregated crime data shape: (3909, 16)\n",
            "INFO:predictive_policing:Aggregated crime data shape: (3909, 16)\n",
            "2025-05-02 07:56:01,885 - predictive_policing - INFO - Creating lagged features for time series analysis\n",
            "INFO:predictive_policing:Creating lagged features for time series analysis\n",
            "2025-05-02 07:56:01,944 - predictive_policing - INFO - Merged dataset shape: (3909, 42)\n",
            "INFO:predictive_policing:Merged dataset shape: (3909, 42)\n",
            "2025-05-02 07:56:01,945 - predictive_policing - INFO - Applying reporting bias correction to crime_count\n",
            "INFO:predictive_policing:Applying reporting bias correction to crime_count\n",
            "2025-05-02 07:56:01,949 - predictive_policing - INFO - Original crime_count mean: 25.58\n",
            "INFO:predictive_policing:Original crime_count mean: 25.58\n",
            "2025-05-02 07:56:01,951 - predictive_policing - INFO - Adjusted crime_count_adjusted mean: 25.58\n",
            "INFO:predictive_policing:Adjusted crime_count_adjusted mean: 25.58\n",
            "2025-05-02 07:56:01,952 - predictive_policing - INFO - Using bias-adjusted target: crime_count_adjusted\n",
            "INFO:predictive_policing:Using bias-adjusted target: crime_count_adjusted\n",
            "2025-05-02 07:56:01,976 - predictive_policing - INFO - Created interaction features\n",
            "INFO:predictive_policing:Created interaction features\n",
            "2025-05-02 07:56:01,997 - predictive_policing - INFO - Final merged dataset shape: (3909, 73)\n",
            "INFO:predictive_policing:Final merged dataset shape: (3909, 73)\n",
            "2025-05-02 07:56:02,084 - predictive_policing - INFO - Saved processed data to /content/drive/MyDrive/predictive_policing/data/processed_data.parquet\n",
            "INFO:predictive_policing:Saved processed data to /content/drive/MyDrive/predictive_policing/data/processed_data.parquet\n",
            "2025-05-02 07:56:02,099 - predictive_policing - INFO - Saved column information to /content/drive/MyDrive/predictive_policing/data/processed_data_info.json\n",
            "INFO:predictive_policing:Saved column information to /content/drive/MyDrive/predictive_policing/data/processed_data_info.json\n",
            "2025-05-02 07:56:02,100 - predictive_policing - INFO - Preparing train/test split using by_year method\n",
            "INFO:predictive_policing:Preparing train/test split using by_year method\n",
            "2025-05-02 07:56:02,105 - predictive_policing - INFO - Time-based split: train years=[np.int16(2018), np.int16(2019), np.int16(2020), np.int16(2021), np.int16(2022), np.int16(2023)], test year=2024\n",
            "INFO:predictive_policing:Time-based split: train years=[np.int16(2018), np.int16(2019), np.int16(2020), np.int16(2021), np.int16(2022), np.int16(2023)], test year=2024\n",
            "2025-05-02 07:56:02,111 - predictive_policing - INFO - Train-test split: (3251, 72) train, (658, 72) test\n",
            "INFO:predictive_policing:Train-test split: (3251, 72) train, (658, 72) test\n",
            "2025-05-02 07:56:02,112 - predictive_policing - INFO - Preparing model features\n",
            "INFO:predictive_policing:Preparing model features\n",
            "2025-05-02 07:56:02,116 - predictive_policing - INFO - Found 60 numeric columns available for modeling\n",
            "INFO:predictive_policing:Found 60 numeric columns available for modeling\n",
            "2025-05-02 07:56:02,129 - predictive_policing - INFO - Found 14 columns with NaN values. Applying column-wise imputation.\n",
            "INFO:predictive_policing:Found 14 columns with NaN values. Applying column-wise imputation.\n",
            "2025-05-02 07:56:02,139 - predictive_policing - WARNING - NaNs still present after imputation: 45514 in train, 9212 in test\n",
            "WARNING:predictive_policing:NaNs still present after imputation: 45514 in train, 9212 in test\n",
            "2025-05-02 07:56:02,142 - predictive_policing - INFO - Fallback imputation applied: remaining NaNs filled with 0\n",
            "INFO:predictive_policing:Fallback imputation applied: remaining NaNs filled with 0\n",
            "2025-05-02 07:56:02,143 - predictive_policing - INFO - Selecting features using importance method (max=30)\n",
            "INFO:predictive_policing:Selecting features using importance method (max=30)\n",
            "2025-05-02 07:56:02,475 - predictive_policing - INFO - Selected 30 features\n",
            "INFO:predictive_policing:Selected 30 features\n",
            "2025-05-02 07:56:02,479 - predictive_policing - INFO - Selected 30 features\n",
            "INFO:predictive_policing:Selected 30 features\n",
            "2025-05-02 07:56:02,480 - predictive_policing - INFO - Prepared features: train=(3251, 30), test=(658, 30)\n",
            "INFO:predictive_policing:Prepared features: train=(3251, 30), test=(658, 30)\n",
            "2025-05-02 07:56:02,482 - predictive_policing - INFO - Training all models\n",
            "INFO:predictive_policing:Training all models\n",
            "2025-05-02 07:56:02,483 - predictive_policing - INFO - Training xgboost model\n",
            "INFO:predictive_policing:Training xgboost model\n",
            "2025-05-02 07:56:02,804 - predictive_policing - INFO - xgboost model metrics - RMSE: 1.8587, R²: 0.9990\n",
            "INFO:predictive_policing:xgboost model metrics - RMSE: 1.8587, R²: 0.9990\n",
            "2025-05-02 07:56:02,817 - predictive_policing - INFO - Training random_forest model\n",
            "INFO:predictive_policing:Training random_forest model\n",
            "2025-05-02 07:56:03,128 - predictive_policing - INFO - random_forest model metrics - RMSE: 0.4166, R²: 1.0000\n",
            "INFO:predictive_policing:random_forest model metrics - RMSE: 0.4166, R²: 1.0000\n",
            "2025-05-02 07:56:03,130 - predictive_policing - INFO - Training hist_gradient_boosting model\n",
            "INFO:predictive_policing:Training hist_gradient_boosting model\n",
            "2025-05-02 07:56:03,324 - predictive_policing - INFO - hist_gradient_boosting model metrics - RMSE: 4.5359, R²: 0.9942\n",
            "INFO:predictive_policing:hist_gradient_boosting model metrics - RMSE: 4.5359, R²: 0.9942\n",
            "2025-05-02 07:56:03,327 - predictive_policing - INFO - Training fairness-aware model\n",
            "INFO:predictive_policing:Training fairness-aware model\n",
            "2025-05-02 07:56:03,329 - predictive_policing - INFO - Training fairness-aware model\n",
            "INFO:predictive_policing:Training fairness-aware model\n",
            "2025-05-02 07:56:03,330 - predictive_policing - INFO - Applying reweighing for protected attribute: hardship_index\n",
            "INFO:predictive_policing:Applying reweighing for protected attribute: hardship_index\n",
            "2025-05-02 07:56:03,332 - predictive_policing - ERROR - Error in reweighing: 'hardship_index' is not in list\n",
            "ERROR:predictive_policing:Error in reweighing: 'hardship_index' is not in list\n",
            "2025-05-02 07:56:03,333 - predictive_policing - INFO - Training base model\n",
            "INFO:predictive_policing:Training base model\n",
            "2025-05-02 07:56:03,669 - predictive_policing - INFO - Training adversarial debiasing model for hardship_index\n",
            "INFO:predictive_policing:Training adversarial debiasing model for hardship_index\n",
            "2025-05-02 07:56:03,671 - predictive_policing - WARNING - Protected attribute hardship_index not found\n",
            "WARNING:predictive_policing:Protected attribute hardship_index not found\n",
            "2025-05-02 07:56:03,672 - predictive_policing - INFO - Making fairness-aware predictions\n",
            "INFO:predictive_policing:Making fairness-aware predictions\n",
            "2025-05-02 07:56:03,682 - predictive_policing - INFO - Applying threshold moving for hardship_index\n",
            "INFO:predictive_policing:Applying threshold moving for hardship_index\n",
            "2025-05-02 07:56:03,683 - predictive_policing - WARNING - Protected attribute hardship_index not found\n",
            "WARNING:predictive_policing:Protected attribute hardship_index not found\n",
            "2025-05-02 07:56:03,687 - predictive_policing - INFO - Fairness model metrics - RMSE: 2.0779, R²: 0.9988\n",
            "INFO:predictive_policing:Fairness model metrics - RMSE: 2.0779, R²: 0.9988\n",
            "2025-05-02 07:56:03,688 - predictive_policing - INFO - Evaluating model fairness\n",
            "INFO:predictive_policing:Evaluating model fairness\n",
            "2025-05-02 07:56:03,689 - predictive_policing - INFO - Selected best model: random_forest (RMSE: 0.4166)\n",
            "INFO:predictive_policing:Selected best model: random_forest (RMSE: 0.4166)\n",
            "2025-05-02 07:56:03,690 - predictive_policing - INFO - Generating comprehensive model explanations\n",
            "INFO:predictive_policing:Generating comprehensive model explanations\n",
            "2025-05-02 07:56:03,717 - predictive_policing - INFO - Extracting built-in feature importances\n",
            "INFO:predictive_policing:Extracting built-in feature importances\n",
            "2025-05-02 07:56:03,743 - predictive_policing - INFO - Generating SHAP explanations\n",
            "INFO:predictive_policing:Generating SHAP explanations\n",
            "2025-05-02 07:56:04,332 - predictive_policing - INFO - SHAP explanations generated successfully\n",
            "INFO:predictive_policing:SHAP explanations generated successfully\n",
            "2025-05-02 07:56:04,334 - predictive_policing - INFO - Generating enhanced predictions with uncertainty (ensemble method)\n",
            "INFO:predictive_policing:Generating enhanced predictions with uncertainty (ensemble method)\n",
            "2025-05-02 07:56:08,689 - predictive_policing - INFO - Successfully generated enhanced uncertainty estimates\n",
            "INFO:predictive_policing:Successfully generated enhanced uncertainty estimates\n",
            "2025-05-02 07:56:08,690 - predictive_policing - INFO - Analyzing predictions for ethical concerns\n",
            "INFO:predictive_policing:Analyzing predictions for ethical concerns\n",
            "2025-05-02 07:56:08,693 - predictive_policing - INFO - Ethical assessment: red with 2 warnings\n",
            "INFO:predictive_policing:Ethical assessment: red with 2 warnings\n",
            "2025-05-02 07:56:08,695 - predictive_policing - INFO - Generating enhanced predictions with uncertainty (ensemble method)\n",
            "INFO:predictive_policing:Generating enhanced predictions with uncertainty (ensemble method)\n",
            "2025-05-02 07:56:13,548 - predictive_policing - INFO - Successfully generated enhanced uncertainty estimates\n",
            "INFO:predictive_policing:Successfully generated enhanced uncertainty estimates\n",
            "2025-05-02 07:56:13,549 - predictive_policing - INFO - Performing comprehensive fairness analysis\n",
            "INFO:predictive_policing:Performing comprehensive fairness analysis\n",
            "2025-05-02 07:56:13,551 - predictive_policing - WARNING - No protected attributes found in test data\n",
            "WARNING:predictive_policing:No protected attributes found in test data\n",
            "2025-05-02 07:56:13,552 - predictive_policing - INFO - Analyzing predictions for ethical concerns\n",
            "INFO:predictive_policing:Analyzing predictions for ethical concerns\n",
            "2025-05-02 07:56:13,554 - predictive_policing - INFO - Ethical assessment: red with 2 warnings\n",
            "INFO:predictive_policing:Ethical assessment: red with 2 warnings\n",
            "2025-05-02 07:56:13,573 - predictive_policing - INFO - Model saved to /content/drive/MyDrive/predictive_policing/models/random_forest_model.pkl\n",
            "INFO:predictive_policing:Model saved to /content/drive/MyDrive/predictive_policing/models/random_forest_model.pkl\n",
            "2025-05-02 07:56:13,574 - predictive_policing - WARNING - No fairness metrics to visualize\n",
            "WARNING:predictive_policing:No fairness metrics to visualize\n",
            "2025-05-02 07:56:14,281 - predictive_policing - INFO - Feature importance plot saved to /content/drive/MyDrive/predictive_policing/results/visualizations/feature_importance.png\n",
            "INFO:predictive_policing:Feature importance plot saved to /content/drive/MyDrive/predictive_policing/results/visualizations/feature_importance.png\n",
            "2025-05-02 07:56:22,512 - predictive_policing - INFO - Prediction map saved to /content/drive/MyDrive/predictive_policing/results/visualizations/community_predictions.png\n",
            "INFO:predictive_policing:Prediction map saved to /content/drive/MyDrive/predictive_policing/results/visualizations/community_predictions.png\n",
            "2025-05-02 07:56:22,517 - predictive_policing - INFO - Flask dashboard created successfully\n",
            "INFO:predictive_policing:Flask dashboard created successfully\n",
            "2025-05-02 07:56:22,518 - predictive_policing - INFO - Dashboard ready to run at http://127.0.0.1:5000\n",
            "INFO:predictive_policing:Dashboard ready to run at http://127.0.0.1:5000\n",
            "2025-05-02 07:56:22,520 - predictive_policing - INFO - Use run_dashboard(app) to start the server\n",
            "INFO:predictive_policing:Use run_dashboard(app) to start the server\n",
            "2025-05-02 07:56:22,521 - predictive_policing - INFO - Pipeline completed in 97.73 seconds\n",
            "INFO:predictive_policing:Pipeline completed in 97.73 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in callback <function flush_figures at 0x7a281c69cae0> (for post_execute):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;31m# ignore the tracking, just draw and close all figures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# safely show traceback if in IPython, else raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib_inline/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tight\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m                     bbox_inches = self.figure.get_tightbbox(\n\u001b[0m\u001b[1;32m   2159\u001b[0m                         renderer, bbox_extra_artists=bbox_extra_artists)\n\u001b[1;32m   2160\u001b[0m                     if (isinstance(layout_engine, ConstrainedLayoutEngine) and\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0;31m# need this conditional....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m                     bbox = ax.get_tightbbox(\n\u001b[0m\u001b[1;32m   1849\u001b[0m                         renderer, bbox_extra_artists=bbox_extra_artists)\n\u001b[1;32m   1850\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4517\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axis_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4518\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxison\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4519\u001b[0;31m                 \u001b[0mba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmartist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tightbbox_for_layout_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4520\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mba\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4521\u001b[0m                     \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_get_tightbbox_for_layout_only\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \"\"\"\n\u001b[1;32m   1401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"for_layout_only\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, for_layout_only)\u001b[0m\n\u001b[1;32m   1367\u001b[0m         \u001b[0mtlb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ticklabel_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_offset_text_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffsetText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_offset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_offset_text_position\u001b[0;34m(self, bboxes, bboxes2)\u001b[0m\n\u001b[1;32m   2485\u001b[0m                 \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2486\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2487\u001b[0;31m                 \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2488\u001b[0m                 \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2489\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOFFSETTEXTPAD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(bboxes)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxmax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymin\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mymax\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mymin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mymin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;34m\"\"\"The bottom edge of the bounding box.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mmin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3040\u001b[0m     \u001b[0;36m6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3041\u001b[0m     \"\"\"\n\u001b[0;32m-> 3042\u001b[0;31m     return _wrapreduction(a, np.minimum, 'min', axis, None, out,\n\u001b[0m\u001b[1;32m   3043\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   3044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}